The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'codellama/CodeLlama-13b-Instruct-hf', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_codellama']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 2, LOCAL_RANK: 2
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'codellama/CodeLlama-13b-Instruct-hf', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_codellama']
1
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 3, LOCAL_RANK: 3
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'codellama/CodeLlama-13b-Instruct-hf', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_codellama']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 6, LOCAL_RANK: 6
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
2
3
args:
Namespace(model_name_or_path='codellama/CodeLlama-13b-Instruct-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_codellama', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=500, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_codellama/runs/Sep01_06-26-50_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=2, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 2
Local process index: 2
Device: cuda:2
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=2), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 2}
max_memory{'': '80000MB'}
loading base model codellama/CodeLlama-13b-Instruct-hf...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='codellama/CodeLlama-13b-Instruct-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_codellama', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=500, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_codellama/runs/Sep01_06-26-50_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=3, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 3
Local process index: 3
Device: cuda:3
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=3), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 3}
max_memory{'': '80000MB'}
loading base model codellama/CodeLlama-13b-Instruct-hf...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'codellama/CodeLlama-13b-Instruct-hf', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_codellama']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 7, LOCAL_RANK: 7
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
2
3
args:
Namespace(model_name_or_path='codellama/CodeLlama-13b-Instruct-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_codellama', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=500, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_codellama/runs/Sep01_06-26-50_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=6, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 6
Local process index: 6
Device: cuda:6
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=6), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 6}
max_memory{'': '80000MB'}
loading base model codellama/CodeLlama-13b-Instruct-hf...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'codellama/CodeLlama-13b-Instruct-hf', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_codellama']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 5, LOCAL_RANK: 5
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'codellama/CodeLlama-13b-Instruct-hf', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_codellama']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 0, LOCAL_RANK: 0
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'codellama/CodeLlama-13b-Instruct-hf', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_codellama']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 1, LOCAL_RANK: 1
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'codellama/CodeLlama-13b-Instruct-hf', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_codellama']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 4, LOCAL_RANK: 4
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
2
3
args:
Namespace(model_name_or_path='codellama/CodeLlama-13b-Instruct-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_codellama', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=500, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_codellama/runs/Sep01_06-26-50_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=7, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 7
Local process index: 7
Device: cuda:7
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=7), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 7}
max_memory{'': '80000MB'}
loading base model codellama/CodeLlama-13b-Instruct-hf...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='codellama/CodeLlama-13b-Instruct-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_codellama', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=500, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_codellama/runs/Sep01_06-26-50_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 0
Local process index: 0
Device: cuda:0
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 0}
max_memory{'': '80000MB'}
loading base model codellama/CodeLlama-13b-Instruct-hf...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='codellama/CodeLlama-13b-Instruct-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_codellama', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=500, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_codellama/runs/Sep01_06-26-50_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=5, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 5
Local process index: 5
Device: cuda:5
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=5), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 5}
max_memory{'': '80000MB'}
loading base model codellama/CodeLlama-13b-Instruct-hf...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='codellama/CodeLlama-13b-Instruct-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_codellama', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=500, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_codellama/runs/Sep01_06-26-51_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=1, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 1
Local process index: 1
Device: cuda:1
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=1), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 1}
max_memory{'': '80000MB'}
loading base model codellama/CodeLlama-13b-Instruct-hf...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='codellama/CodeLlama-13b-Instruct-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_codellama', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=500, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_codellama/runs/Sep01_06-26-51_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=4, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 4
Local process index: 4
Device: cuda:4
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=4), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 4}
max_memory{'': '80000MB'}
loading base model codellama/CodeLlama-13b-Instruct-hf...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.33s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.14s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.83s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.01s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:23, 11.75s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:23, 11.71s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:23, 11.98s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:12<00:24, 12.21s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.53s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.93s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.31s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.03s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:22<00:11, 11.18s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:22<00:11, 11.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.27s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:12, 12.30s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:12, 12.45s/it]/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.58s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.65s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.92s/it]
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Adding special tokens.
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'down_proj', 'gate_proj', 'up_proj', 'v_proj', 'o_proj', 'k_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32017, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32017, bias=False)
)
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Adding special tokens.
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'q_proj', 'up_proj', 'v_proj', 'o_proj', 'gate_proj', 'k_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32017, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32017, bias=False)
)
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Adding special tokens.
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'v_proj', 'q_proj', 'up_proj', 'gate_proj', 'o_proj', 'k_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32017, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32017, bias=False)
)
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Adding special tokens.
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'q_proj', 'down_proj', 'v_proj', 'gate_proj', 'up_proj', 'k_proj', 'o_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32017, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32017, bias=False)
)
Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  8.99s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  9.64s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  9.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  9.66s/it]
Done getting peft model
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
trainable params: 125173760.0 || all params: 6922501120 || trainable: 1.8082158143442815
torch.bfloat16 578201600 0.08352495578938961
torch.uint8 6343884800 0.9164151352278871
torch.float32 414720 5.9908982723284845e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 32016}.
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Done getting peft model
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Done getting peft model
trainable params: 125173760.0 || all params: 6922501120 || trainable: 1.8082158143442815
torch.bfloat16 578201600 0.08352495578938961
torch.uint8 6343884800 0.9164151352278871
torch.float32 414720 5.9908982723284845e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 32016}.
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 125173760.0 || all params: 6922501120 || trainable: 1.8082158143442815
torch.bfloat16 578201600 0.08352495578938961
torch.uint8 6343884800 0.9164151352278871
torch.float32 414720 5.9908982723284845e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 32016}.
Done getting peft model
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
loaded model
Adding special tokens.
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'o_proj', 'down_proj', 'gate_proj', 'q_proj', 'v_proj', 'up_proj', 'k_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32017, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32017, bias=False)
)
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 125173760.0 || all params: 6922501120 || trainable: 1.8082158143442815
Adding special tokens.
torch.bfloat16 578201600 0.08352495578938961
torch.uint8 6343884800 0.9164151352278871
torch.float32 414720 5.9908982723284845e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 32016}.
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'v_proj', 'k_proj', 'down_proj', 'gate_proj', 'o_proj', 'q_proj', 'up_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32017, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32017, bias=False)
)
Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00,  9.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.36s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00,  9.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.48s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Adding special tokens.
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'o_proj', 'down_proj', 'q_proj', 'v_proj', 'k_proj', 'gate_proj', 'up_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32017, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32017, bias=False)
)
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Adding special tokens.
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'o_proj', 'q_proj', 'v_proj', 'up_proj', 'down_proj', 'k_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32017, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32017, bias=False)
)
Done getting peft model
Done getting peft model
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
loaded model
trainable params: 125173760.0 || all params: 6922501120 || trainable: 1.8082158143442815
torch.bfloat16 578201600 0.08352495578938961
torch.uint8 6343884800 0.9164151352278871
torch.float32 414720 5.9908982723284845e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 32016}.
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 125173760.0 || all params: 6922501120 || trainable: 1.8082158143442815
torch.bfloat16 578201600 0.08352495578938961
torch.uint8 6343884800 0.9164151352278871
torch.float32 414720 5.9908982723284845e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 32016}.
Done getting peft model
Done getting peft model
loaded model
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 125173760.0 || all params: 6922501120 || trainable: 1.8082158143442815
torch.bfloat16 578201600 0.08352495578938961
torch.uint8 6343884800 0.9164151352278871
torch.float32 414720 5.9908982723284845e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 32016}.
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 125173760.0 || all params: 6922501120 || trainable: 1.8082158143442815
torch.bfloat16 578201600 0.08352495578938961
torch.uint8 6343884800 0.9164151352278871
torch.float32 414720 5.9908982723284845e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 32016}.
  0%|          | 0/500 [00:00<?, ?it/s][rank5]:[W901 06:28:05.943061197 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W901 06:28:05.948158132 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W901 06:28:06.178054979 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W901 06:28:06.440157809 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W901 06:28:06.571382316 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W901 06:28:06.642714335 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W901 06:28:07.541492288 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W901 06:28:07.705552323 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/500 [00:39<5:25:49, 39.18s/it]  0%|          | 2/500 [00:47<2:56:44, 21.29s/it]  1%|          | 3/500 [00:55<2:03:49, 14.95s/it]  1%|          | 4/500 [01:02<1:39:35, 12.05s/it]  1%|          | 5/500 [01:10<1:25:30, 10.37s/it]  1%|          | 6/500 [01:18<1:18:01,  9.48s/it]  1%|▏         | 7/500 [01:31<1:29:17, 10.87s/it]  2%|▏         | 8/500 [01:40<1:23:44, 10.21s/it]  2%|▏         | 9/500 [01:48<1:18:14,  9.56s/it]  2%|▏         | 10/500 [01:56<1:13:54,  9.05s/it]                                                  {'loss': 0.3958, 'grad_norm': 0.05908203125, 'learning_rate': 0.0002, 'epoch': 0.01}
  2%|▏         | 10/500 [01:56<1:13:54,  9.05s/it]  2%|▏         | 11/500 [02:04<1:10:41,  8.67s/it]  2%|▏         | 12/500 [02:12<1:09:16,  8.52s/it]  3%|▎         | 13/500 [02:25<1:19:03,  9.74s/it]  3%|▎         | 14/500 [02:34<1:17:52,  9.61s/it]  3%|▎         | 15/500 [02:42<1:14:27,  9.21s/it]  3%|▎         | 16/500 [02:50<1:11:07,  8.82s/it]  3%|▎         | 17/500 [02:58<1:09:36,  8.65s/it]  4%|▎         | 18/500 [03:05<1:05:12,  8.12s/it]  4%|▍         | 19/500 [03:16<1:11:55,  8.97s/it]  4%|▍         | 20/500 [03:26<1:12:21,  9.04s/it]                                                  {'loss': 0.2602, 'grad_norm': 0.0238037109375, 'learning_rate': 0.0002, 'epoch': 0.03}
  4%|▍         | 20/500 [03:26<1:12:21,  9.04s/it]  4%|▍         | 21/500 [03:34<1:10:35,  8.84s/it]  4%|▍         | 22/500 [03:42<1:07:59,  8.53s/it]  5%|▍         | 23/500 [03:49<1:05:50,  8.28s/it]  5%|▍         | 24/500 [03:57<1:03:23,  7.99s/it]  5%|▌         | 25/500 [04:04<1:00:50,  7.68s/it]  5%|▌         | 26/500 [04:14<1:06:30,  8.42s/it]  5%|▌         | 27/500 [04:22<1:06:32,  8.44s/it]  6%|▌         | 28/500 [04:30<1:05:15,  8.30s/it]  6%|▌         | 29/500 [04:38<1:03:43,  8.12s/it]  6%|▌         | 30/500 [04:46<1:04:07,  8.19s/it]                                                  {'loss': 0.2489, 'grad_norm': 0.0306396484375, 'learning_rate': 0.0002, 'epoch': 0.04}
  6%|▌         | 30/500 [04:46<1:04:07,  8.19s/it]  6%|▌         | 31/500 [04:54<1:03:51,  8.17s/it]  6%|▋         | 32/500 [05:08<1:15:30,  9.68s/it]  7%|▋         | 33/500 [05:17<1:14:26,  9.56s/it]  7%|▋         | 34/500 [05:25<1:11:04,  9.15s/it]  7%|▋         | 35/500 [05:32<1:06:21,  8.56s/it]  7%|▋         | 36/500 [05:40<1:03:36,  8.23s/it]  7%|▋         | 37/500 [05:46<59:49,  7.75s/it]    8%|▊         | 38/500 [05:57<1:05:53,  8.56s/it]  8%|▊         | 39/500 [06:06<1:07:12,  8.75s/it]  8%|▊         | 40/500 [06:14<1:06:26,  8.67s/it]                                                  {'loss': 0.2105, 'grad_norm': 0.039306640625, 'learning_rate': 0.0002, 'epoch': 0.05}
  8%|▊         | 40/500 [06:15<1:06:26,  8.67s/it]  8%|▊         | 41/500 [06:22<1:04:43,  8.46s/it]  8%|▊         | 42/500 [06:31<1:03:57,  8.38s/it]  9%|▊         | 43/500 [06:39<1:03:16,  8.31s/it]  9%|▉         | 44/500 [06:51<1:11:44,  9.44s/it]  9%|▉         | 45/500 [07:01<1:12:18,  9.54s/it]  9%|▉         | 46/500 [07:10<1:11:13,  9.41s/it]  9%|▉         | 47/500 [07:18<1:08:50,  9.12s/it] 10%|▉         | 48/500 [07:26<1:05:38,  8.71s/it] 10%|▉         | 49/500 [07:35<1:05:12,  8.67s/it] 10%|█         | 50/500 [07:44<1:06:51,  8.91s/it]                                                  {'loss': 0.2054, 'grad_norm': 0.054931640625, 'learning_rate': 0.0002, 'epoch': 0.07}
 10%|█         | 50/500 [07:44<1:06:51,  8.91s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 10%|█         | 51/500 [08:08<1:40:41, 13.46s/it] 10%|█         | 52/500 [08:17<1:29:23, 11.97s/it] 11%|█         | 53/500 [08:24<1:20:04, 10.75s/it] 11%|█         | 54/500 [08:32<1:13:19,  9.86s/it] 11%|█         | 55/500 [08:40<1:09:21,  9.35s/it] 11%|█         | 56/500 [08:47<1:03:51,  8.63s/it] 11%|█▏        | 57/500 [09:00<1:13:34,  9.96s/it] 12%|█▏        | 58/500 [09:09<1:11:12,  9.67s/it] 12%|█▏        | 59/500 [09:18<1:08:42,  9.35s/it] 12%|█▏        | 60/500 [09:26<1:06:20,  9.05s/it]                                                  {'loss': 0.153, 'grad_norm': 0.045166015625, 'learning_rate': 0.0002, 'epoch': 0.08}
 12%|█▏        | 60/500 [09:27<1:06:20,  9.05s/it] 12%|█▏        | 61/500 [09:34<1:02:32,  8.55s/it] 12%|█▏        | 62/500 [09:41<58:52,  8.06s/it]   13%|█▎        | 63/500 [09:52<1:06:35,  9.14s/it] 13%|█▎        | 64/500 [10:02<1:07:02,  9.23s/it] 13%|█▎        | 65/500 [10:10<1:05:06,  8.98s/it] 13%|█▎        | 66/500 [10:19<1:03:41,  8.81s/it] 13%|█▎        | 67/500 [10:27<1:02:01,  8.59s/it] 14%|█▎        | 68/500 [10:34<59:36,  8.28s/it]   14%|█▍        | 69/500 [10:44<1:02:29,  8.70s/it] 14%|█▍        | 70/500 [10:54<1:04:51,  9.05s/it]                                                  {'loss': 0.1345, 'grad_norm': 0.0546875, 'learning_rate': 0.0002, 'epoch': 0.09}
 14%|█▍        | 70/500 [10:54<1:04:51,  9.05s/it] 14%|█▍        | 71/500 [11:03<1:04:18,  9.00s/it] 14%|█▍        | 72/500 [11:11<1:03:18,  8.88s/it] 15%|█▍        | 73/500 [11:20<1:02:43,  8.81s/it] 15%|█▍        | 74/500 [11:28<1:00:19,  8.50s/it] 15%|█▌        | 75/500 [11:38<1:03:57,  9.03s/it] 15%|█▌        | 76/500 [11:50<1:10:06,  9.92s/it] 15%|█▌        | 77/500 [11:59<1:08:53,  9.77s/it] 16%|█▌        | 78/500 [12:08<1:07:03,  9.53s/it] 16%|█▌        | 79/500 [12:17<1:04:23,  9.18s/it] 16%|█▌        | 80/500 [12:25<1:03:06,  9.02s/it]                                                  {'loss': 0.115, 'grad_norm': 0.08935546875, 'learning_rate': 0.0002, 'epoch': 0.11}
 16%|█▌        | 80/500 [12:26<1:03:06,  9.02s/it] 16%|█▌        | 81/500 [12:34<1:01:58,  8.87s/it] 16%|█▋        | 82/500 [12:46<1:09:10,  9.93s/it] 17%|█▋        | 83/500 [12:55<1:07:30,  9.71s/it] 17%|█▋        | 84/500 [13:04<1:04:59,  9.37s/it] 17%|█▋        | 85/500 [13:12<1:02:46,  9.08s/it] 17%|█▋        | 86/500 [13:21<1:01:46,  8.95s/it] 17%|█▋        | 87/500 [13:30<1:00:32,  8.79s/it] 18%|█▊        | 88/500 [13:40<1:04:24,  9.38s/it] 18%|█▊        | 89/500 [13:50<1:04:11,  9.37s/it] 18%|█▊        | 90/500 [13:58<1:02:57,  9.21s/it]                                                  {'loss': 0.0849, 'grad_norm': 0.05810546875, 'learning_rate': 0.0002, 'epoch': 0.12}
 18%|█▊        | 90/500 [13:59<1:02:57,  9.21s/it] 18%|█▊        | 91/500 [14:07<1:01:29,  9.02s/it] 18%|█▊        | 92/500 [14:15<1:00:02,  8.83s/it] 19%|█▊        | 93/500 [14:24<59:02,  8.70s/it]   19%|█▉        | 94/500 [14:33<1:00:49,  8.99s/it] 19%|█▉        | 95/500 [14:43<1:02:02,  9.19s/it] 19%|█▉        | 96/500 [14:52<1:01:14,  9.10s/it] 19%|█▉        | 97/500 [15:01<1:00:27,  9.00s/it] 20%|█▉        | 98/500 [15:08<57:37,  8.60s/it]   20%|█▉        | 99/500 [15:17<57:00,  8.53s/it] 20%|██        | 100/500 [15:27<59:37,  8.94s/it]                                                 {'loss': 0.0724, 'grad_norm': 0.0966796875, 'learning_rate': 0.0002, 'epoch': 0.13}
 20%|██        | 100/500 [15:27<59:37,  8.94s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...
Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 20%|██        | 101/500 [15:49<1:26:04, 12.94s/it] 20%|██        | 102/500 [15:58<1:17:57, 11.75s/it] 21%|██        | 103/500 [16:07<1:11:36, 10.82s/it] 21%|██        | 104/500 [16:15<1:07:21, 10.20s/it] 21%|██        | 105/500 [16:24<1:04:18,  9.77s/it] 21%|██        | 106/500 [16:33<1:02:03,  9.45s/it] 21%|██▏       | 107/500 [16:45<1:06:49, 10.20s/it] 22%|██▏       | 108/500 [16:54<1:03:57,  9.79s/it] 22%|██▏       | 109/500 [17:02<1:00:05,  9.22s/it] 22%|██▏       | 110/500 [17:10<57:29,  8.84s/it]                                                   {'loss': 0.0528, 'grad_norm': 0.037353515625, 'learning_rate': 0.0002, 'epoch': 0.15}
 22%|██▏       | 110/500 [17:10<57:29,  8.84s/it] 22%|██▏       | 111/500 [17:18<57:05,  8.81s/it] 22%|██▏       | 112/500 [17:26<54:19,  8.40s/it] 23%|██▎       | 113/500 [17:37<1:00:24,  9.37s/it] 23%|██▎       | 114/500 [17:47<1:00:07,  9.35s/it] 23%|██▎       | 115/500 [17:55<58:30,  9.12s/it]   23%|██▎       | 116/500 [18:03<56:42,  8.86s/it] 23%|██▎       | 117/500 [18:11<54:57,  8.61s/it] 24%|██▎       | 118/500 [18:19<52:59,  8.32s/it] 24%|██▍       | 119/500 [18:29<55:42,  8.77s/it] 24%|██▍       | 120/500 [18:39<57:21,  9.06s/it]                                                 {'loss': 0.0466, 'grad_norm': 0.046630859375, 'learning_rate': 0.0002, 'epoch': 0.16}
 24%|██▍       | 120/500 [18:39<57:21,  9.06s/it] 24%|██▍       | 121/500 [18:47<56:09,  8.89s/it] 24%|██▍       | 122/500 [18:55<54:40,  8.68s/it] 25%|██▍       | 123/500 [19:04<54:33,  8.68s/it] 25%|██▍       | 124/500 [19:13<54:00,  8.62s/it] 25%|██▌       | 125/500 [19:21<52:56,  8.47s/it] 25%|██▌       | 126/500 [19:31<56:46,  9.11s/it] 25%|██▌       | 127/500 [19:40<56:13,  9.04s/it] 26%|██▌       | 128/500 [19:48<54:46,  8.83s/it] 26%|██▌       | 129/500 [19:57<53:06,  8.59s/it] 26%|██▌       | 130/500 [20:05<51:55,  8.42s/it]                                                 {'loss': 0.0365, 'grad_norm': 0.037353515625, 'learning_rate': 0.0002, 'epoch': 0.17}
 26%|██▌       | 130/500 [20:05<51:55,  8.42s/it] 26%|██▌       | 131/500 [20:13<51:55,  8.44s/it] 26%|██▋       | 132/500 [20:25<57:23,  9.36s/it] 27%|██▋       | 133/500 [20:34<56:43,  9.27s/it] 27%|██▋       | 134/500 [20:42<55:03,  9.02s/it] 27%|██▋       | 135/500 [20:50<53:14,  8.75s/it] 27%|██▋       | 136/500 [20:58<51:41,  8.52s/it] 27%|██▋       | 137/500 [21:07<51:17,  8.48s/it] 28%|██▊       | 138/500 [21:17<55:28,  9.20s/it] 28%|██▊       | 139/500 [21:27<55:14,  9.18s/it] 28%|██▊       | 140/500 [21:35<53:51,  8.98s/it]                                                 {'loss': 0.0353, 'grad_norm': 0.059814453125, 'learning_rate': 0.0002, 'epoch': 0.19}
 28%|██▊       | 140/500 [21:35<53:51,  8.98s/it] 28%|██▊       | 141/500 [21:43<51:51,  8.67s/it] 28%|██▊       | 142/500 [21:50<49:05,  8.23s/it] 29%|██▊       | 143/500 [21:57<46:18,  7.78s/it] 29%|██▉       | 144/500 [22:05<47:32,  8.01s/it] 29%|██▉       | 145/500 [22:15<50:32,  8.54s/it] 29%|██▉       | 146/500 [22:24<50:04,  8.49s/it] 29%|██▉       | 147/500 [22:31<48:48,  8.30s/it] 30%|██▉       | 148/500 [22:39<47:35,  8.11s/it] 30%|██▉       | 149/500 [22:46<45:31,  7.78s/it] 30%|███       | 150/500 [22:54<45:17,  7.76s/it]                                                 {'loss': 0.0275, 'grad_norm': 0.08349609375, 'learning_rate': 0.0002, 'epoch': 0.2}
 30%|███       | 150/500 [22:54<45:17,  7.76s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 30%|███       | 151/500 [23:17<1:11:42, 12.33s/it] 30%|███       | 152/500 [23:26<1:05:08, 11.23s/it] 31%|███       | 153/500 [23:34<59:35, 10.31s/it]   31%|███       | 154/500 [23:41<54:31,  9.45s/it] 31%|███       | 155/500 [23:48<50:04,  8.71s/it] 31%|███       | 156/500 [23:55<46:19,  8.08s/it] 31%|███▏      | 157/500 [24:06<51:29,  9.01s/it] 32%|███▏      | 158/500 [24:15<51:50,  9.10s/it] 32%|███▏      | 159/500 [24:23<50:04,  8.81s/it] 32%|███▏      | 160/500 [24:31<48:25,  8.54s/it]                                                 {'loss': 0.0245, 'grad_norm': 0.037109375, 'learning_rate': 0.0002, 'epoch': 0.21}
 32%|███▏      | 160/500 [24:32<48:25,  8.54s/it] 32%|███▏      | 161/500 [24:39<46:29,  8.23s/it] 32%|███▏      | 162/500 [24:46<44:35,  7.92s/it] 33%|███▎      | 163/500 [24:57<49:55,  8.89s/it] 33%|███▎      | 164/500 [25:06<50:24,  9.00s/it] 33%|███▎      | 165/500 [25:15<49:10,  8.81s/it] 33%|███▎      | 166/500 [25:22<46:45,  8.40s/it] 33%|███▎      | 167/500 [25:30<45:19,  8.17s/it] 34%|███▎      | 168/500 [25:38<44:28,  8.04s/it] 34%|███▍      | 169/500 [25:47<46:57,  8.51s/it] 34%|███▍      | 170/500 [25:57<48:31,  8.82s/it]                                                 {'loss': 0.0198, 'grad_norm': 0.04296875, 'learning_rate': 0.0002, 'epoch': 0.23}
 34%|███▍      | 170/500 [25:57<48:31,  8.82s/it] 34%|███▍      | 171/500 [26:06<48:27,  8.84s/it] 34%|███▍      | 172/500 [26:14<47:48,  8.75s/it] 35%|███▍      | 173/500 [26:23<47:32,  8.72s/it] 35%|███▍      | 174/500 [26:30<45:03,  8.29s/it] 35%|███▌      | 175/500 [26:37<42:59,  7.94s/it] 35%|███▌      | 176/500 [26:49<48:55,  9.06s/it] 35%|███▌      | 177/500 [26:58<48:23,  8.99s/it] 36%|███▌      | 178/500 [27:06<47:02,  8.77s/it] 36%|███▌      | 179/500 [27:14<45:50,  8.57s/it] 36%|███▌      | 180/500 [27:22<45:22,  8.51s/it]                                                 {'loss': 0.0137, 'grad_norm': 0.05126953125, 'learning_rate': 0.0002, 'epoch': 0.24}
 36%|███▌      | 180/500 [27:23<45:22,  8.51s/it] 36%|███▌      | 181/500 [27:30<43:10,  8.12s/it] 36%|███▋      | 182/500 [27:41<48:05,  9.07s/it] 37%|███▋      | 183/500 [27:50<48:15,  9.14s/it] 37%|███▋      | 184/500 [28:00<48:55,  9.29s/it] 37%|███▋      | 185/500 [28:10<50:40,  9.65s/it] 37%|███▋      | 186/500 [28:19<49:09,  9.39s/it] 37%|███▋      | 187/500 [28:26<45:37,  8.74s/it] 38%|███▊      | 188/500 [28:37<48:38,  9.35s/it] 38%|███▊      | 189/500 [28:46<48:29,  9.35s/it] 38%|███▊      | 190/500 [28:55<47:20,  9.16s/it]                                                 {'loss': 0.015, 'grad_norm': 0.052001953125, 'learning_rate': 0.0002, 'epoch': 0.25}
 38%|███▊      | 190/500 [28:56<47:20,  9.16s/it] 38%|███▊      | 191/500 [29:03<45:33,  8.85s/it] 38%|███▊      | 192/500 [29:11<43:58,  8.57s/it] 39%|███▊      | 193/500 [29:19<42:13,  8.25s/it] 39%|███▉      | 194/500 [29:29<45:06,  8.85s/it] 39%|███▉      | 195/500 [29:39<46:08,  9.08s/it] 39%|███▉      | 196/500 [29:47<45:33,  8.99s/it] 39%|███▉      | 197/500 [29:55<43:49,  8.68s/it] 40%|███▉      | 198/500 [30:03<41:41,  8.28s/it] 40%|███▉      | 199/500 [30:11<41:29,  8.27s/it] 40%|████      | 200/500 [30:20<41:48,  8.36s/it]                                                 {'loss': 0.0131, 'grad_norm': 0.054931640625, 'learning_rate': 0.0002, 'epoch': 0.27}
 40%|████      | 200/500 [30:20<41:48,  8.36s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a9db433a-2d75-459d-baeb-3d2a80c0a22f)') - silently ignoring the lookup for the file config.json in codellama/CodeLlama-13b-Instruct-hf.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in codellama/CodeLlama-13b-Instruct-hf - will assume that the vocabulary was not modified.
  warnings.warn(
 40%|████      | 201/500 [31:01<1:30:41, 18.20s/it] 40%|████      | 202/500 [31:09<1:16:07, 15.33s/it] 41%|████      | 203/500 [31:17<1:05:02, 13.14s/it] 41%|████      | 204/500 [31:26<58:21, 11.83s/it]   41%|████      | 205/500 [31:35<53:26, 10.87s/it] 41%|████      | 206/500 [31:42<47:27,  9.68s/it] 41%|████▏     | 207/500 [31:53<49:07, 10.06s/it] 42%|████▏     | 208/500 [32:02<47:22,  9.73s/it] 42%|████▏     | 209/500 [32:10<45:08,  9.31s/it] 42%|████▏     | 210/500 [32:19<44:04,  9.12s/it]                                                 {'loss': 0.0139, 'grad_norm': 0.024169921875, 'learning_rate': 0.0002, 'epoch': 0.28}
 42%|████▏     | 210/500 [32:19<44:04,  9.12s/it] 42%|████▏     | 211/500 [32:27<42:45,  8.88s/it] 42%|████▏     | 212/500 [32:35<40:56,  8.53s/it] 43%|████▎     | 213/500 [32:46<45:08,  9.44s/it] 43%|████▎     | 214/500 [32:55<44:33,  9.35s/it] 43%|████▎     | 215/500 [33:04<42:50,  9.02s/it] 43%|████▎     | 216/500 [33:12<41:22,  8.74s/it] 43%|████▎     | 217/500 [33:19<39:14,  8.32s/it] 44%|████▎     | 218/500 [33:26<37:05,  7.89s/it] 44%|████▍     | 219/500 [33:36<39:36,  8.46s/it] 44%|████▍     | 220/500 [33:45<41:14,  8.84s/it]                                                 {'loss': 0.012, 'grad_norm': 0.0303955078125, 'learning_rate': 0.0002, 'epoch': 0.29}
 44%|████▍     | 220/500 [33:46<41:14,  8.84s/it] 44%|████▍     | 221/500 [33:54<41:16,  8.88s/it] 44%|████▍     | 222/500 [34:03<40:18,  8.70s/it] 45%|████▍     | 223/500 [34:10<38:14,  8.28s/it] 45%|████▍     | 224/500 [34:18<37:36,  8.17s/it] 45%|████▌     | 225/500 [34:25<35:57,  7.84s/it] 45%|████▌     | 226/500 [34:37<41:28,  9.08s/it] 45%|████▌     | 227/500 [34:46<40:43,  8.95s/it] 46%|████▌     | 228/500 [34:53<39:01,  8.61s/it] 46%|████▌     | 229/500 [35:02<39:01,  8.64s/it] 46%|████▌     | 230/500 [35:11<38:47,  8.62s/it]                                                 {'loss': 0.0079, 'grad_norm': 0.0458984375, 'learning_rate': 0.0002, 'epoch': 0.31}
 46%|████▌     | 230/500 [35:11<38:47,  8.62s/it] 46%|████▌     | 231/500 [35:18<36:25,  8.12s/it] 46%|████▋     | 232/500 [35:29<41:11,  9.22s/it] 47%|████▋     | 233/500 [35:38<40:50,  9.18s/it] 47%|████▋     | 234/500 [35:47<40:00,  9.02s/it] 47%|████▋     | 235/500 [35:56<39:18,  8.90s/it] 47%|████▋     | 236/500 [36:04<38:12,  8.68s/it] 47%|████▋     | 237/500 [36:12<37:45,  8.61s/it] 48%|████▊     | 238/500 [36:26<44:05, 10.10s/it] 48%|████▊     | 239/500 [36:35<42:55,  9.87s/it] 48%|████▊     | 240/500 [36:43<40:23,  9.32s/it]                                                 {'loss': 0.0102, 'grad_norm': 0.032958984375, 'learning_rate': 0.0002, 'epoch': 0.32}
 48%|████▊     | 240/500 [36:44<40:23,  9.32s/it] 48%|████▊     | 241/500 [36:51<37:58,  8.80s/it] 48%|████▊     | 242/500 [36:59<37:33,  8.74s/it] 49%|████▊     | 243/500 [37:08<37:09,  8.68s/it] 49%|████▉     | 244/500 [37:20<41:07,  9.64s/it] 49%|████▉     | 245/500 [37:30<41:32,  9.77s/it] 49%|████▉     | 246/500 [37:39<40:12,  9.50s/it] 49%|████▉     | 247/500 [37:47<38:33,  9.14s/it] 50%|████▉     | 248/500 [37:56<37:28,  8.92s/it] 50%|████▉     | 249/500 [38:04<36:36,  8.75s/it] 50%|█████     | 250/500 [38:12<35:19,  8.48s/it]                                                 {'loss': 0.0083, 'grad_norm': 0.044677734375, 'learning_rate': 0.0002, 'epoch': 0.33}
 50%|█████     | 250/500 [38:12<35:19,  8.48s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...Saving PEFT checkpoint...Saving PEFT checkpoint...


Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 50%|█████     | 251/500 [38:35<53:28, 12.89s/it] 50%|█████     | 252/500 [38:43<47:45, 11.56s/it] 51%|█████     | 253/500 [38:51<43:09, 10.48s/it] 51%|█████     | 254/500 [38:59<39:48,  9.71s/it] 51%|█████     | 255/500 [39:06<36:18,  8.89s/it] 51%|█████     | 256/500 [39:14<34:26,  8.47s/it] 51%|█████▏    | 257/500 [39:25<37:11,  9.18s/it] 52%|█████▏    | 258/500 [39:34<37:12,  9.23s/it] 52%|█████▏    | 259/500 [39:42<36:13,  9.02s/it] 52%|█████▏    | 260/500 [39:51<35:52,  8.97s/it]                                                 {'loss': 0.0079, 'grad_norm': 0.16796875, 'learning_rate': 0.0002, 'epoch': 0.35}
 52%|█████▏    | 260/500 [39:52<35:52,  8.97s/it] 52%|█████▏    | 261/500 [40:00<35:26,  8.90s/it] 52%|█████▏    | 262/500 [40:09<35:08,  8.86s/it] 53%|█████▎    | 263/500 [40:20<38:12,  9.67s/it] 53%|█████▎    | 264/500 [40:30<38:23,  9.76s/it] 53%|█████▎    | 265/500 [40:40<38:13,  9.76s/it] 53%|█████▎    | 266/500 [40:49<37:30,  9.62s/it] 53%|█████▎    | 267/500 [40:58<36:14,  9.33s/it] 54%|█████▎    | 268/500 [41:07<35:11,  9.10s/it] 54%|█████▍    | 269/500 [41:16<35:37,  9.25s/it] 54%|█████▍    | 270/500 [41:26<36:31,  9.53s/it]                                                 {'loss': 0.0061, 'grad_norm': 0.033935546875, 'learning_rate': 0.0002, 'epoch': 0.36}
 54%|█████▍    | 270/500 [41:27<36:31,  9.53s/it] 54%|█████▍    | 271/500 [41:36<36:02,  9.44s/it] 54%|█████▍    | 272/500 [41:44<35:05,  9.23s/it] 55%|█████▍    | 273/500 [41:53<34:09,  9.03s/it] 55%|█████▍    | 274/500 [42:00<32:20,  8.58s/it] 55%|█████▌    | 275/500 [42:09<31:44,  8.46s/it] 55%|█████▌    | 276/500 [42:21<35:32,  9.52s/it] 55%|█████▌    | 277/500 [42:30<35:00,  9.42s/it] 56%|█████▌    | 278/500 [42:39<34:45,  9.40s/it] 56%|█████▌    | 279/500 [42:48<34:25,  9.35s/it] 56%|█████▌    | 280/500 [42:57<33:27,  9.13s/it]                                                 {'loss': 0.0058, 'grad_norm': 0.037109375, 'learning_rate': 0.0002, 'epoch': 0.37}
 56%|█████▌    | 280/500 [42:57<33:27,  9.13s/it] 56%|█████▌    | 281/500 [43:06<32:58,  9.03s/it] 56%|█████▋    | 282/500 [43:18<36:48, 10.13s/it] 57%|█████▋    | 283/500 [43:28<35:34,  9.84s/it] 57%|█████▋    | 284/500 [43:36<34:09,  9.49s/it] 57%|█████▋    | 285/500 [43:45<33:08,  9.25s/it] 57%|█████▋    | 286/500 [43:54<32:27,  9.10s/it] 57%|█████▋    | 287/500 [44:02<31:48,  8.96s/it] 58%|█████▊    | 288/500 [44:15<35:32, 10.06s/it] 58%|█████▊    | 289/500 [44:24<34:34,  9.83s/it] 58%|█████▊    | 290/500 [44:33<33:17,  9.51s/it]                                                 {'loss': 0.0084, 'grad_norm': 0.050048828125, 'learning_rate': 0.0002, 'epoch': 0.39}
 58%|█████▊    | 290/500 [44:33<33:17,  9.51s/it] 58%|█████▊    | 291/500 [44:41<31:35,  9.07s/it] 58%|█████▊    | 292/500 [44:48<29:26,  8.49s/it] 59%|█████▊    | 293/500 [44:55<27:56,  8.10s/it] 59%|█████▉    | 294/500 [45:07<31:38,  9.22s/it] 59%|█████▉    | 295/500 [45:17<32:00,  9.37s/it] 59%|█████▉    | 296/500 [45:26<31:04,  9.14s/it] 59%|█████▉    | 297/500 [45:34<29:54,  8.84s/it] 60%|█████▉    | 298/500 [45:41<28:19,  8.42s/it] 60%|█████▉    | 299/500 [45:49<27:21,  8.17s/it] 60%|██████    | 300/500 [45:58<28:39,  8.60s/it]                                                 {'loss': 0.0053, 'grad_norm': 0.044189453125, 'learning_rate': 0.0002, 'epoch': 0.4}
 60%|██████    | 300/500 [45:59<28:39,  8.60s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...
Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 60%|██████    | 301/500 [46:21<42:25, 12.79s/it] 60%|██████    | 302/500 [46:29<37:36, 11.40s/it] 61%|██████    | 303/500 [46:37<33:54, 10.33s/it] 61%|██████    | 304/500 [46:45<31:15,  9.57s/it] 61%|██████    | 305/500 [46:52<28:32,  8.78s/it] 61%|██████    | 306/500 [46:59<26:58,  8.34s/it] 61%|██████▏   | 307/500 [47:13<32:35, 10.13s/it] 62%|██████▏   | 308/500 [47:22<31:23,  9.81s/it] 62%|██████▏   | 309/500 [47:30<29:30,  9.27s/it] 62%|██████▏   | 310/500 [47:39<28:18,  8.94s/it]                                                 {'loss': 0.0066, 'grad_norm': 0.0167236328125, 'learning_rate': 0.0002, 'epoch': 0.41}
 62%|██████▏   | 310/500 [47:39<28:18,  8.94s/it] 62%|██████▏   | 311/500 [47:47<27:43,  8.80s/it] 62%|██████▏   | 312/500 [47:55<26:54,  8.59s/it] 63%|██████▎   | 313/500 [48:06<29:15,  9.39s/it] 63%|██████▎   | 314/500 [48:16<29:17,  9.45s/it] 63%|██████▎   | 315/500 [48:25<28:35,  9.27s/it] 63%|██████▎   | 316/500 [48:33<27:10,  8.86s/it] 63%|██████▎   | 317/500 [48:40<25:32,  8.38s/it] 64%|██████▎   | 318/500 [48:48<24:40,  8.13s/it] 64%|██████▍   | 319/500 [48:57<25:25,  8.43s/it] 64%|██████▍   | 320/500 [49:06<26:18,  8.77s/it]                                                 {'loss': 0.005, 'grad_norm': 0.0186767578125, 'learning_rate': 0.0002, 'epoch': 0.43}
 64%|██████▍   | 320/500 [49:07<26:18,  8.77s/it] 64%|██████▍   | 321/500 [49:15<26:04,  8.74s/it] 64%|██████▍   | 322/500 [49:24<25:55,  8.74s/it] 65%|██████▍   | 323/500 [49:33<26:02,  8.83s/it] 65%|██████▍   | 324/500 [49:41<25:54,  8.83s/it] 65%|██████▌   | 325/500 [49:50<25:15,  8.66s/it] 65%|██████▌   | 326/500 [50:02<28:09,  9.71s/it] 65%|██████▌   | 327/500 [50:11<27:26,  9.52s/it] 66%|██████▌   | 328/500 [50:20<26:33,  9.27s/it] 66%|██████▌   | 329/500 [50:28<25:31,  8.96s/it] 66%|██████▌   | 330/500 [50:36<24:15,  8.56s/it]                                                 {'loss': 0.0064, 'grad_norm': 0.006134033203125, 'learning_rate': 0.0002, 'epoch': 0.44}
 66%|██████▌   | 330/500 [50:36<24:15,  8.56s/it] 66%|██████▌   | 331/500 [50:44<23:43,  8.42s/it] 66%|██████▋   | 332/500 [50:55<25:48,  9.22s/it] 67%|██████▋   | 333/500 [51:04<25:48,  9.27s/it] 67%|██████▋   | 334/500 [51:12<24:54,  9.00s/it] 67%|██████▋   | 335/500 [51:21<24:23,  8.87s/it] 67%|██████▋   | 336/500 [51:30<24:21,  8.91s/it] 67%|██████▋   | 337/500 [51:39<23:51,  8.78s/it] 68%|██████▊   | 338/500 [51:50<26:07,  9.68s/it] 68%|██████▊   | 339/500 [52:00<25:37,  9.55s/it] 68%|██████▊   | 340/500 [52:08<24:43,  9.27s/it]                                                 {'loss': 0.0035, 'grad_norm': 0.022705078125, 'learning_rate': 0.0002, 'epoch': 0.45}
 68%|██████▊   | 340/500 [52:08<24:43,  9.27s/it] 68%|██████▊   | 341/500 [52:16<23:24,  8.83s/it] 68%|██████▊   | 342/500 [52:25<23:11,  8.81s/it] 69%|██████▊   | 343/500 [52:33<22:22,  8.55s/it] 69%|██████▉   | 344/500 [52:41<22:18,  8.58s/it] 69%|██████▉   | 345/500 [52:51<23:11,  8.98s/it] 69%|██████▉   | 346/500 [53:00<22:31,  8.77s/it] 69%|██████▉   | 347/500 [53:08<21:59,  8.62s/it] 70%|██████▉   | 348/500 [53:16<21:31,  8.50s/it] 70%|██████▉   | 349/500 [53:25<21:29,  8.54s/it] 70%|███████   | 350/500 [53:34<22:11,  8.88s/it]                                                 {'loss': 0.0026, 'grad_norm': 0.025390625, 'learning_rate': 0.0002, 'epoch': 0.47}
 70%|███████   | 350/500 [53:35<22:11,  8.88s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 70%|███████   | 351/500 [53:59<33:28, 13.48s/it] 70%|███████   | 352/500 [54:07<29:40, 12.03s/it] 71%|███████   | 353/500 [54:16<27:01, 11.03s/it] 71%|███████   | 354/500 [54:25<25:10, 10.34s/it] 71%|███████   | 355/500 [54:33<23:37,  9.78s/it] 71%|███████   | 356/500 [54:42<22:48,  9.50s/it] 71%|███████▏  | 357/500 [54:55<24:54, 10.45s/it] 72%|███████▏  | 358/500 [55:04<23:50, 10.07s/it] 72%|███████▏  | 359/500 [55:13<22:47,  9.70s/it] 72%|███████▏  | 360/500 [55:21<21:50,  9.36s/it]                                                 {'loss': 0.0041, 'grad_norm': 0.019287109375, 'learning_rate': 0.0002, 'epoch': 0.48}
 72%|███████▏  | 360/500 [55:22<21:50,  9.36s/it] 72%|███████▏  | 361/500 [55:30<21:08,  9.12s/it] 72%|███████▏  | 362/500 [55:38<20:36,  8.96s/it] 73%|███████▎  | 363/500 [55:50<22:34,  9.89s/it] 73%|███████▎  | 364/500 [56:00<22:10,  9.78s/it] 73%|███████▎  | 365/500 [56:08<21:07,  9.39s/it] 73%|███████▎  | 366/500 [56:17<20:33,  9.21s/it] 73%|███████▎  | 367/500 [56:26<20:00,  9.03s/it] 74%|███████▎  | 368/500 [56:34<19:31,  8.87s/it] 74%|███████▍  | 369/500 [56:46<21:06,  9.67s/it] 74%|███████▍  | 370/500 [56:56<21:05,  9.73s/it]                                                 {'loss': 0.007, 'grad_norm': 0.015869140625, 'learning_rate': 0.0002, 'epoch': 0.49}
 74%|███████▍  | 370/500 [56:56<21:05,  9.73s/it] 74%|███████▍  | 371/500 [57:05<20:27,  9.52s/it] 74%|███████▍  | 372/500 [57:13<19:48,  9.29s/it] 75%|███████▍  | 373/500 [57:22<19:00,  8.98s/it] 75%|███████▍  | 374/500 [57:29<17:53,  8.52s/it] 75%|███████▌  | 375/500 [57:37<17:06,  8.21s/it] 75%|███████▌  | 376/500 [57:47<18:31,  8.97s/it] 75%|███████▌  | 377/500 [57:56<18:24,  8.98s/it] 76%|███████▌  | 378/500 [58:05<17:55,  8.82s/it] 76%|███████▌  | 379/500 [58:13<17:40,  8.76s/it] 76%|███████▌  | 380/500 [58:22<17:26,  8.72s/it]                                                 {'loss': 0.004, 'grad_norm': 0.017822265625, 'learning_rate': 0.0002, 'epoch': 0.51}
 76%|███████▌  | 380/500 [58:22<17:26,  8.72s/it] 76%|███████▌  | 381/500 [58:31<17:16,  8.71s/it] 76%|███████▋  | 382/500 [58:42<18:51,  9.59s/it] 77%|███████▋  | 383/500 [58:51<18:21,  9.42s/it] 77%|███████▋  | 384/500 [59:00<17:47,  9.21s/it] 77%|███████▋  | 385/500 [59:09<17:16,  9.01s/it] 77%|███████▋  | 386/500 [59:18<17:02,  8.97s/it] 77%|███████▋  | 387/500 [59:26<16:45,  8.90s/it] 78%|███████▊  | 388/500 [59:40<19:04, 10.22s/it] 78%|███████▊  | 389/500 [59:49<18:33, 10.03s/it] 78%|███████▊  | 390/500 [59:58<17:52,  9.75s/it]                                                 {'loss': 0.0036, 'grad_norm': 0.0228271484375, 'learning_rate': 0.0002, 'epoch': 0.52}
 78%|███████▊  | 390/500 [59:59<17:52,  9.75s/it] 78%|███████▊  | 391/500 [1:00:07<17:02,  9.38s/it] 78%|███████▊  | 392/500 [1:00:15<16:24,  9.11s/it] 79%|███████▊  | 393/500 [1:00:23<15:45,  8.84s/it] 79%|███████▉  | 394/500 [1:00:33<16:11,  9.17s/it] 79%|███████▉  | 395/500 [1:00:43<16:28,  9.41s/it] 79%|███████▉  | 396/500 [1:00:52<16:04,  9.28s/it] 79%|███████▉  | 397/500 [1:01:01<15:45,  9.18s/it] 80%|███████▉  | 398/500 [1:01:10<15:26,  9.08s/it] 80%|███████▉  | 399/500 [1:01:18<14:23,  8.55s/it] 80%|████████  | 400/500 [1:01:25<13:47,  8.27s/it]                                                   {'loss': 0.0025, 'grad_norm': 0.0162353515625, 'learning_rate': 0.0002, 'epoch': 0.53}
 80%|████████  | 400/500 [1:01:26<13:47,  8.27s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 80%|████████  | 401/500 [1:01:46<20:05, 12.17s/it] 80%|████████  | 402/500 [1:01:56<18:22, 11.25s/it] 81%|████████  | 403/500 [1:02:04<16:54, 10.46s/it] 81%|████████  | 404/500 [1:02:13<15:50,  9.90s/it] 81%|████████  | 405/500 [1:02:21<14:49,  9.37s/it] 81%|████████  | 406/500 [1:02:29<14:15,  9.10s/it] 81%|████████▏ | 407/500 [1:02:40<14:46,  9.53s/it] 82%|████████▏ | 408/500 [1:02:49<14:31,  9.48s/it] 82%|████████▏ | 409/500 [1:02:57<13:42,  9.03s/it] 82%|████████▏ | 410/500 [1:03:05<12:57,  8.64s/it]                                                   {'loss': 0.0041, 'grad_norm': 0.0234375, 'learning_rate': 0.0002, 'epoch': 0.55}
 82%|████████▏ | 410/500 [1:03:05<12:57,  8.64s/it] 82%|████████▏ | 411/500 [1:03:13<12:46,  8.61s/it] 82%|████████▏ | 412/500 [1:03:20<11:49,  8.06s/it] 83%|████████▎ | 413/500 [1:03:31<12:39,  8.73s/it] 83%|████████▎ | 414/500 [1:03:40<12:38,  8.82s/it] 83%|████████▎ | 415/500 [1:03:48<12:09,  8.58s/it] 83%|████████▎ | 416/500 [1:03:55<11:37,  8.30s/it] 83%|████████▎ | 417/500 [1:04:03<11:06,  8.03s/it] 84%|████████▎ | 418/500 [1:04:09<10:27,  7.65s/it] 84%|████████▍ | 419/500 [1:04:19<11:11,  8.29s/it] 84%|████████▍ | 420/500 [1:04:29<11:44,  8.81s/it]                                                   {'loss': 0.0048, 'grad_norm': 0.0218505859375, 'learning_rate': 0.0002, 'epoch': 0.56}
 84%|████████▍ | 420/500 [1:04:30<11:44,  8.81s/it] 84%|████████▍ | 421/500 [1:04:38<11:38,  8.84s/it] 84%|████████▍ | 422/500 [1:04:46<11:13,  8.63s/it] 85%|████████▍ | 423/500 [1:04:55<11:03,  8.61s/it] 85%|████████▍ | 424/500 [1:05:03<10:46,  8.50s/it] 85%|████████▌ | 425/500 [1:05:11<10:28,  8.38s/it] 85%|████████▌ | 426/500 [1:05:23<11:42,  9.49s/it] 85%|████████▌ | 427/500 [1:05:33<11:27,  9.42s/it] 86%|████████▌ | 428/500 [1:05:41<10:48,  9.01s/it] 86%|████████▌ | 429/500 [1:05:49<10:31,  8.89s/it] 86%|████████▌ | 430/500 [1:05:56<09:46,  8.38s/it]                                                   {'loss': 0.0018, 'grad_norm': 0.0289306640625, 'learning_rate': 0.0002, 'epoch': 0.57}
 86%|████████▌ | 430/500 [1:05:57<09:46,  8.38s/it] 86%|████████▌ | 431/500 [1:06:03<09:02,  7.87s/it] 86%|████████▋ | 432/500 [1:06:16<10:29,  9.25s/it] 87%|████████▋ | 433/500 [1:06:25<10:29,  9.40s/it] 87%|████████▋ | 434/500 [1:06:34<10:06,  9.19s/it] 87%|████████▋ | 435/500 [1:06:43<09:46,  9.03s/it] 87%|████████▋ | 436/500 [1:06:51<09:27,  8.86s/it] 87%|████████▋ | 437/500 [1:07:00<09:18,  8.86s/it] 88%|████████▊ | 438/500 [1:07:12<10:16,  9.95s/it] 88%|████████▊ | 439/500 [1:07:22<09:57,  9.79s/it] 88%|████████▊ | 440/500 [1:07:31<09:28,  9.47s/it]                                                   {'loss': 0.0023, 'grad_norm': 0.01171875, 'learning_rate': 0.0002, 'epoch': 0.59}
 88%|████████▊ | 440/500 [1:07:31<09:28,  9.47s/it] 88%|████████▊ | 441/500 [1:07:39<09:01,  9.18s/it] 88%|████████▊ | 442/500 [1:07:47<08:26,  8.73s/it] 89%|████████▊ | 443/500 [1:07:54<07:48,  8.22s/it] 89%|████████▉ | 444/500 [1:08:04<08:20,  8.93s/it] 89%|████████▉ | 445/500 [1:08:14<08:27,  9.24s/it] 89%|████████▉ | 446/500 [1:08:23<08:08,  9.05s/it] 89%|████████▉ | 447/500 [1:08:31<07:47,  8.82s/it] 90%|████████▉ | 448/500 [1:08:39<07:18,  8.44s/it] 90%|████████▉ | 449/500 [1:08:47<07:07,  8.39s/it] 90%|█████████ | 450/500 [1:08:59<07:53,  9.48s/it]                                                   {'loss': 0.0057, 'grad_norm': 0.054931640625, 'learning_rate': 0.0002, 'epoch': 0.6}
 90%|█████████ | 450/500 [1:08:59<07:53,  9.48s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 90%|█████████ | 451/500 [1:09:23<11:10, 13.69s/it] 90%|█████████ | 452/500 [1:09:31<09:47, 12.23s/it] 91%|█████████ | 453/500 [1:09:40<08:48, 11.24s/it] 91%|█████████ | 454/500 [1:09:49<08:04, 10.53s/it] 91%|█████████ | 455/500 [1:09:58<07:25,  9.89s/it] 91%|█████████ | 456/500 [1:10:06<06:57,  9.48s/it] 91%|█████████▏| 457/500 [1:10:19<07:27, 10.42s/it] 92%|█████████▏| 458/500 [1:10:28<07:04, 10.11s/it] 92%|█████████▏| 459/500 [1:10:37<06:37,  9.69s/it] 92%|█████████▏| 460/500 [1:10:46<06:14,  9.37s/it]                                                   {'loss': 0.006, 'grad_norm': 0.021240234375, 'learning_rate': 0.0002, 'epoch': 0.61}
 92%|█████████▏| 460/500 [1:10:46<06:14,  9.37s/it] 92%|█████████▏| 461/500 [1:10:54<05:59,  9.21s/it] 92%|█████████▏| 462/500 [1:11:03<05:41,  8.99s/it] 93%|█████████▎| 463/500 [1:11:14<05:55,  9.62s/it] 93%|█████████▎| 464/500 [1:11:23<05:43,  9.54s/it] 93%|█████████▎| 465/500 [1:11:32<05:29,  9.42s/it] 93%|█████████▎| 466/500 [1:11:41<05:09,  9.11s/it] 93%|█████████▎| 467/500 [1:11:49<04:52,  8.85s/it] 94%|█████████▎| 468/500 [1:11:58<04:41,  8.80s/it] 94%|█████████▍| 469/500 [1:12:08<04:47,  9.27s/it] 94%|█████████▍| 470/500 [1:12:18<04:41,  9.39s/it]                                                   {'loss': 0.0045, 'grad_norm': 0.015625, 'learning_rate': 0.0002, 'epoch': 0.63}
 94%|█████████▍| 470/500 [1:12:18<04:41,  9.39s/it] 94%|█████████▍| 471/500 [1:12:26<04:24,  9.13s/it] 94%|█████████▍| 472/500 [1:12:35<04:10,  8.93s/it] 95%|█████████▍| 473/500 [1:12:43<03:58,  8.82s/it] 95%|█████████▍| 474/500 [1:12:52<03:48,  8.79s/it] 95%|█████████▌| 475/500 [1:13:02<03:47,  9.09s/it] 95%|█████████▌| 476/500 [1:13:14<03:59,  9.96s/it] 95%|█████████▌| 477/500 [1:13:23<03:40,  9.58s/it] 96%|█████████▌| 478/500 [1:13:31<03:21,  9.18s/it] 96%|█████████▌| 479/500 [1:13:39<03:08,  9.00s/it] 96%|█████████▌| 480/500 [1:13:47<02:50,  8.51s/it]                                                   {'loss': 0.0017, 'grad_norm': 0.0189208984375, 'learning_rate': 0.0002, 'epoch': 0.64}
 96%|█████████▌| 480/500 [1:13:47<02:50,  8.51s/it] 96%|█████████▌| 481/500 [1:13:55<02:38,  8.36s/it] 96%|█████████▋| 482/500 [1:14:06<02:49,  9.39s/it] 97%|█████████▋| 483/500 [1:14:16<02:38,  9.33s/it] 97%|█████████▋| 484/500 [1:14:24<02:24,  9.03s/it] 97%|█████████▋| 485/500 [1:14:32<02:12,  8.83s/it] 97%|█████████▋| 486/500 [1:14:40<01:59,  8.52s/it] 97%|█████████▋| 487/500 [1:14:48<01:47,  8.26s/it] 98%|█████████▊| 488/500 [1:15:00<01:52,  9.35s/it] 98%|█████████▊| 489/500 [1:15:09<01:43,  9.40s/it] 98%|█████████▊| 490/500 [1:15:18<01:32,  9.22s/it]                                                   {'loss': 0.002, 'grad_norm': 0.0172119140625, 'learning_rate': 0.0002, 'epoch': 0.65}
 98%|█████████▊| 490/500 [1:15:18<01:32,  9.22s/it] 98%|█████████▊| 491/500 [1:15:27<01:21,  9.04s/it] 98%|█████████▊| 492/500 [1:15:35<01:10,  8.86s/it] 99%|█████████▊| 493/500 [1:15:43<00:59,  8.52s/it] 99%|█████████▉| 494/500 [1:15:54<00:55,  9.29s/it] 99%|█████████▉| 495/500 [1:16:04<00:47,  9.49s/it] 99%|█████████▉| 496/500 [1:16:13<00:37,  9.33s/it] 99%|█████████▉| 497/500 [1:16:21<00:26,  8.87s/it]100%|█████████▉| 498/500 [1:16:28<00:16,  8.42s/it]100%|█████████▉| 499/500 [1:16:37<00:08,  8.45s/it]100%|██████████| 500/500 [1:16:46<00:00,  8.61s/it]                                                   {'loss': 0.0045, 'grad_norm': 0.0184326171875, 'learning_rate': 0.0002, 'epoch': 0.67}
100%|██████████| 500/500 [1:16:46<00:00,  8.61s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...Saving PEFT checkpoint...


Saving PEFT checkpoint...Saving PEFT checkpoint...
Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
                                                   {'train_runtime': 4616.4943, 'train_samples_per_second': 13.863, 'train_steps_per_second': 0.108, 'train_loss': 0.04666917187720537, 'epoch': 0.67}
100%|██████████| 500/500 [1:16:56<00:00,  8.61s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...
Saving PEFT checkpoint...

Saving PEFT checkpoint...
100%|██████████| 500/500 [1:16:56<00:00,  9.23s/it]
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Repo card metadata block was not found. Setting CardData to empty.
Saving PEFT checkpoint...
Repo card metadata block was not found. Setting CardData to empty.
***** train metrics *****
  epoch                    =       0.6658
  total_flos               = 2911675620GF
  train_loss               =       0.0467
  train_runtime            =   1:16:56.49
  train_samples_per_second =       13.863
  train_steps_per_second   =        0.108
[rank0]:[W901 07:44:35.219368719 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
