++ date
+ echo STARTING at Mon Sep 1 03:18:22 UTC 2025
STARTING at Mon Sep 1 03:18:22 UTC 2025
+ echo 'GPUs assigned: '
GPUs assigned: 
+ git rev-parse HEAD
5aeea3aede3a3ce45f997234e7a63ac7f4b65912
+ accelerate launch --multi_gpu --num_processes=8 --mixed_precision bf16 qlora.py --model_name_or_path deepseek-ai/deepseek-coder-6.7b-base --dataset=data/train_all_shuffled.jsonl --no_gradient_checkpointing --num_train_epochs 1 --bf16 --output_dir full_output_shuffle_deepseek
+ tee d_shuffle_dp.log
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_deepseek']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 4, LOCAL_RANK: 4
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_deepseek', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=300, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_deepseek/runs/Sep01_03-18-35_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=4, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 4
Local process index: 4
Device: cuda:4
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=4), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 4}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_deepseek']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 3, LOCAL_RANK: 3
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_deepseek']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 6, LOCAL_RANK: 6
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_deepseek']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 5, LOCAL_RANK: 5
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_deepseek', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=300, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_deepseek/runs/Sep01_03-18-35_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=3, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 3
Local process index: 3
Device: cuda:3
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=3), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 3}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_deepseek']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 2, LOCAL_RANK: 2
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_deepseek']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 0, LOCAL_RANK: 0
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_deepseek', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=300, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_deepseek/runs/Sep01_03-18-35_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=6, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 6
Local process index: 6
Device: cuda:6
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=6), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 6}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
1
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_deepseek', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=300, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_deepseek/runs/Sep01_03-18-35_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=5, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 5
Local process index: 5
Device: cuda:5
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=5), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 5}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_deepseek']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 1, LOCAL_RANK: 1
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '1', '--bf16', '--output_dir', 'full_output_shuffle_deepseek']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 7, LOCAL_RANK: 7
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_deepseek', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=300, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_deepseek/runs/Sep01_03-18-36_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=2, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 2
Local process index: 2
Device: cuda:2
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=2), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 2}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_deepseek', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=300, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_deepseek/runs/Sep01_03-18-36_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 0
Local process index: 0
Device: cuda:0
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
---
getting accelerate model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][DEBUG] device_map used: {'': 0}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_deepseek', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=300, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_deepseek/runs/Sep01_03-18-36_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=1, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 1
Local process index: 1
Device: cuda:1
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=1), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 1}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle_deepseek', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=1.0, max_steps=300, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle_deepseek/runs/Sep01_03-18-36_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=7, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 7
Local process index: 7
Device: cuda:7
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=7), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 7}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.15s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.73s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'gate_proj', 'o_proj', 'v_proj', 'k_proj', 'q_proj', 'up_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.74s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.87s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.96s/it]
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'down_proj', 'k_proj', 'gate_proj', 'o_proj', 'q_proj', 'v_proj', 'up_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'v_proj', 'up_proj', 'q_proj', 'gate_proj', 'o_proj', 'k_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'k_proj', 'up_proj', 'q_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'up_proj', 'v_proj', 'gate_proj', 'o_proj', 'down_proj', 'k_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
Done getting peft model
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'v_proj', 'up_proj', 'q_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
loaded model
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.71s/it]
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.80s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'down_proj', 'k_proj', 'o_proj', 'q_proj', 'gate_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
Done getting peft model
loaded model
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'up_proj', 'o_proj', 'v_proj', 'gate_proj', 'q_proj', 'down_proj', 'k_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Done getting peft model
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Done getting peft model
Done getting peft model
loaded model
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Done getting peft model
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Done getting peft model
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Done getting peft model
loaded model
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
  0%|          | 0/300 [00:00<?, ?it/s][rank5]:[W901 03:19:29.104264596 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W901 03:19:30.276267566 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W901 03:19:30.281017290 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W901 03:19:30.409187469 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W901 03:19:30.448555562 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W901 03:19:30.704893788 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W901 03:19:31.309326348 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W901 03:19:31.624556987 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/300 [00:34<2:53:13, 34.76s/it]  1%|          | 2/300 [00:40<1:28:50, 17.89s/it]  1%|          | 3/300 [00:47<1:03:44, 12.88s/it]  1%|▏         | 4/300 [00:54<51:58, 10.54s/it]    2%|▏         | 5/300 [01:01<45:20,  9.22s/it]  2%|▏         | 6/300 [01:08<41:11,  8.41s/it]  2%|▏         | 7/300 [01:16<40:44,  8.34s/it]  3%|▎         | 8/300 [01:22<37:07,  7.63s/it]  3%|▎         | 9/300 [01:28<34:54,  7.20s/it]  3%|▎         | 10/300 [01:34<32:18,  6.68s/it]                                                {'loss': 0.4021, 'grad_norm': 0.038818359375, 'learning_rate': 0.0002, 'epoch': 0.01}
  3%|▎         | 10/300 [01:34<32:18,  6.68s/it]  4%|▎         | 11/300 [01:40<31:38,  6.57s/it]  4%|▍         | 12/300 [01:46<30:15,  6.30s/it]  4%|▍         | 13/300 [01:54<33:08,  6.93s/it]  5%|▍         | 14/300 [02:00<31:34,  6.62s/it]  5%|▌         | 15/300 [02:07<31:36,  6.66s/it]  5%|▌         | 16/300 [02:14<31:36,  6.68s/it]  6%|▌         | 17/300 [02:20<31:28,  6.67s/it]  6%|▌         | 18/300 [02:27<31:22,  6.67s/it]  6%|▋         | 19/300 [02:35<33:09,  7.08s/it]  7%|▋         | 20/300 [02:42<32:28,  6.96s/it]                                                {'loss': 0.274, 'grad_norm': 0.0245361328125, 'learning_rate': 0.0002, 'epoch': 0.03}
  7%|▋         | 20/300 [02:42<32:28,  6.96s/it]  7%|▋         | 21/300 [02:48<31:57,  6.87s/it]  7%|▋         | 22/300 [02:55<31:34,  6.81s/it]  8%|▊         | 23/300 [03:02<31:13,  6.76s/it]  8%|▊         | 24/300 [03:08<30:56,  6.73s/it]  8%|▊         | 25/300 [03:15<30:46,  6.72s/it]  9%|▊         | 26/300 [03:22<30:54,  6.77s/it]  9%|▉         | 27/300 [03:29<30:49,  6.78s/it]  9%|▉         | 28/300 [03:35<30:31,  6.73s/it] 10%|▉         | 29/300 [03:42<30:09,  6.68s/it] 10%|█         | 30/300 [03:49<29:50,  6.63s/it]                                                {'loss': 0.2417, 'grad_norm': 0.02734375, 'learning_rate': 0.0002, 'epoch': 0.04}
 10%|█         | 30/300 [03:49<29:50,  6.63s/it] 10%|█         | 31/300 [03:55<29:33,  6.59s/it] 11%|█         | 32/300 [04:03<31:43,  7.10s/it] 11%|█         | 33/300 [04:10<31:23,  7.05s/it] 11%|█▏        | 34/300 [04:15<28:36,  6.45s/it] 12%|█▏        | 35/300 [04:20<26:33,  6.02s/it] 12%|█▏        | 36/300 [04:25<25:06,  5.71s/it] 12%|█▏        | 37/300 [04:30<24:15,  5.54s/it] 13%|█▎        | 38/300 [04:38<26:48,  6.14s/it] 13%|█▎        | 39/300 [04:45<27:50,  6.40s/it] 13%|█▎        | 40/300 [04:52<28:49,  6.65s/it]                                                {'loss': 0.2072, 'grad_norm': 0.026611328125, 'learning_rate': 0.0002, 'epoch': 0.05}
 13%|█▎        | 40/300 [04:52<28:49,  6.65s/it] 14%|█▎        | 41/300 [04:59<28:56,  6.71s/it] 14%|█▍        | 42/300 [05:05<27:52,  6.48s/it] 14%|█▍        | 43/300 [05:12<28:03,  6.55s/it] 15%|█▍        | 44/300 [05:21<31:11,  7.31s/it] 15%|█▌        | 45/300 [05:28<30:31,  7.18s/it] 15%|█▌        | 46/300 [05:34<28:56,  6.84s/it] 16%|█▌        | 47/300 [05:39<26:47,  6.36s/it] 16%|█▌        | 48/300 [05:44<25:00,  5.96s/it] 16%|█▋        | 49/300 [05:49<23:45,  5.68s/it] 17%|█▋        | 50/300 [05:54<23:08,  5.55s/it]                                                {'loss': 0.2099, 'grad_norm': 0.033447265625, 'learning_rate': 0.0002, 'epoch': 0.07}
 17%|█▋        | 50/300 [05:54<23:08,  5.55s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
 17%|█▋        | 51/300 [06:07<32:26,  7.82s/it] 17%|█▋        | 52/300 [06:14<30:58,  7.49s/it] 18%|█▊        | 53/300 [06:21<29:39,  7.20s/it] 18%|█▊        | 54/300 [06:27<28:58,  7.07s/it] 18%|█▊        | 55/300 [06:33<27:17,  6.68s/it] 19%|█▊        | 56/300 [06:38<25:07,  6.18s/it] 19%|█▉        | 57/300 [06:47<27:57,  6.90s/it] 19%|█▉        | 58/300 [06:53<27:32,  6.83s/it] 20%|█▉        | 59/300 [07:00<27:18,  6.80s/it] 20%|██        | 60/300 [07:07<26:58,  6.74s/it]                                                {'loss': 0.1631, 'grad_norm': 0.039306640625, 'learning_rate': 0.0002, 'epoch': 0.08}
 20%|██        | 60/300 [07:07<26:58,  6.74s/it] 20%|██        | 61/300 [07:12<25:30,  6.41s/it] 21%|██        | 62/300 [07:19<25:53,  6.53s/it] 21%|██        | 63/300 [07:29<29:25,  7.45s/it] 21%|██▏       | 64/300 [07:36<28:52,  7.34s/it] 22%|██▏       | 65/300 [07:42<27:53,  7.12s/it] 22%|██▏       | 66/300 [07:49<26:50,  6.88s/it] 22%|██▏       | 67/300 [07:55<25:27,  6.55s/it] 23%|██▎       | 68/300 [08:01<24:43,  6.39s/it] 23%|██▎       | 69/300 [08:08<25:23,  6.59s/it] 23%|██▎       | 70/300 [08:14<24:45,  6.46s/it]                                                {'loss': 0.1472, 'grad_norm': 0.025390625, 'learning_rate': 0.0002, 'epoch': 0.09}
 23%|██▎       | 70/300 [08:14<24:45,  6.46s/it] 24%|██▎       | 71/300 [08:19<23:15,  6.09s/it] 24%|██▍       | 72/300 [08:25<22:44,  5.98s/it] 24%|██▍       | 73/300 [08:30<22:02,  5.83s/it] 25%|██▍       | 74/300 [08:37<22:52,  6.07s/it] 25%|██▌       | 75/300 [08:43<22:50,  6.09s/it] 25%|██▌       | 76/300 [08:51<24:39,  6.60s/it] 26%|██▌       | 77/300 [08:56<23:18,  6.27s/it] 26%|██▌       | 78/300 [09:01<21:50,  5.90s/it] 26%|██▋       | 79/300 [09:06<20:45,  5.63s/it] 27%|██▋       | 80/300 [09:11<19:59,  5.45s/it]                                                {'loss': 0.1435, 'grad_norm': 0.057861328125, 'learning_rate': 0.0002, 'epoch': 0.11}
 27%|██▋       | 80/300 [09:11<19:59,  5.45s/it] 27%|██▋       | 81/300 [09:17<19:39,  5.39s/it] 27%|██▋       | 82/300 [09:25<22:28,  6.19s/it] 28%|██▊       | 83/300 [09:30<21:51,  6.04s/it] 28%|██▊       | 84/300 [09:36<21:47,  6.05s/it] 28%|██▊       | 85/300 [09:43<22:10,  6.19s/it] 29%|██▊       | 86/300 [09:50<22:34,  6.33s/it] 29%|██▉       | 87/300 [09:56<22:53,  6.45s/it] 29%|██▉       | 88/300 [10:03<23:17,  6.59s/it] 30%|██▉       | 89/300 [10:09<22:29,  6.40s/it] 30%|███       | 90/300 [10:15<22:10,  6.34s/it]                                                {'loss': 0.1047, 'grad_norm': 0.05908203125, 'learning_rate': 0.0002, 'epoch': 0.12}
 30%|███       | 90/300 [10:15<22:10,  6.34s/it] 30%|███       | 91/300 [10:21<20:50,  5.98s/it] 31%|███       | 92/300 [10:26<20:16,  5.85s/it] 31%|███       | 93/300 [10:32<20:35,  5.97s/it] 31%|███▏      | 94/300 [10:39<21:34,  6.28s/it] 32%|███▏      | 95/300 [10:46<22:03,  6.46s/it] 32%|███▏      | 96/300 [10:54<22:54,  6.74s/it] 32%|███▏      | 97/300 [11:00<22:23,  6.62s/it] 33%|███▎      | 98/300 [11:07<22:14,  6.61s/it] 33%|███▎      | 99/300 [11:12<20:59,  6.27s/it] 33%|███▎      | 100/300 [11:19<21:20,  6.40s/it]                                                 {'loss': 0.097, 'grad_norm': 0.07861328125, 'learning_rate': 0.0002, 'epoch': 0.13}
 33%|███▎      | 100/300 [11:19<21:20,  6.40s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
 34%|███▎      | 101/300 [11:33<28:45,  8.67s/it] 34%|███▍      | 102/300 [11:40<26:51,  8.14s/it] 34%|███▍      | 103/300 [11:46<25:07,  7.65s/it] 35%|███▍      | 104/300 [11:53<23:56,  7.33s/it] 35%|███▌      | 105/300 [11:59<22:50,  7.03s/it] 35%|███▌      | 106/300 [12:05<21:21,  6.60s/it] 36%|███▌      | 107/300 [12:13<22:58,  7.14s/it] 36%|███▌      | 108/300 [12:19<21:56,  6.86s/it] 36%|███▋      | 109/300 [12:26<21:29,  6.75s/it] 37%|███▋      | 110/300 [12:32<20:47,  6.57s/it]                                                 {'loss': 0.0658, 'grad_norm': 0.057861328125, 'learning_rate': 0.0002, 'epoch': 0.15}
 37%|███▋      | 110/300 [12:32<20:47,  6.57s/it] 37%|███▋      | 111/300 [12:38<20:34,  6.53s/it] 37%|███▋      | 112/300 [12:45<20:14,  6.46s/it] 38%|███▊      | 113/300 [12:53<21:56,  7.04s/it] 38%|███▊      | 114/300 [12:59<21:10,  6.83s/it] 38%|███▊      | 115/300 [13:06<20:32,  6.66s/it] 39%|███▊      | 116/300 [13:12<20:30,  6.69s/it] 39%|███▉      | 117/300 [13:18<19:38,  6.44s/it] 39%|███▉      | 118/300 [13:23<18:26,  6.08s/it] 40%|███▉      | 119/300 [13:31<19:16,  6.39s/it] 40%|████      | 120/300 [13:37<18:49,  6.28s/it]                                                 {'loss': 0.058, 'grad_norm': 0.03955078125, 'learning_rate': 0.0002, 'epoch': 0.16}
 40%|████      | 120/300 [13:37<18:49,  6.28s/it] 40%|████      | 121/300 [13:42<18:09,  6.09s/it] 41%|████      | 122/300 [13:49<18:22,  6.20s/it] 41%|████      | 123/300 [13:54<17:25,  5.90s/it] 41%|████▏     | 124/300 [14:00<17:36,  6.00s/it] 42%|████▏     | 125/300 [14:06<17:05,  5.86s/it] 42%|████▏     | 126/300 [14:12<17:44,  6.12s/it] 42%|████▏     | 127/300 [14:19<18:05,  6.27s/it] 43%|████▎     | 128/300 [14:26<18:24,  6.42s/it] 43%|████▎     | 129/300 [14:32<18:30,  6.49s/it] 43%|████▎     | 130/300 [14:39<18:30,  6.54s/it]                                                 {'loss': 0.0461, 'grad_norm': 0.06005859375, 'learning_rate': 0.0002, 'epoch': 0.17}
 43%|████▎     | 130/300 [14:39<18:30,  6.54s/it] 44%|████▎     | 131/300 [14:45<17:30,  6.22s/it] 44%|████▍     | 132/300 [14:52<18:46,  6.70s/it] 44%|████▍     | 133/300 [14:58<18:03,  6.49s/it] 45%|████▍     | 134/300 [15:04<17:28,  6.32s/it] 45%|████▌     | 135/300 [15:10<17:03,  6.20s/it] 45%|████▌     | 136/300 [15:16<16:34,  6.06s/it] 46%|████▌     | 137/300 [15:22<16:09,  5.95s/it] 46%|████▌     | 138/300 [15:30<17:41,  6.55s/it] 46%|████▋     | 139/300 [15:36<17:18,  6.45s/it] 47%|████▋     | 140/300 [15:41<16:34,  6.21s/it]                                                 {'loss': 0.0447, 'grad_norm': 0.0615234375, 'learning_rate': 0.0002, 'epoch': 0.19}
 47%|████▋     | 140/300 [15:41<16:34,  6.21s/it] 47%|████▋     | 141/300 [15:47<15:59,  6.03s/it] 47%|████▋     | 142/300 [15:53<15:33,  5.91s/it] 48%|████▊     | 143/300 [15:58<15:19,  5.85s/it] 48%|████▊     | 144/300 [16:05<16:09,  6.22s/it] 48%|████▊     | 145/300 [16:13<17:03,  6.60s/it] 49%|████▊     | 146/300 [16:20<17:10,  6.69s/it] 49%|████▉     | 147/300 [16:26<16:28,  6.46s/it] 49%|████▉     | 148/300 [16:33<16:37,  6.56s/it] 50%|████▉     | 149/300 [16:40<16:48,  6.68s/it] 50%|█████     | 150/300 [16:46<16:14,  6.50s/it]                                                 {'loss': 0.0347, 'grad_norm': 0.057861328125, 'learning_rate': 0.0002, 'epoch': 0.2}
 50%|█████     | 150/300 [16:46<16:14,  6.50s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...
Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
 50%|█████     | 151/300 [16:59<21:10,  8.53s/it] 51%|█████     | 152/300 [17:06<19:38,  7.96s/it] 51%|█████     | 153/300 [17:12<18:30,  7.55s/it] 51%|█████▏    | 154/300 [17:19<17:36,  7.23s/it] 52%|█████▏    | 155/300 [17:24<16:01,  6.63s/it] 52%|█████▏    | 156/300 [17:29<14:47,  6.16s/it] 52%|█████▏    | 157/300 [17:37<15:46,  6.62s/it] 53%|█████▎    | 158/300 [17:43<15:25,  6.52s/it] 53%|█████▎    | 159/300 [17:49<15:14,  6.49s/it] 53%|█████▎    | 160/300 [17:55<14:23,  6.17s/it]                                                 {'loss': 0.0306, 'grad_norm': 0.04052734375, 'learning_rate': 0.0002, 'epoch': 0.21}
 53%|█████▎    | 160/300 [17:55<14:23,  6.17s/it] 54%|█████▎    | 161/300 [18:01<14:32,  6.28s/it] 54%|█████▍    | 162/300 [18:08<14:34,  6.34s/it] 54%|█████▍    | 163/300 [18:16<15:43,  6.89s/it] 55%|█████▍    | 164/300 [18:23<15:30,  6.84s/it] 55%|█████▌    | 165/300 [18:28<14:22,  6.39s/it] 55%|█████▌    | 166/300 [18:33<13:24,  6.00s/it] 56%|█████▌    | 167/300 [18:40<13:36,  6.14s/it] 56%|█████▌    | 168/300 [18:45<12:58,  5.90s/it] 56%|█████▋    | 169/300 [18:52<13:42,  6.28s/it] 57%|█████▋    | 170/300 [18:59<13:51,  6.40s/it]                                                 {'loss': 0.0275, 'grad_norm': 0.039794921875, 'learning_rate': 0.0002, 'epoch': 0.23}
 57%|█████▋    | 170/300 [18:59<13:51,  6.40s/it] 57%|█████▋    | 171/300 [19:05<13:55,  6.48s/it] 57%|█████▋    | 172/300 [19:12<13:57,  6.55s/it] 58%|█████▊    | 173/300 [19:18<13:21,  6.31s/it] 58%|█████▊    | 174/300 [19:23<12:31,  5.96s/it] 58%|█████▊    | 175/300 [19:29<12:15,  5.88s/it] 59%|█████▊    | 176/300 [19:36<13:18,  6.44s/it] 59%|█████▉    | 177/300 [19:43<13:02,  6.36s/it] 59%|█████▉    | 178/300 [19:48<12:33,  6.18s/it] 60%|█████▉    | 179/300 [19:55<12:39,  6.27s/it] 60%|██████    | 180/300 [20:02<12:46,  6.39s/it]                                                 {'loss': 0.0189, 'grad_norm': 0.043212890625, 'learning_rate': 0.0002, 'epoch': 0.24}
 60%|██████    | 180/300 [20:02<12:46,  6.39s/it] 60%|██████    | 181/300 [20:08<12:54,  6.51s/it] 61%|██████    | 182/300 [20:16<13:33,  6.89s/it] 61%|██████    | 183/300 [20:23<13:16,  6.81s/it] 61%|██████▏   | 184/300 [20:30<13:15,  6.86s/it] 62%|██████▏   | 185/300 [20:36<13:02,  6.80s/it] 62%|██████▏   | 186/300 [20:43<12:38,  6.66s/it] 62%|██████▏   | 187/300 [20:49<12:32,  6.66s/it] 63%|██████▎   | 188/300 [20:57<13:00,  6.97s/it] 63%|██████▎   | 189/300 [21:04<12:43,  6.87s/it] 63%|██████▎   | 190/300 [21:09<11:37,  6.34s/it]                                                 {'loss': 0.0217, 'grad_norm': 0.0361328125, 'learning_rate': 0.0002, 'epoch': 0.25}
 63%|██████▎   | 190/300 [21:09<11:37,  6.34s/it] 64%|██████▎   | 191/300 [21:14<10:49,  5.96s/it] 64%|██████▍   | 192/300 [21:19<10:16,  5.70s/it] 64%|██████▍   | 193/300 [21:24<09:49,  5.51s/it] 65%|██████▍   | 194/300 [21:31<10:33,  5.98s/it] 65%|██████▌   | 195/300 [21:37<10:29,  6.00s/it] 65%|██████▌   | 196/300 [21:42<09:56,  5.74s/it] 66%|██████▌   | 197/300 [21:47<09:29,  5.52s/it] 66%|██████▌   | 198/300 [21:53<09:17,  5.47s/it] 66%|██████▋   | 199/300 [21:58<09:16,  5.51s/it] 67%|██████▋   | 200/300 [22:05<09:33,  5.74s/it]                                                 {'loss': 0.0142, 'grad_norm': 0.03662109375, 'learning_rate': 0.0002, 'epoch': 0.27}
 67%|██████▋   | 200/300 [22:05<09:33,  5.74s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
 67%|██████▋   | 201/300 [22:18<13:06,  7.94s/it] 67%|██████▋   | 202/300 [22:24<12:09,  7.45s/it] 68%|██████▊   | 203/300 [22:30<11:09,  6.90s/it] 68%|██████▊   | 204/300 [22:35<10:21,  6.47s/it] 68%|██████▊   | 205/300 [22:40<09:45,  6.17s/it] 69%|██████▊   | 206/300 [22:47<09:43,  6.21s/it] 69%|██████▉   | 207/300 [22:54<10:17,  6.64s/it] 69%|██████▉   | 208/300 [23:01<10:05,  6.58s/it] 70%|██████▉   | 209/300 [23:07<09:56,  6.56s/it] 70%|███████   | 210/300 [23:14<09:44,  6.49s/it]                                                 {'loss': 0.0173, 'grad_norm': 0.0311279296875, 'learning_rate': 0.0002, 'epoch': 0.28}
 70%|███████   | 210/300 [23:14<09:44,  6.49s/it] 70%|███████   | 211/300 [23:19<09:04,  6.12s/it] 71%|███████   | 212/300 [23:25<09:08,  6.24s/it] 71%|███████   | 213/300 [23:33<09:38,  6.65s/it] 71%|███████▏  | 214/300 [23:39<09:15,  6.46s/it] 72%|███████▏  | 215/300 [23:45<08:49,  6.23s/it] 72%|███████▏  | 216/300 [23:52<09:06,  6.50s/it] 72%|███████▏  | 217/300 [23:59<09:15,  6.69s/it] 73%|███████▎  | 218/300 [24:06<09:19,  6.83s/it] 73%|███████▎  | 219/300 [24:14<09:39,  7.16s/it] 73%|███████▎  | 220/300 [24:21<09:23,  7.04s/it]                                                 {'loss': 0.014, 'grad_norm': 0.0289306640625, 'learning_rate': 0.0002, 'epoch': 0.29}
 73%|███████▎  | 220/300 [24:21<09:23,  7.04s/it] 74%|███████▎  | 221/300 [24:27<08:53,  6.75s/it] 74%|███████▍  | 222/300 [24:33<08:20,  6.41s/it] 74%|███████▍  | 223/300 [24:39<08:07,  6.33s/it] 75%|███████▍  | 224/300 [24:44<07:42,  6.09s/it] 75%|███████▌  | 225/300 [24:50<07:29,  5.99s/it] 75%|███████▌  | 226/300 [24:58<08:06,  6.58s/it] 76%|███████▌  | 227/300 [25:03<07:31,  6.18s/it] 76%|███████▌  | 228/300 [25:08<07:04,  5.89s/it] 76%|███████▋  | 229/300 [25:14<06:45,  5.72s/it] 77%|███████▋  | 230/300 [25:19<06:26,  5.52s/it]                                                 {'loss': 0.0114, 'grad_norm': 0.031982421875, 'learning_rate': 0.0002, 'epoch': 0.31}
 77%|███████▋  | 230/300 [25:19<06:26,  5.52s/it] 77%|███████▋  | 231/300 [25:24<06:12,  5.40s/it] 77%|███████▋  | 232/300 [25:32<07:08,  6.30s/it] 78%|███████▊  | 233/300 [25:39<07:08,  6.40s/it] 78%|███████▊  | 234/300 [25:44<06:35,  5.99s/it] 78%|███████▊  | 235/300 [25:49<06:11,  5.72s/it] 79%|███████▊  | 236/300 [25:55<06:15,  5.86s/it] 79%|███████▉  | 237/300 [26:01<06:08,  5.85s/it] 79%|███████▉  | 238/300 [26:11<07:20,  7.11s/it] 80%|███████▉  | 239/300 [26:17<06:52,  6.76s/it] 80%|████████  | 240/300 [26:23<06:22,  6.38s/it]                                                 {'loss': 0.0129, 'grad_norm': 0.08154296875, 'learning_rate': 0.0002, 'epoch': 0.32}
 80%|████████  | 240/300 [26:23<06:22,  6.38s/it] 80%|████████  | 241/300 [26:28<05:53,  5.99s/it] 81%|████████  | 242/300 [26:34<05:51,  6.06s/it] 81%|████████  | 243/300 [26:41<05:56,  6.26s/it] 81%|████████▏ | 244/300 [26:49<06:33,  7.03s/it] 82%|████████▏ | 245/300 [26:56<06:25,  7.01s/it] 82%|████████▏ | 246/300 [27:02<05:57,  6.63s/it] 82%|████████▏ | 247/300 [27:08<05:41,  6.44s/it] 83%|████████▎ | 248/300 [27:14<05:31,  6.37s/it] 83%|████████▎ | 249/300 [27:20<05:11,  6.11s/it] 83%|████████▎ | 250/300 [27:25<04:55,  5.92s/it]                                                 {'loss': 0.0118, 'grad_norm': 0.046630859375, 'learning_rate': 0.0002, 'epoch': 0.33}
 83%|████████▎ | 250/300 [27:25<04:55,  5.92s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
 84%|████████▎ | 251/300 [27:38<06:35,  8.08s/it] 84%|████████▍ | 252/300 [27:45<06:01,  7.53s/it] 84%|████████▍ | 253/300 [27:50<05:23,  6.88s/it] 85%|████████▍ | 254/300 [27:55<04:53,  6.37s/it] 85%|████████▌ | 255/300 [28:01<04:32,  6.05s/it] 85%|████████▌ | 256/300 [28:07<04:37,  6.31s/it] 86%|████████▌ | 257/300 [28:15<04:53,  6.83s/it] 86%|████████▌ | 258/300 [28:22<04:48,  6.86s/it] 86%|████████▋ | 259/300 [28:29<04:41,  6.88s/it] 87%|████████▋ | 260/300 [28:36<04:34,  6.85s/it]                                                 {'loss': 0.0119, 'grad_norm': 0.04833984375, 'learning_rate': 0.0002, 'epoch': 0.35}
 87%|████████▋ | 260/300 [28:36<04:34,  6.85s/it] 87%|████████▋ | 261/300 [28:43<04:26,  6.83s/it] 87%|████████▋ | 262/300 [28:50<04:17,  6.78s/it] 88%|████████▊ | 263/300 [28:58<04:32,  7.37s/it] 88%|████████▊ | 264/300 [29:05<04:17,  7.16s/it] 88%|████████▊ | 265/300 [29:11<03:59,  6.83s/it] 89%|████████▊ | 266/300 [29:16<03:34,  6.31s/it] 89%|████████▉ | 267/300 [29:21<03:17,  6.00s/it] 89%|████████▉ | 268/300 [29:26<03:02,  5.71s/it] 90%|████████▉ | 269/300 [29:33<03:02,  5.90s/it] 90%|█████████ | 270/300 [29:40<03:08,  6.27s/it]                                                 {'loss': 0.008, 'grad_norm': 0.0233154296875, 'learning_rate': 0.0002, 'epoch': 0.36}
 90%|█████████ | 270/300 [29:40<03:08,  6.27s/it] 90%|█████████ | 271/300 [29:45<02:52,  5.96s/it] 91%|█████████ | 272/300 [29:52<02:52,  6.16s/it] 91%|█████████ | 273/300 [29:57<02:38,  5.86s/it] 91%|█████████▏| 274/300 [30:02<02:28,  5.70s/it] 92%|█████████▏| 275/300 [30:10<02:36,  6.26s/it] 92%|█████████▏| 276/300 [30:18<02:43,  6.80s/it] 92%|█████████▏| 277/300 [30:24<02:28,  6.44s/it] 93%|█████████▎| 278/300 [30:29<02:16,  6.22s/it] 93%|█████████▎| 279/300 [30:36<02:13,  6.34s/it] 93%|█████████▎| 280/300 [30:42<02:06,  6.30s/it]                                                 {'loss': 0.0068, 'grad_norm': 0.0263671875, 'learning_rate': 0.0002, 'epoch': 0.37}
 93%|█████████▎| 280/300 [30:42<02:06,  6.30s/it] 94%|█████████▎| 281/300 [30:48<02:00,  6.33s/it] 94%|█████████▍| 282/300 [30:57<02:05,  6.96s/it] 94%|█████████▍| 283/300 [31:03<01:55,  6.79s/it] 95%|█████████▍| 284/300 [31:09<01:45,  6.62s/it] 95%|█████████▌| 285/300 [31:16<01:38,  6.54s/it] 95%|█████████▌| 286/300 [31:22<01:29,  6.37s/it] 96%|█████████▌| 287/300 [31:27<01:19,  6.11s/it] 96%|█████████▌| 288/300 [31:35<01:19,  6.63s/it] 96%|█████████▋| 289/300 [31:41<01:09,  6.33s/it] 97%|█████████▋| 290/300 [31:46<01:00,  6.06s/it]                                                 {'loss': 0.0082, 'grad_norm': 0.0322265625, 'learning_rate': 0.0002, 'epoch': 0.39}
 97%|█████████▋| 290/300 [31:46<01:00,  6.06s/it] 97%|█████████▋| 291/300 [31:52<00:52,  5.88s/it] 97%|█████████▋| 292/300 [31:57<00:45,  5.74s/it] 98%|█████████▊| 293/300 [32:03<00:39,  5.66s/it] 98%|█████████▊| 294/300 [32:12<00:40,  6.80s/it] 98%|█████████▊| 295/300 [32:18<00:33,  6.68s/it] 99%|█████████▊| 296/300 [32:25<00:26,  6.51s/it] 99%|█████████▉| 297/300 [32:30<00:18,  6.11s/it] 99%|█████████▉| 298/300 [32:36<00:12,  6.28s/it]100%|█████████▉| 299/300 [32:43<00:06,  6.23s/it]100%|██████████| 300/300 [32:49<00:00,  6.32s/it]                                                 {'loss': 0.0062, 'grad_norm': 0.034912109375, 'learning_rate': 0.0002, 'epoch': 0.4}
100%|██████████| 300/300 [32:49<00:00,  6.32s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
                                                 {'train_runtime': 1975.3977, 'train_samples_per_second': 19.439, 'train_steps_per_second': 0.152, 'train_loss': 0.08203642323613167, 'epoch': 0.4}
100%|██████████| 300/300 [32:55<00:00,  6.32s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...
100%|██████████| 300/300 [32:55<00:00,  6.58s/it]
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =      0.3995
  total_flos               = 895484772GF
  train_loss               =       0.082
  train_runtime            =  0:32:55.39
  train_samples_per_second =      19.439
  train_steps_per_second   =       0.152
[rank0]:[W901 03:51:56.383238391 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
++ date
+ echo END at Mon Sep 1 03:52:00 UTC 2025
END at Mon Sep 1 03:52:00 UTC 2025
