++ date
+ echo STARTING at Mon Sep 1 00:45:36 UTC 2025
STARTING at Mon Sep 1 00:45:36 UTC 2025
+ echo 'GPUs assigned: '
GPUs assigned: 
+ git rev-parse HEAD
5aeea3aede3a3ce45f997234e7a63ac7f4b65912
+ accelerate launch --multi_gpu --num_processes=8 --mixed_precision bf16 qlora.py --model_name_or_path deepseek-ai/deepseek-coder-6.7b-base --dataset=data/train_all_shuffled.jsonl --no_gradient_checkpointing --num_train_epochs 2 --bf16 --output_dir full_output_shuffle
+ tee d_shuffle.log
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '2', '--bf16', '--output_dir', 'full_output_shuffle']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 2, LOCAL_RANK: 2
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '2', '--bf16', '--output_dir', 'full_output_shuffle']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 0, LOCAL_RANK: 0
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '2', '--bf16', '--output_dir', 'full_output_shuffle']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 7, LOCAL_RANK: 7
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '2', '--bf16', '--output_dir', 'full_output_shuffle']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 4, LOCAL_RANK: 4
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '2', '--bf16', '--output_dir', 'full_output_shuffle']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 6, LOCAL_RANK: 6
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '2', '--bf16', '--output_dir', 'full_output_shuffle']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 3, LOCAL_RANK: 3
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
1
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=2.0, max_steps=10000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle/runs/Sep01_00-45-49_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=2, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 2
Local process index: 2
Device: cuda:2
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=2), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 2}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '2', '--bf16', '--output_dir', 'full_output_shuffle']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 1, LOCAL_RANK: 1
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
Starting qlora...
[DEBUG] sys.argv = ['qlora.py', '--model_name_or_path', 'deepseek-ai/deepseek-coder-6.7b-base', '--dataset=data/train_all_shuffled.jsonl', '--no_gradient_checkpointing', '--num_train_epochs', '2', '--bf16', '--output_dir', 'full_output_shuffle']
Memory Allocated: 0.0 GB
Memory Reserved: 0.0 GB
[DEBUG] RANK: 5, LOCAL_RANK: 5
[DEBUG] torch.cuda.device_count() = 8
[DEBUG] torch.cuda.current_device() = 0
[DEBUG] torch.cuda.get_device_name = NVIDIA A100-SXM4-80GB
1
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=2.0, max_steps=10000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle/runs/Sep01_00-45-49_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 0
Local process index: 0
Device: cuda:0
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 0}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=2.0, max_steps=10000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle/runs/Sep01_00-45-49_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=7, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 7
Local process index: 7
Device: cuda:7
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=7), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 7}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=2.0, max_steps=10000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle/runs/Sep01_00-45-49_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=3, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 3
Local process index: 3
Device: cuda:3
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=3), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 3}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=2.0, max_steps=10000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle/runs/Sep01_00-45-49_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=4, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 4
Local process index: 4
Device: cuda:4
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=4), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 4}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=2.0, max_steps=10000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle/runs/Sep01_00-45-49_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=6, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 6
Local process index: 6
Device: cuda:6
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=6), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 6}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=2.0, max_steps=10000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle/runs/Sep01_00-45-49_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=1, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 1
Local process index: 1
Device: cuda:1
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=1), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 1}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2
3
args:
Namespace(model_name_or_path='deepseek-ai/deepseek-coder-6.7b-base', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=4096, target_max_len=2048, dataset='data/train_all_shuffled.jsonl', dataset_format=None, output_dir='full_output_shuffle', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=2.0, max_steps=10000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='full_output_shuffle/runs/Sep01_00-45-50_163-192-127-101', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=5, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 2048
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 5
Local process index: 5
Device: cuda:5
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=5), deepspeed_plugin=None)
---
getting accelerate model
[DEBUG] device_map used: {'': 5}
max_memory{'': '80000MB'}
loading base model deepseek-ai/deepseek-coder-6.7b-base...
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.75s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.08s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.46s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.49s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.87s/it]
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'k_proj', 'v_proj', 'gate_proj', 'o_proj', 'q_proj', 'up_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
adding LoRA modules...
Done lora config
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.30s/it]LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'up_proj', 'gate_proj', 'o_proj', 'k_proj', 'down_proj', 'q_proj', 'v_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.92s/it]
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.44s/it]
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'gate_proj', 'v_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.33s/it]
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'up_proj', 'gate_proj', 'q_proj', 'v_proj', 'k_proj', 'down_proj', 'o_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.40s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.49s/it]
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'k_proj', 'gate_proj', 'o_proj', 'q_proj', 'v_proj', 'up_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
/home/ubuntu/finetune-qlora/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'gate_proj', 'q_proj', 'o_proj', 'down_proj', 'up_proj', 'k_proj', 'v_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'gate_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'up_proj', 'o_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
Done getting peft model
Done getting peft model
loaded model
loaded model
Generating train split: 0 examples [00:00, ? examples/s]adding LoRA modules...
Done lora config
LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'down_proj', 'o_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'up_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)
)
Generating train split: 4373 examples [00:00, 35536.96 examples/s]Generating train split: 10139 examples [00:00, 23397.35 examples/s]Generating train split: 14638 examples [00:00, 28608.96 examples/s]Done getting peft model
loaded model
Generating train split: 18540 examples [00:00, 30713.87 examples/s]Done getting peft model
loaded model
Generating train split: 23104 examples [00:00, 33085.95 examples/s]Generating train split: 26924 examples [00:00, 32749.69 examples/s]Generating train split: 31167 examples [00:00, 34571.20 examples/s]Done getting peft model
loaded model
Generating train split: 38985 examples [00:01, 35295.87 examples/s]Generating train split: 43407 examples [00:01, 36535.49 examples/s]Done getting peft model
loaded model
Generating train split: 50336 examples [00:01, 33958.73 examples/s]Generating train split: 54746 examples [00:01, 35176.80 examples/s]Done getting peft model
Done getting peft model
loaded model
loaded model
Generating train split: 59074 examples [00:01, 36042.23 examples/s]Generating train split: 67666 examples [00:01, 37549.26 examples/s]Generating train split: 75813 examples [00:02, 36887.62 examples/s]Generating train split: 81863 examples [00:02, 33489.71 examples/s]Generating train split: 85335 examples [00:02, 33023.80 examples/s]Generating train split: 89395 examples [00:02, 33615.82 examples/s]Generating train split: 93304 examples [00:02, 34181.70 examples/s]Generating train split: 97361 examples [00:02, 34615.36 examples/s]Generating train split: 105480 examples [00:03, 25170.30 examples/s]Generating train split: 106805 examples [00:03, 31914.08 examples/s]
Map:   0%|          | 0/96124 [00:00<?, ? examples/s]Map:   0%|          | 0/96124 [00:00<?, ? examples/s]Map:   0%|          | 0/96124 [00:00<?, ? examples/s]Map:   0%|          | 0/96124 [00:00<?, ? examples/s]Map:   0%|          | 0/96124 [00:00<?, ? examples/s]Map:   0%|          | 0/96124 [00:00<?, ? examples/s]Map:   0%|          | 0/96124 [00:00<?, ? examples/s]Map:   0%|          | 0/96124 [00:00<?, ? examples/s]Map:   1%|          | 1034/96124 [00:00<00:09, 10261.11 examples/s]Map:   1%|          | 1064/96124 [00:00<00:09, 10552.03 examples/s]Map:   1%|          | 1096/96124 [00:00<00:08, 10874.89 examples/s]Map:   1%|          | 1019/96124 [00:00<00:09, 10087.48 examples/s]Map:   1%|          | 1054/96124 [00:00<00:09, 10447.03 examples/s]Map:   1%|          | 1000/96124 [00:00<00:10, 9298.40 examples/s]Map:   1%|          | 1035/96124 [00:00<00:09, 10257.78 examples/s]Map:   1%|          | 1020/96124 [00:00<00:09, 10118.83 examples/s]Map:   2%|▏         | 2175/96124 [00:00<00:08, 10929.30 examples/s]Map:   2%|▏         | 2212/96124 [00:00<00:08, 11088.69 examples/s]Map:   2%|▏         | 2265/96124 [00:00<00:08, 11349.85 examples/s]Map:   2%|▏         | 2108/96124 [00:00<00:08, 10552.06 examples/s]Map:   2%|▏         | 2207/96124 [00:00<00:08, 11077.15 examples/s]Map:   2%|▏         | 2185/96124 [00:00<00:08, 10752.64 examples/s]Map:   2%|▏         | 2180/96124 [00:00<00:08, 10952.18 examples/s]Map:   2%|▏         | 2158/96124 [00:00<00:08, 10851.90 examples/s]Map:   4%|▎         | 3453/96124 [00:00<00:07, 11769.41 examples/s]Map:   4%|▎         | 3500/96124 [00:00<00:07, 11902.93 examples/s]Map:   4%|▎         | 3554/96124 [00:00<00:07, 12049.92 examples/s]Map:   3%|▎         | 3354/96124 [00:00<00:08, 11413.93 examples/s]Map:   4%|▎         | 3483/96124 [00:00<00:07, 11837.12 examples/s]Map:   4%|▎         | 3482/96124 [00:00<00:07, 11744.28 examples/s]Map:   4%|▎         | 3457/96124 [00:00<00:07, 11774.93 examples/s]Map:   4%|▎         | 3436/96124 [00:00<00:07, 11729.61 examples/s]Map:   5%|▍         | 4744/96124 [00:00<00:07, 12216.17 examples/s]Map:   5%|▍         | 4766/96124 [00:00<00:07, 12197.21 examples/s]Map:   5%|▌         | 4849/96124 [00:00<00:07, 12401.92 examples/s]Map:   5%|▍         | 4776/96124 [00:00<00:07, 12265.33 examples/s]Map:   5%|▌         | 4808/96124 [00:00<00:07, 12331.96 examples/s]Map:   5%|▍         | 4674/96124 [00:00<00:07, 11873.53 examples/s]Map:   5%|▍         | 4781/96124 [00:00<00:07, 12345.46 examples/s]Map:   5%|▍         | 4732/96124 [00:00<00:07, 12211.11 examples/s]Map:   6%|▌         | 5999/96124 [00:00<00:07, 12332.28 examples/s]Map:   6%|▌         | 6000/96124 [00:00<00:07, 12077.96 examples/s]Map:   6%|▌         | 5894/96124 [00:00<00:07, 11988.26 examples/s]Map:   7%|▋         | 6715/96124 [00:00<00:07, 12416.59 examples/s]Map:   7%|▋         | 6709/96124 [00:00<00:07, 12480.79 examples/s]Map:   7%|▋         | 6702/96124 [00:00<00:07, 12469.50 examples/s]Map:   6%|▌         | 6000/96124 [00:00<00:07, 12180.03 examples/s]Map:   8%|▊         | 7256/96124 [00:00<00:07, 12410.70 examples/s]Map:   7%|▋         | 6713/96124 [00:00<00:07, 12586.75 examples/s]Map:   8%|▊         | 7341/96124 [00:00<00:07, 12522.59 examples/s]Map:   7%|▋         | 7093/96124 [00:00<00:07, 11986.09 examples/s]Map:   8%|▊         | 8000/96124 [00:00<00:07, 12441.35 examples/s]Map:   8%|▊         | 7308/96124 [00:00<00:07, 12479.92 examples/s]Map:   8%|▊         | 8000/96124 [00:00<00:07, 12502.79 examples/s]Map:   8%|▊         | 8000/96124 [00:00<00:07, 12464.84 examples/s]Map:   9%|▉         | 8538/96124 [00:00<00:07, 12499.64 examples/s]Map:   8%|▊         | 8000/96124 [00:00<00:06, 12598.93 examples/s]Map:   9%|▉         | 8698/96124 [00:00<00:06, 12751.21 examples/s]Map:   9%|▊         | 8337/96124 [00:00<00:07, 12129.00 examples/s]Map:   9%|▉         | 8601/96124 [00:00<00:06, 12623.65 examples/s]Map:  10%|█         | 9915/96124 [00:00<00:06, 12565.90 examples/s]Map:  10%|█         | 9895/96124 [00:00<00:06, 12549.88 examples/s]Map:  10%|█         | 9907/96124 [00:00<00:06, 12555.35 examples/s]Map:  10%|█         | 9678/96124 [00:00<00:07, 12079.33 examples/s]Map:  11%|█         | 10297/96124 [00:00<00:07, 12178.59 examples/s]Map:  10%|█         | 9769/96124 [00:00<00:07, 12277.15 examples/s]Map:  11%|█         | 10437/96124 [00:00<00:06, 12275.51 examples/s]Map:  11%|█         | 10399/96124 [00:00<00:06, 12357.27 examples/s]Map:  11%|█▏        | 10896/96124 [00:00<00:07, 12106.67 examples/s]Map:  12%|█▏        | 11594/96124 [00:00<00:06, 12398.99 examples/s]Map:  12%|█▏        | 11779/96124 [00:00<00:06, 12593.51 examples/s]Map:  12%|█▏        | 11800/96124 [00:00<00:06, 12564.54 examples/s]Map:  11%|█▏        | 11000/96124 [00:00<00:07, 12101.29 examples/s]Map:  12%|█▏        | 11750/96124 [00:00<00:06, 12482.03 examples/s]Map:  12%|█▏        | 11797/96124 [00:00<00:06, 12566.51 examples/s]Map:  12%|█▏        | 11695/96124 [00:00<00:06, 12525.83 examples/s]Map:  13%|█▎        | 12326/96124 [00:01<00:06, 12419.33 examples/s]Map:  14%|█▎        | 13005/96124 [00:01<00:06, 12402.97 examples/s]Map:  13%|█▎        | 12780/96124 [00:01<00:06, 12282.51 examples/s]Map:  14%|█▎        | 13179/96124 [00:01<00:07, 11706.59 examples/s]Map:  14%|█▍        | 13697/96124 [00:01<00:06, 12661.26 examples/s]Map:  14%|█▍        | 13718/96124 [00:01<00:06, 12636.68 examples/s]Map:  14%|█▍        | 13700/96124 [00:01<00:06, 12595.90 examples/s]Map:  14%|█▎        | 13000/96124 [00:01<00:06, 12516.95 examples/s]Map:  14%|█▍        | 13615/96124 [00:01<00:06, 12389.16 examples/s]Map:  15%|█▌        | 14849/96124 [00:01<00:06, 12361.32 examples/s]Map:  15%|█▌        | 14686/96124 [00:01<00:06, 12105.68 examples/s]Map:  16%|█▌        | 15000/96124 [00:01<00:06, 11809.41 examples/s]Map:  16%|█▌        | 15472/96124 [00:01<00:06, 12369.82 examples/s]Map:  16%|█▌        | 15472/96124 [00:01<00:06, 12325.51 examples/s]Map:  16%|█▌        | 15466/96124 [00:01<00:06, 12325.57 examples/s]Map:  15%|█▌        | 14731/96124 [00:01<00:06, 12145.02 examples/s]Map:  16%|█▌        | 15332/96124 [00:01<00:06, 12031.39 examples/s]Map:  17%|█▋        | 15954/96124 [00:01<00:06, 12246.90 examples/s]Map:  17%|█▋        | 16300/96124 [00:01<00:06, 12102.61 examples/s]Map:  17%|█▋        | 16775/96124 [00:01<00:06, 12532.90 examples/s]Map:  17%|█▋        | 16776/96124 [00:01<00:06, 12493.73 examples/s]Map:  17%|█▋        | 16766/96124 [00:01<00:06, 12484.39 examples/s]Map:  17%|█▋        | 16765/96124 [00:01<00:06, 12495.45 examples/s]Map:  17%|█▋        | 16000/96124 [00:01<00:06, 12147.71 examples/s]Map:  17%|█▋        | 16587/96124 [00:01<00:06, 12164.69 examples/s]Map:  18%|█▊        | 17596/96124 [00:01<00:06, 12326.72 examples/s]Map:  18%|█▊        | 17268/96124 [00:01<00:06, 12290.09 examples/s]Map:  19%|█▊        | 17787/96124 [00:01<00:06, 12235.85 examples/s]Map:  19%|█▉        | 18730/96124 [00:01<00:06, 12690.73 examples/s]Map:  19%|█▉        | 18730/96124 [00:01<00:06, 12660.12 examples/s]Map:  19%|█▊        | 17853/96124 [00:01<00:06, 12297.03 examples/s]Map:  19%|█▉        | 18701/96124 [00:01<00:06, 12562.70 examples/s]Map:  19%|█▉        | 18710/96124 [00:01<00:06, 12590.03 examples/s]Map:  20%|█▉        | 18943/96124 [00:01<00:06, 12634.84 examples/s]Map:  19%|█▉        | 18594/96124 [00:01<00:06, 12559.01 examples/s]Map:  20%|█▉        | 19019/96124 [00:01<00:06, 12253.01 examples/s]Map:  21%|██        | 20000/96124 [00:01<00:06, 12578.19 examples/s]Map:  20%|█▉        | 19097/96124 [00:01<00:06, 12334.31 examples/s]Map:  21%|██        | 20000/96124 [00:01<00:06, 12531.76 examples/s]Map:  21%|██        | 20000/96124 [00:01<00:06, 12490.05 examples/s]Map:  22%|██▏       | 20714/96124 [00:01<00:05, 12789.86 examples/s]Map:  21%|██        | 19906/96124 [00:01<00:05, 12715.47 examples/s]Map:  21%|██        | 20284/96124 [00:01<00:06, 12356.68 examples/s]Map:  22%|██▏       | 20863/96124 [00:01<00:05, 12691.39 examples/s]Map:  22%|██▏       | 21339/96124 [00:01<00:05, 12783.95 examples/s]Map:  22%|██▏       | 21326/96124 [00:01<00:05, 12711.27 examples/s]Map:  22%|██▏       | 21338/96124 [00:01<00:05, 12712.66 examples/s]Map:  23%|██▎       | 22000/96124 [00:01<00:05, 12756.45 examples/s]Map:  22%|██▏       | 21000/96124 [00:01<00:06, 12307.74 examples/s]Map:  22%|██▏       | 21583/96124 [00:01<00:05, 12527.88 examples/s]Map:  24%|██▎       | 22719/96124 [00:01<00:05, 12999.95 examples/s]Map:  23%|██▎       | 21828/96124 [00:01<00:05, 12748.96 examples/s]Map:  24%|██▎       | 22688/96124 [00:01<00:05, 12822.59 examples/s]Map:  24%|██▎       | 22707/96124 [00:01<00:05, 12874.28 examples/s]Map:  24%|██▎       | 22829/96124 [00:01<00:05, 12833.47 examples/s]Map:  24%|██▍       | 23280/96124 [00:01<00:05, 12764.87 examples/s]Map:  23%|██▎       | 22297/96124 [00:01<00:05, 12478.07 examples/s]Map:  24%|██▍       | 22852/96124 [00:01<00:05, 12570.49 examples/s]Map:  26%|██▌       | 24542/96124 [00:01<00:05, 12689.07 examples/s]Map:  25%|██▍       | 23718/96124 [00:01<00:05, 12693.00 examples/s]Map:  25%|██▌       | 24483/96124 [00:01<00:05, 12511.86 examples/s]Map:  25%|██▌       | 24503/96124 [00:01<00:05, 12545.83 examples/s]Map:  26%|██▌       | 24685/96124 [00:02<00:05, 12610.49 examples/s]Map:  26%|██▌       | 25058/96124 [00:02<00:05, 12440.49 examples/s]Map:  25%|██▌       | 24041/96124 [00:01<00:05, 12170.55 examples/s]Map:  26%|██▌       | 24637/96124 [00:02<00:05, 12133.56 examples/s]Map:  27%|██▋       | 26414/96124 [00:02<00:05, 12719.36 examples/s]Map:  27%|██▋       | 26427/96124 [00:02<00:05, 12645.28 examples/s]Map:  27%|██▋       | 25486/96124 [00:02<00:05, 12385.83 examples/s]Map:  27%|██▋       | 26369/96124 [00:02<00:05, 12530.73 examples/s]Map:  27%|██▋       | 26395/96124 [00:02<00:05, 12565.60 examples/s]Map:  28%|██▊       | 26579/96124 [00:02<00:05, 12614.09 examples/s]Map:  27%|██▋       | 25941/96124 [00:02<00:05, 12334.32 examples/s]Map:  27%|██▋       | 26431/96124 [00:02<00:05, 12071.03 examples/s]Map:  29%|██▉       | 27731/96124 [00:02<00:05, 12836.94 examples/s]Map:  29%|██▉       | 27731/96124 [00:02<00:05, 12739.89 examples/s]Map:  28%|██▊       | 26811/96124 [00:02<00:05, 12592.87 examples/s]Map:  29%|██▉       | 27700/96124 [00:02<00:05, 12678.85 examples/s]Map:  29%|██▉       | 27700/96124 [00:02<00:05, 12635.62 examples/s]Map:  29%|██▉       | 27867/96124 [00:02<00:05, 12674.45 examples/s]Map:  29%|██▉       | 27669/96124 [00:02<00:05, 12141.49 examples/s]Map:  29%|██▉       | 27837/96124 [00:02<00:05, 12431.89 examples/s]Map:  31%|███       | 29712/96124 [00:02<00:05, 12862.89 examples/s]Map:  31%|███       | 29723/96124 [00:02<00:05, 12809.98 examples/s]Map:  30%|██▉       | 28711/96124 [00:02<00:05, 12613.96 examples/s]Map:  31%|███       | 29577/96124 [00:02<00:05, 12619.42 examples/s]Map:  31%|███       | 29574/96124 [00:02<00:05, 12582.81 examples/s]Map:  30%|███       | 28906/96124 [00:02<00:05, 12199.10 examples/s]Map:  31%|███       | 29793/96124 [00:02<00:05, 12725.37 examples/s]Map:  31%|███       | 29717/96124 [00:02<00:05, 12461.14 examples/s]Map:  31%|███       | 30000/96124 [00:02<00:05, 12583.22 examples/s]Map:  32%|███▏      | 30884/96124 [00:02<00:05, 12727.74 examples/s]Map:  32%|███▏      | 30902/96124 [00:02<00:05, 12752.12 examples/s]Map:  33%|███▎      | 31725/96124 [00:02<00:04, 12945.13 examples/s]Map:  33%|███▎      | 31718/96124 [00:02<00:04, 12971.99 examples/s]Map:  32%|███▏      | 30791/96124 [00:02<00:05, 12329.63 examples/s]Map:  33%|███▎      | 31767/96124 [00:02<00:05, 12863.97 examples/s]Map:  32%|███▏      | 31000/96124 [00:02<00:05, 12395.29 examples/s]Map:  33%|███▎      | 31331/96124 [00:02<00:05, 12766.85 examples/s]Map:  34%|███▍      | 32812/96124 [00:02<00:04, 12768.49 examples/s]Map:  34%|███▍      | 32808/96124 [00:02<00:04, 12733.69 examples/s]Map:  35%|███▌      | 33716/96124 [00:02<00:04, 12972.23 examples/s]Map:  35%|███▌      | 33713/96124 [00:02<00:04, 12866.70 examples/s]Map:  34%|███▎      | 32282/96124 [00:02<00:05, 12497.72 examples/s]Map:  34%|███▍      | 32700/96124 [00:02<00:04, 12820.48 examples/s]Map:  34%|███▍      | 32688/96124 [00:02<00:05, 12314.56 examples/s]Map:  35%|███▌      | 33712/96124 [00:02<00:04, 12843.58 examples/s]Map:  35%|███▍      | 33565/96124 [00:02<00:04, 12582.25 examples/s]Map:  36%|███▌      | 34728/96124 [00:02<00:04, 12766.47 examples/s]Map:  36%|███▌      | 34698/96124 [00:02<00:04, 12684.89 examples/s]Map:  35%|███▌      | 33993/96124 [00:02<00:04, 12850.03 examples/s]Map:  36%|███▋      | 35000/96124 [00:02<00:04, 12592.59 examples/s]Map:  37%|███▋      | 35707/96124 [00:02<00:04, 12878.09 examples/s]Map:  37%|███▋      | 35716/96124 [00:02<00:04, 12823.81 examples/s]Map:  36%|███▌      | 34494/96124 [00:02<00:05, 12223.11 examples/s]Map:  36%|███▌      | 34828/96124 [00:02<00:04, 12592.68 examples/s]Map:  38%|███▊      | 36294/96124 [00:02<00:04, 12675.84 examples/s]Map:  38%|███▊      | 36598/96124 [00:02<00:04, 12665.27 examples/s]Map:  38%|███▊      | 36524/96124 [00:02<00:04, 12520.40 examples/s]Map:  37%|███▋      | 35879/96124 [00:02<00:04, 12745.87 examples/s]Map:  38%|███▊      | 37000/96124 [00:02<00:04, 12705.15 examples/s]Map:  39%|███▉      | 37601/96124 [00:02<00:04, 12798.31 examples/s]Map:  38%|███▊      | 36250/96124 [00:02<00:04, 12056.01 examples/s]Map:  39%|███▉      | 37630/96124 [00:03<00:04, 12848.33 examples/s]Map:  38%|███▊      | 36684/96124 [00:02<00:04, 12388.20 examples/s]Map:  39%|███▉      | 37898/96124 [00:03<00:04, 12740.33 examples/s]Map:  39%|███▉      | 37816/96124 [00:03<00:04, 12612.41 examples/s]Map:  40%|███▉      | 38297/96124 [00:03<00:04, 12767.92 examples/s]Map:  40%|████      | 38915/96124 [00:03<00:04, 12873.39 examples/s]Map:  39%|███▉      | 37800/96124 [00:03<00:04, 12764.19 examples/s]Map:  39%|███▉      | 37507/96124 [00:03<00:04, 12174.82 examples/s]Map:  41%|████      | 38945/96124 [00:03<00:04, 12926.94 examples/s]Map:  39%|███▉      | 37953/96124 [00:03<00:04, 12463.05 examples/s]Map:  41%|████      | 39623/96124 [00:03<00:04, 12892.95 examples/s]Map:  41%|████▏     | 39849/96124 [00:03<00:04, 12826.85 examples/s]Map:  41%|████▏     | 39740/96124 [00:03<00:04, 12682.55 examples/s]Map:  40%|████      | 38785/96124 [00:03<00:04, 12322.22 examples/s]Map:  43%|████▎     | 40861/96124 [00:03<00:04, 12903.66 examples/s]Map:  41%|████▏     | 39747/96124 [00:03<00:04, 12832.94 examples/s]Map:  43%|████▎     | 40952/96124 [00:03<00:04, 12999.00 examples/s]Map:  43%|████▎     | 40911/96124 [00:03<00:04, 12988.57 examples/s]Map:  41%|████▏     | 39859/96124 [00:03<00:04, 12545.14 examples/s]Map:  43%|████▎     | 41805/96124 [00:03<00:04, 12894.31 examples/s]Map:  43%|████▎     | 41704/96124 [00:03<00:04, 12741.52 examples/s]Map:  42%|████▏     | 40687/96124 [00:03<00:04, 12429.02 examples/s]Map:  45%|████▍     | 42821/96124 [00:03<00:04, 12951.82 examples/s]Map:  43%|████▎     | 41700/96124 [00:03<00:04, 12858.74 examples/s]Map:  45%|████▍     | 42921/96124 [00:03<00:04, 13042.18 examples/s]Map:  45%|████▍     | 42872/96124 [00:03<00:04, 13013.00 examples/s]Map:  43%|████▎     | 41787/96124 [00:03<00:04, 12644.86 examples/s]Map:  45%|████▍     | 43000/96124 [00:03<00:04, 12660.66 examples/s]Map:  44%|████▎     | 41953/96124 [00:03<00:04, 12484.28 examples/s]Map:  46%|████▌     | 43758/96124 [00:03<00:04, 12929.61 examples/s]Map:  45%|████▍     | 43000/96124 [00:03<00:04, 12761.89 examples/s]Map:  47%|████▋     | 44760/96124 [00:03<00:03, 12941.14 examples/s]Map:  47%|████▋     | 44893/96124 [00:03<00:03, 13075.56 examples/s]Map:  46%|████▌     | 44311/96124 [00:03<00:04, 12771.14 examples/s]Map:  47%|████▋     | 44837/96124 [00:03<00:03, 13039.11 examples/s]Map:  45%|████▌     | 43683/96124 [00:03<00:04, 12600.50 examples/s]Map:  46%|████▌     | 44315/96124 [00:03<00:04, 12854.34 examples/s]Map:  46%|████▌     | 43836/96124 [00:03<00:04, 12505.20 examples/s]Map:  48%|████▊     | 45725/96124 [00:03<00:03, 12983.75 examples/s]Map:  48%|████▊     | 45704/96124 [00:03<00:03, 12950.21 examples/s]Map:  49%|████▊     | 46753/96124 [00:03<00:03, 13044.65 examples/s]Map:  47%|████▋     | 44978/96124 [00:03<00:04, 12681.01 examples/s]Map:  49%|████▉     | 46879/96124 [00:03<00:03, 13127.95 examples/s]Map:  48%|████▊     | 45707/96124 [00:03<00:03, 13011.33 examples/s]Map:  49%|████▊     | 46808/96124 [00:03<00:03, 13067.51 examples/s]Map:  48%|████▊     | 45703/96124 [00:03<00:04, 12422.02 examples/s]Map:  50%|████▉     | 47618/96124 [00:03<00:03, 12869.40 examples/s]Map:  50%|████▉     | 47699/96124 [00:03<00:03, 12916.35 examples/s]Map:  51%|█████     | 48712/96124 [00:03<00:03, 13038.39 examples/s]Map:  49%|████▉     | 46880/96124 [00:03<00:03, 12675.69 examples/s]Map:  51%|█████     | 48838/96124 [00:03<00:03, 13102.46 examples/s]Map:  50%|████▉     | 47702/96124 [00:03<00:03, 12951.37 examples/s]Map:  49%|████▉     | 47000/96124 [00:03<00:03, 12381.80 examples/s]Map:  51%|█████     | 48723/96124 [00:03<00:03, 12971.88 examples/s]Map:  51%|█████     | 48933/96124 [00:03<00:03, 12931.22 examples/s]Map:  51%|█████     | 49000/96124 [00:03<00:03, 12787.04 examples/s]Map:  51%|█████     | 49000/96124 [00:03<00:03, 12835.67 examples/s]Map:  50%|█████     | 48297/96124 [00:03<00:03, 12526.92 examples/s]Map:  53%|█████▎    | 50716/96124 [00:03<00:03, 13120.75 examples/s]Map:  51%|█████     | 48749/96124 [00:03<00:03, 12602.22 examples/s]Map:  53%|█████▎    | 50827/96124 [00:03<00:03, 13148.30 examples/s]Map:  52%|█████▏    | 50317/96124 [00:03<00:03, 12884.61 examples/s]Map:  53%|█████▎    | 50705/96124 [00:04<00:03, 12995.25 examples/s]Map:  53%|█████▎    | 50893/96124 [00:04<00:03, 12970.18 examples/s]Map:  52%|█████▏    | 50322/96124 [00:03<00:03, 12932.69 examples/s]Map:  52%|█████▏    | 49627/96124 [00:04<00:03, 12731.02 examples/s]Map:  54%|█████▍    | 51688/96124 [00:04<00:03, 12953.85 examples/s]Map:  53%|█████▎    | 50691/96124 [00:04<00:03, 12638.69 examples/s]Map:  55%|█████▍    | 52703/96124 [00:04<00:03, 13038.45 examples/s]Map:  55%|█████▍    | 52767/96124 [00:04<00:03, 13077.80 examples/s]Map:  54%|█████▍    | 51694/96124 [00:04<00:03, 13007.25 examples/s]Map:  53%|█████▎    | 50943/96124 [00:04<00:03, 12846.96 examples/s]Map:  55%|█████▍    | 52702/96124 [00:04<00:03, 12964.88 examples/s]Map:  55%|█████▍    | 52819/96124 [00:04<00:03, 12924.70 examples/s]Map:  54%|█████▍    | 51973/96124 [00:04<00:03, 12677.61 examples/s]Map:  56%|█████▌    | 53605/96124 [00:04<00:03, 12884.32 examples/s]Map:  57%|█████▋    | 54593/96124 [00:04<00:03, 12904.59 examples/s]Map:  57%|█████▋    | 54711/96124 [00:04<00:03, 12937.63 examples/s]Map:  56%|█████▌    | 53592/96124 [00:04<00:03, 12873.98 examples/s]Map:  55%|█████▍    | 52852/96124 [00:04<00:03, 12798.80 examples/s]Map:  57%|█████▋    | 54587/96124 [00:04<00:03, 12841.69 examples/s]Map:  57%|█████▋    | 54712/96124 [00:04<00:03, 12826.31 examples/s]Map:  56%|█████▌    | 53821/96124 [00:04<00:03, 12554.51 examples/s]Map:  58%|█████▊    | 55889/96124 [00:04<00:03, 12916.21 examples/s]Map:  58%|█████▊    | 55465/96124 [00:04<00:03, 12715.32 examples/s]Map:  59%|█████▉    | 56717/96124 [00:04<00:03, 12970.72 examples/s]Map:  58%|█████▊    | 55456/96124 [00:04<00:03, 12714.94 examples/s]Map:  57%|█████▋    | 54708/96124 [00:04<00:03, 12650.10 examples/s]Map:  59%|█████▉    | 56494/96124 [00:04<00:03, 12798.58 examples/s]Map:  59%|█████▉    | 56612/96124 [00:04<00:03, 12774.92 examples/s]Map:  59%|█████▉    | 56773/96124 [00:04<00:03, 12804.09 examples/s]Map:  60%|██████    | 57822/96124 [00:04<00:02, 12902.35 examples/s]Map:  58%|█████▊    | 55678/96124 [00:04<00:03, 12388.75 examples/s]Map:  59%|█████▉    | 56757/96124 [00:04<00:03, 12786.01 examples/s]Map:  61%|██████    | 58707/96124 [00:04<00:02, 12846.85 examples/s]Map:  59%|█████▉    | 56602/96124 [00:04<00:03, 12639.19 examples/s]Map:  61%|██████    | 58334/96124 [00:04<00:02, 12635.65 examples/s]Map:  61%|██████    | 58471/96124 [00:04<00:02, 12655.24 examples/s]Map:  59%|█████▉    | 56959/96124 [00:04<00:03, 12485.86 examples/s]Map:  61%|██████    | 58690/96124 [00:04<00:02, 12650.47 examples/s]Map:  62%|██████▏   | 59724/96124 [00:04<00:02, 12827.91 examples/s]Map:  62%|██████▏   | 60000/96124 [00:04<00:02, 12768.12 examples/s]Map:  61%|██████    | 58693/96124 [00:04<00:02, 12652.73 examples/s]Map:  62%|██████▏   | 59700/96124 [00:04<00:02, 12747.03 examples/s]Map:  62%|██████▏   | 59774/96124 [00:04<00:02, 12737.28 examples/s]Map:  61%|██████    | 58375/96124 [00:04<00:03, 12373.87 examples/s]Map:  62%|██████▏   | 59996/96124 [00:04<00:02, 12747.69 examples/s]Map:  61%|██████    | 58736/96124 [00:04<00:03, 12271.73 examples/s]Map:  64%|██████▍   | 61317/96124 [00:04<00:02, 12860.25 examples/s]Map:  62%|██████▏   | 59970/96124 [00:04<00:02, 12679.24 examples/s]Map:  63%|██████▎   | 60999/96124 [00:04<00:02, 12802.59 examples/s]Map:  64%|██████▍   | 61689/96124 [00:04<00:02, 12830.71 examples/s]Map:  62%|██████▏   | 59694/96124 [00:04<00:02, 12533.44 examples/s]Map:  64%|██████▍   | 61696/96124 [00:04<00:02, 12759.96 examples/s]Map:  62%|██████▏   | 60000/96124 [00:04<00:02, 12208.78 examples/s]Map:  65%|██████▌   | 62699/96124 [00:04<00:02, 12881.60 examples/s]Map:  64%|██████▍   | 61941/96124 [00:04<00:02, 12817.88 examples/s]Map:  63%|██████▎   | 60990/96124 [00:04<00:02, 12637.06 examples/s]Map:  64%|██████▍   | 61869/96124 [00:04<00:02, 12668.72 examples/s]Map:  65%|██████▌   | 62870/96124 [00:04<00:02, 12686.10 examples/s]Map:  66%|██████▌   | 63508/96124 [00:04<00:02, 12613.41 examples/s]Map:  64%|██████▎   | 61276/96124 [00:04<00:02, 12345.82 examples/s]Map:  66%|██████▌   | 63453/96124 [00:05<00:02, 12424.15 examples/s]Map:  67%|██████▋   | 64538/96124 [00:05<00:02, 12660.35 examples/s]Map:  66%|██████▋   | 63719/96124 [00:05<00:02, 12496.03 examples/s]Map:  67%|██████▋   | 64821/96124 [00:05<00:02, 12727.96 examples/s]Map:  65%|██████▌   | 62521/96124 [00:05<00:02, 12371.01 examples/s]Map:  65%|██████▌   | 62857/96124 [00:05<00:02, 12567.44 examples/s]Map:  66%|██████▋   | 63698/96124 [00:05<00:02, 12472.64 examples/s]Map:  67%|██████▋   | 64708/96124 [00:05<00:02, 12540.39 examples/s]Map:  67%|██████▋   | 64770/96124 [00:05<00:02, 12593.44 examples/s]Map:  69%|██████▊   | 65848/96124 [00:05<00:02, 12767.59 examples/s]Map:  68%|██████▊   | 65000/96124 [00:05<00:02, 12490.58 examples/s]Map:  68%|██████▊   | 65000/96124 [00:05<00:02, 12444.56 examples/s]Map:  69%|██████▊   | 65995/96124 [00:05<00:02, 12615.55 examples/s]Map:  69%|██████▉   | 66760/96124 [00:05<00:02, 12790.54 examples/s]Map:  67%|██████▋   | 64309/96124 [00:05<00:02, 12205.98 examples/s]Map:  67%|██████▋   | 64693/96124 [00:05<00:02, 12422.88 examples/s]Map:  69%|██████▉   | 66304/96124 [00:05<00:02, 12626.99 examples/s]Map:  69%|██████▉   | 66699/96124 [00:05<00:02, 12646.53 examples/s]Map:  71%|███████   | 67796/96124 [00:05<00:02, 12826.08 examples/s]Map:  69%|██████▉   | 66293/96124 [00:05<00:02, 12564.97 examples/s]Map:  68%|██████▊   | 65567/96124 [00:05<00:02, 12299.14 examples/s]Map:  69%|██████▊   | 65962/96124 [00:05<00:02, 12484.69 examples/s]Map:  70%|███████   | 67602/96124 [00:05<00:02, 12717.97 examples/s]Map:  71%|███████   | 67909/96124 [00:05<00:02, 12659.96 examples/s]Map:  71%|███████▏  | 68699/96124 [00:05<00:02, 12718.16 examples/s]Map:  71%|███████   | 68000/96124 [00:05<00:02, 12441.54 examples/s]Map:  70%|███████   | 67586/96124 [00:05<00:02, 12658.54 examples/s]Map:  73%|███████▎  | 69707/96124 [00:05<00:02, 12647.80 examples/s]Map:  73%|███████▎  | 69976/96124 [00:05<00:02, 12726.45 examples/s]Map:  72%|███████▏  | 69281/96124 [00:05<00:02, 12531.47 examples/s]Map:  70%|███████   | 67408/96124 [00:05<00:02, 12282.99 examples/s]Map:  71%|███████   | 67848/96124 [00:05<00:02, 12511.31 examples/s]Map:  72%|███████▏  | 69419/96124 [00:05<00:02, 12494.35 examples/s]Map:  73%|███████▎  | 69716/96124 [00:05<00:02, 12459.50 examples/s]Map:  74%|███████▍  | 70987/96124 [00:05<00:01, 12682.21 examples/s]Map:  72%|███████▏  | 69385/96124 [00:05<00:02, 12414.67 examples/s]Map:  73%|███████▎  | 70576/96124 [00:05<00:02, 12638.41 examples/s]Map:  71%|███████▏  | 68679/96124 [00:05<00:02, 12217.00 examples/s]Map:  74%|███████▎  | 70679/96124 [00:05<00:02, 12511.44 examples/s]Map:  74%|███████▍  | 70987/96124 [00:05<00:02, 12516.35 examples/s]Map:  75%|███████▍  | 71839/96124 [00:05<00:01, 12621.38 examples/s]Map:  72%|███████▏  | 69677/96124 [00:05<00:02, 12288.48 examples/s]Map:  74%|███████▎  | 70689/96124 [00:05<00:02, 12484.00 examples/s]Map:  76%|███████▌  | 72849/96124 [00:05<00:01, 12585.92 examples/s]Map:  75%|███████▌  | 72410/96124 [00:05<00:01, 12487.51 examples/s]Map:  73%|███████▎  | 70467/96124 [00:05<00:02, 12109.86 examples/s]Map:  74%|███████▍  | 70928/96124 [00:05<00:02, 12339.28 examples/s]Map:  75%|███████▌  | 72533/96124 [00:05<00:01, 12455.49 examples/s]Map:  76%|███████▌  | 72839/96124 [00:05<00:01, 12456.57 examples/s]Map:  77%|███████▋  | 73688/96124 [00:05<00:01, 12522.82 examples/s]Map:  75%|███████▌  | 72543/96124 [00:05<00:01, 12437.27 examples/s]Map:  77%|███████▋  | 73672/96124 [00:05<00:01, 12448.82 examples/s]Map:  75%|███████▍  | 71687/96124 [00:05<00:02, 12129.64 examples/s]Map:  78%|███████▊  | 74711/96124 [00:05<00:01, 12523.23 examples/s]Map:  78%|███████▊  | 74973/96124 [00:05<00:01, 12598.38 examples/s]Map:  76%|███████▌  | 72734/96124 [00:05<00:01, 12235.27 examples/s]Map:  77%|███████▋  | 74370/96124 [00:05<00:01, 12381.25 examples/s]Map:  78%|███████▊  | 74672/96124 [00:05<00:01, 12375.05 examples/s]Map:  78%|███████▊  | 74957/96124 [00:05<00:01, 12555.00 examples/s]Map:  76%|███████▌  | 72943/96124 [00:05<00:01, 12239.49 examples/s]Map:  77%|███████▋  | 74374/96124 [00:05<00:01, 12358.12 examples/s]Map:  79%|███████▉  | 76000/96124 [00:05<00:01, 12417.32 examples/s]Map:  79%|███████▉  | 75699/96124 [00:06<00:01, 12572.17 examples/s]Map:  79%|███████▉  | 75968/96124 [00:06<00:01, 12510.84 examples/s]Map:  80%|███████▉  | 76892/96124 [00:06<00:01, 12660.02 examples/s]Map:  78%|███████▊  | 74536/96124 [00:06<00:01, 12160.28 examples/s]Map:  79%|███████▊  | 75693/96124 [00:06<00:01, 12532.19 examples/s]Map:  80%|████████  | 77322/96124 [00:06<00:01, 12616.51 examples/s]Map:  80%|███████▉  | 76871/96124 [00:06<00:01, 12624.45 examples/s]Map:  78%|███████▊  | 74754/96124 [00:06<00:01, 12175.04 examples/s]Map:  79%|███████▉  | 75808/96124 [00:06<00:01, 12290.01 examples/s]Map:  81%|████████  | 77581/96124 [00:06<00:01, 12560.71 examples/s]Map:  80%|████████  | 76971/96124 [00:06<00:01, 12594.16 examples/s]Map:  81%|████████  | 77878/96124 [00:06<00:01, 12582.85 examples/s]Map:  82%|████████▏ | 78697/96124 [00:06<00:01, 12796.46 examples/s]Map:  82%|████████▏ | 78844/96124 [00:06<00:01, 12771.54 examples/s]Map:  79%|███████▉  | 76000/96124 [00:06<00:01, 12094.32 examples/s]Map:  82%|████████▏ | 78809/96124 [00:06<00:01, 12721.97 examples/s]Map:  82%|████████▏ | 78869/96124 [00:06<00:01, 12636.12 examples/s]Map:  80%|████████  | 77262/96124 [00:06<00:01, 12232.27 examples/s]Map:  81%|████████  | 77682/96124 [00:06<00:01, 12282.51 examples/s]Map:  82%|████████▏ | 78897/96124 [00:06<00:01, 12679.99 examples/s]Map:  83%|████████▎ | 79759/96124 [00:06<00:01, 12567.73 examples/s]Map:  84%|████████▍ | 80552/96124 [00:06<00:01, 12640.46 examples/s]Map:  84%|████████▍ | 80697/96124 [00:06<00:01, 12578.86 examples/s]Map:  84%|████████▍ | 80697/96124 [00:06<00:01, 12654.11 examples/s]Map:  82%|████████▏ | 78525/96124 [00:06<00:01, 12337.85 examples/s]Map:  84%|████████▍ | 80714/96124 [00:06<00:01, 12515.60 examples/s]Map:  82%|████████▏ | 78961/96124 [00:06<00:01, 12402.21 examples/s]Map:  85%|████████▌ | 81845/96124 [00:06<00:01, 12712.39 examples/s]Map:  85%|████████▌ | 82000/96124 [00:06<00:01, 12518.86 examples/s]Map:  84%|████████▍ | 80738/96124 [00:06<00:01, 12538.84 examples/s]Map:  85%|████████▍ | 81702/96124 [00:06<00:01, 12623.32 examples/s]Map:  85%|████████▌ | 81995/96124 [00:06<00:01, 12729.75 examples/s]Map:  85%|████████▌ | 82000/96124 [00:06<00:01, 12441.28 examples/s]Map:  87%|████████▋ | 83300/96124 [00:06<00:01, 12633.51 examples/s]Map:  84%|████████▎ | 80322/96124 [00:06<00:01, 12202.84 examples/s]Map:  84%|████████▍ | 80784/96124 [00:06<00:01, 12314.47 examples/s]Map:  85%|████████▌ | 82000/96124 [00:06<00:01, 12472.42 examples/s]Map:  87%|████████▋ | 83745/96124 [00:06<00:00, 12694.31 examples/s]Map:  86%|████████▋ | 83000/96124 [00:06<00:01, 12537.42 examples/s]Map:  87%|████████▋ | 83290/96124 [00:06<00:01, 12556.97 examples/s]Map:  87%|████████▋ | 83931/96124 [00:06<00:00, 12785.44 examples/s]Map:  88%|████████▊ | 84626/96124 [00:06<00:00, 12794.82 examples/s]Map:  85%|████████▍ | 81593/96124 [00:06<00:01, 12331.95 examples/s]Map:  87%|████████▋ | 83285/96124 [00:06<00:01, 12565.67 examples/s]Map:  88%|████████▊ | 84325/96124 [00:06<00:00, 12709.40 examples/s]Map:  86%|████████▌ | 82683/96124 [00:06<00:01, 12354.47 examples/s]Map:  88%|████████▊ | 84607/96124 [00:06<00:00, 12718.34 examples/s]Map:  89%|████████▉ | 85695/96124 [00:06<00:00, 12717.27 examples/s]Map:  89%|████████▉ | 85914/96124 [00:06<00:00, 12815.51 examples/s]Map:  86%|████████▌ | 82834/96124 [00:06<00:01, 12351.06 examples/s]Map:  88%|████████▊ | 84570/96124 [00:06<00:00, 12638.87 examples/s]Map:  89%|████████▉ | 85691/96124 [00:06<00:00, 12755.46 examples/s]Map:  89%|████████▉ | 85863/96124 [00:06<00:00, 12813.14 examples/s]Map:  87%|████████▋ | 83965/96124 [00:06<00:00, 12461.99 examples/s]Map:  89%|████████▉ | 85889/96124 [00:06<00:00, 12742.27 examples/s]Map:  91%|█████████ | 87488/96124 [00:06<00:00, 12469.03 examples/s]Map:  90%|█████████ | 86974/96124 [00:06<00:00, 12772.78 examples/s]Map:  91%|█████████▏| 87720/96124 [00:06<00:00, 12527.01 examples/s]Map:  88%|████████▊ | 84678/96124 [00:06<00:00, 12327.43 examples/s]Map:  90%|████████▉ | 86406/96124 [00:06<00:00, 12492.81 examples/s]Map:  91%|█████████ | 87695/96124 [00:06<00:00, 12581.03 examples/s]Map:  89%|████████▉ | 85823/96124 [00:06<00:00, 12434.98 examples/s]Map:  91%|█████████ | 87712/96124 [00:06<00:00, 12518.24 examples/s]Map:  92%|█████████▏| 88777/96124 [00:06<00:00, 12565.10 examples/s]Map:  93%|█████████▎| 89000/96124 [00:07<00:00, 12446.91 examples/s]Map:  91%|█████████ | 87677/96124 [00:06<00:00, 12368.38 examples/s]Map:  92%|█████████▏| 88787/96124 [00:07<00:00, 12519.49 examples/s]Map:  93%|█████████▎| 89000/96124 [00:07<00:00, 12565.17 examples/s]Map:  90%|████████▉ | 86472/96124 [00:07<00:00, 12199.83 examples/s]Map:  93%|█████████▎| 89000/96124 [00:07<00:00, 12520.91 examples/s]Map:  94%|█████████▍| 90314/96124 [00:07<00:00, 12629.04 examples/s]Map:  91%|█████████ | 87681/96124 [00:07<00:00, 12204.54 examples/s]Map:  93%|█████████▎| 88959/96124 [00:07<00:00, 12486.11 examples/s]Map:  94%|█████████▍| 90709/96124 [00:07<00:00, 12666.91 examples/s]Map:  94%|█████████▍| 90317/96124 [00:07<00:00, 12712.59 examples/s]Map:  94%|█████████▍| 90306/96124 [00:07<00:00, 12664.95 examples/s]Map:  94%|█████████▍| 90709/96124 [00:07<00:00, 12616.71 examples/s]Map:  92%|█████████▏| 88262/96124 [00:07<00:00, 12051.42 examples/s]Map:  93%|█████████▎| 88927/96124 [00:07<00:00, 12260.98 examples/s]Map:  96%|█████████▌| 91999/96124 [00:07<00:00, 12722.16 examples/s]Map:  95%|█████████▌| 91608/96124 [00:07<00:00, 12761.54 examples/s]Map:  96%|█████████▌| 92180/96124 [00:07<00:00, 12555.09 examples/s]Map:  95%|█████████▌| 91590/96124 [00:07<00:00, 12709.56 examples/s]Map:  95%|█████████▍| 90859/96124 [00:07<00:00, 12547.87 examples/s]Map:  96%|█████████▌| 91998/96124 [00:07<00:00, 12681.50 examples/s]Map:  93%|█████████▎| 89555/96124 [00:07<00:00, 12258.04 examples/s]Map:  97%|█████████▋| 92893/96124 [00:07<00:00, 12785.28 examples/s]Map:  97%|█████████▋| 93466/96124 [00:07<00:00, 12631.67 examples/s]Map:  94%|█████████▍| 90803/96124 [00:07<00:00, 12338.02 examples/s]Map:  98%|█████████▊| 93866/96124 [00:07<00:00, 12625.82 examples/s]Map:  94%|█████████▍| 90822/96124 [00:07<00:00, 12361.34 examples/s]Map:  97%|█████████▋| 93449/96124 [00:07<00:00, 12588.39 examples/s]Map:  96%|█████████▋| 92709/96124 [00:07<00:00, 12471.73 examples/s]Map:  98%|█████████▊| 93884/96124 [00:07<00:00, 12639.85 examples/s]Map:  99%|█████████▊| 94735/96124 [00:07<00:00, 12644.94 examples/s]Map:  99%|█████████▊| 94751/96124 [00:07<00:00, 12636.55 examples/s]Map:  99%|█████████▊| 94717/96124 [00:07<00:00, 12609.89 examples/s]Map:  98%|█████████▊| 93979/96124 [00:07<00:00, 12525.07 examples/s]Map: 100%|█████████▉| 95747/96124 [00:07<00:00, 12595.06 examples/s]Map:  96%|█████████▋| 92683/96124 [00:07<00:00, 12285.75 examples/s]Map:  96%|█████████▋| 92673/96124 [00:07<00:00, 12270.91 examples/s]Map: 100%|█████████▉| 95766/96124 [00:07<00:00, 12606.83 examples/s]Map:  98%|█████████▊| 93947/96124 [00:07<00:00, 12365.30 examples/s]Map:  98%|█████████▊| 93942/96124 [00:07<00:00, 12373.52 examples/s]Map: 100%|█████████▉| 95814/96124 [00:07<00:00, 12422.41 examples/s]Map: 100%|██████████| 96124/96124 [00:07<00:00, 12420.73 examples/s]
Map: 100%|██████████| 96124/96124 [00:07<00:00, 8514.65 examples/s] /home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
Map: 100%|██████████| 96124/96124 [00:07<00:00, 12393.56 examples/s]
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
Map: 100%|█████████▉| 95769/96124 [00:07<00:00, 12290.67 examples/s]Map: 100%|█████████▉| 95682/96124 [00:07<00:00, 12102.19 examples/s]trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Map: 100%|██████████| 96124/96124 [00:07<00:00, 8143.87 examples/s] Map: 100%|██████████| 96124/96124 [00:07<00:00, 12284.73 examples/s]
Map: 100%|██████████| 96124/96124 [00:07<00:00, 8228.26 examples/s] /home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
Map: 100%|██████████| 96124/96124 [00:07<00:00, 12244.40 examples/s]
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Map: 100%|██████████| 96124/96124 [00:07<00:00, 12158.03 examples/s]
Map: 100%|██████████| 96124/96124 [00:07<00:00, 12151.32 examples/s]
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
Map: 100%|██████████| 96124/96124 [00:07<00:00, 12125.49 examples/s]
Map: 100%|██████████| 96124/96124 [00:07<00:00, 12111.08 examples/s]
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/ubuntu/finetune-qlora/qlora.py:762: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
trainable params: 79953920.0 || all params: 3662417920 || trainable: 2.183091109383825
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
torch.bfloat16 424148992 0.11581119393386978
torch.uint8 3238002688 0.8841161109216067
torch.float32 266240 7.269514452353925e-05
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 32014, 'bos_token_id': 32013, 'pad_token_id': 32014}.
  0%|          | 0/10000 [00:00<?, ?it/s][rank7]:[W901 00:47:00.863602743 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W901 00:47:00.009358600 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W901 00:47:01.212796506 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W901 00:47:01.343697742 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W901 00:47:01.362845341 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W901 00:47:01.459259772 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W901 00:47:01.017895455 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W901 00:47:02.511612565 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/10000 [00:42<119:22:25, 42.98s/it]  0%|          | 2/10000 [00:49<59:40:06, 21.48s/it]   0%|          | 3/10000 [00:55<40:25:59, 14.56s/it]  0%|          | 4/10000 [01:01<31:18:13, 11.27s/it]  0%|          | 5/10000 [01:07<25:37:27,  9.23s/it]  0%|          | 6/10000 [01:13<22:03:29,  7.95s/it]  0%|          | 7/10000 [01:21<22:17:49,  8.03s/it]  0%|          | 8/10000 [01:28<21:18:26,  7.68s/it]  0%|          | 9/10000 [01:34<20:28:26,  7.38s/it]  0%|          | 10/10000 [01:41<19:59:32,  7.20s/it]                                                     {'loss': 0.4022, 'grad_norm': 0.03857421875, 'learning_rate': 0.0002, 'epoch': 0.01}
  0%|          | 10/10000 [01:41<19:59:32,  7.20s/it]  0%|          | 11/10000 [01:48<19:37:27,  7.07s/it]  0%|          | 12/10000 [01:55<19:22:28,  6.98s/it]  0%|          | 13/10000 [02:03<20:46:08,  7.49s/it]  0%|          | 14/10000 [02:10<20:20:40,  7.33s/it]  0%|          | 15/10000 [02:17<19:32:53,  7.05s/it]  0%|          | 16/10000 [02:23<19:04:43,  6.88s/it]  0%|          | 17/10000 [02:28<17:35:51,  6.35s/it]  0%|          | 18/10000 [02:35<17:36:48,  6.35s/it]  0%|          | 19/10000 [02:42<18:19:26,  6.61s/it]  0%|          | 20/10000 [02:49<18:25:31,  6.65s/it]                                                     {'loss': 0.274, 'grad_norm': 0.0247802734375, 'learning_rate': 0.0002, 'epoch': 0.03}
  0%|          | 20/10000 [02:49<18:25:31,  6.65s/it]  0%|          | 21/10000 [02:54<17:31:24,  6.32s/it]  0%|          | 22/10000 [03:00<16:53:41,  6.10s/it]  0%|          | 23/10000 [03:06<16:44:32,  6.04s/it]  0%|          | 24/10000 [03:12<16:35:00,  5.98s/it]  0%|          | 25/10000 [03:18<16:55:49,  6.11s/it]  0%|          | 26/10000 [03:25<17:21:10,  6.26s/it]  0%|          | 27/10000 [03:30<16:47:35,  6.06s/it]  0%|          | 28/10000 [03:37<17:05:48,  6.17s/it]  0%|          | 29/10000 [03:43<17:13:32,  6.22s/it]  0%|          | 30/10000 [03:49<17:21:53,  6.27s/it]                                                     {'loss': 0.2419, 'grad_norm': 0.0274658203125, 'learning_rate': 0.0002, 'epoch': 0.04}
  0%|          | 30/10000 [03:49<17:21:53,  6.27s/it]  0%|          | 31/10000 [03:56<17:20:29,  6.26s/it]  0%|          | 32/10000 [04:04<19:23:56,  7.01s/it]  0%|          | 33/10000 [04:11<19:08:37,  6.91s/it]  0%|          | 34/10000 [04:18<19:11:20,  6.93s/it]  0%|          | 35/10000 [04:25<18:52:07,  6.82s/it]  0%|          | 36/10000 [04:31<18:30:22,  6.69s/it]  0%|          | 37/10000 [04:38<18:43:26,  6.77s/it]  0%|          | 38/10000 [04:45<19:16:33,  6.97s/it]  0%|          | 39/10000 [04:52<19:14:13,  6.95s/it]  0%|          | 40/10000 [04:59<19:05:59,  6.90s/it]                                                     {'loss': 0.2073, 'grad_norm': 0.0263671875, 'learning_rate': 0.0002, 'epoch': 0.05}
  0%|          | 40/10000 [04:59<19:05:59,  6.90s/it]  0%|          | 41/10000 [05:04<17:42:38,  6.40s/it]  0%|          | 42/10000 [05:11<17:44:34,  6.41s/it]  0%|          | 43/10000 [05:17<17:48:42,  6.44s/it]  0%|          | 44/10000 [05:25<19:06:25,  6.91s/it]  0%|          | 45/10000 [05:32<18:38:08,  6.74s/it]  0%|          | 46/10000 [05:38<18:16:37,  6.61s/it]  0%|          | 47/10000 [05:44<18:17:29,  6.62s/it]  0%|          | 48/10000 [05:50<17:15:53,  6.25s/it]  0%|          | 49/10000 [05:56<16:49:50,  6.09s/it]  0%|          | 50/10000 [06:01<16:35:41,  6.00s/it]                                                     {'loss': 0.2098, 'grad_norm': 0.033935546875, 'learning_rate': 0.0002, 'epoch': 0.07}
  0%|          | 50/10000 [06:01<16:35:41,  6.00s/it]  1%|          | 51/10000 [06:09<18:05:09,  6.54s/it]  1%|          | 52/10000 [06:15<17:22:38,  6.29s/it]  1%|          | 53/10000 [06:20<16:30:23,  5.97s/it]  1%|          | 54/10000 [06:27<17:16:32,  6.25s/it]  1%|          | 55/10000 [06:34<17:38:40,  6.39s/it]  1%|          | 56/10000 [06:40<17:21:53,  6.29s/it]  1%|          | 57/10000 [06:48<19:12:10,  6.95s/it]  1%|          | 58/10000 [06:54<18:19:49,  6.64s/it]  1%|          | 59/10000 [07:01<18:06:17,  6.56s/it]  1%|          | 60/10000 [07:07<18:01:00,  6.53s/it]                                                     {'loss': 0.1628, 'grad_norm': 0.033203125, 'learning_rate': 0.0002, 'epoch': 0.08}
  1%|          | 60/10000 [07:07<18:01:00,  6.53s/it]  1%|          | 61/10000 [07:12<16:46:34,  6.08s/it]  1%|          | 62/10000 [07:17<16:01:49,  5.81s/it]  1%|          | 63/10000 [07:25<18:04:02,  6.55s/it]  1%|          | 64/10000 [07:32<18:20:33,  6.65s/it]  1%|          | 65/10000 [07:39<18:11:10,  6.59s/it]  1%|          | 66/10000 [07:45<18:15:40,  6.62s/it]  1%|          | 67/10000 [07:52<18:27:43,  6.69s/it]  1%|          | 68/10000 [07:59<18:34:10,  6.73s/it]  1%|          | 69/10000 [08:06<18:47:56,  6.81s/it]  1%|          | 70/10000 [08:13<18:50:10,  6.83s/it]                                                     {'loss': 0.1469, 'grad_norm': 0.025390625, 'learning_rate': 0.0002, 'epoch': 0.09}
  1%|          | 70/10000 [08:13<18:50:10,  6.83s/it]  1%|          | 71/10000 [08:19<18:27:20,  6.69s/it]  1%|          | 72/10000 [08:25<17:53:54,  6.49s/it]  1%|          | 73/10000 [08:31<16:56:01,  6.14s/it]  1%|          | 74/10000 [08:36<16:01:00,  5.81s/it]  1%|          | 75/10000 [08:42<16:04:19,  5.83s/it]  1%|          | 76/10000 [08:51<18:48:04,  6.82s/it]  1%|          | 77/10000 [08:56<17:26:37,  6.33s/it]  1%|          | 78/10000 [09:01<16:34:42,  6.02s/it]  1%|          | 79/10000 [09:08<16:44:53,  6.08s/it]  1%|          | 80/10000 [09:14<17:16:09,  6.27s/it]                                                     {'loss': 0.1439, 'grad_norm': 0.0654296875, 'learning_rate': 0.0002, 'epoch': 0.11}
  1%|          | 80/10000 [09:14<17:16:09,  6.27s/it]  1%|          | 81/10000 [09:21<17:30:02,  6.35s/it]  1%|          | 82/10000 [09:29<19:25:08,  7.05s/it]  1%|          | 83/10000 [09:36<19:06:00,  6.93s/it]  1%|          | 84/10000 [09:43<18:57:31,  6.88s/it]  1%|          | 85/10000 [09:49<18:16:37,  6.64s/it]  1%|          | 86/10000 [09:55<18:04:40,  6.56s/it]  1%|          | 87/10000 [10:02<17:56:36,  6.52s/it]  1%|          | 88/10000 [10:09<18:49:35,  6.84s/it]  1%|          | 89/10000 [10:16<18:21:56,  6.67s/it]  1%|          | 90/10000 [10:22<18:05:03,  6.57s/it]                                                     {'loss': 0.1063, 'grad_norm': 0.060302734375, 'learning_rate': 0.0002, 'epoch': 0.12}
  1%|          | 90/10000 [10:22<18:05:03,  6.57s/it]  1%|          | 91/10000 [10:28<17:58:21,  6.53s/it]  1%|          | 92/10000 [10:35<17:58:25,  6.53s/it]  1%|          | 93/10000 [10:42<18:05:35,  6.57s/it]  1%|          | 94/10000 [10:50<19:40:29,  7.15s/it]  1%|          | 95/10000 [10:57<19:22:42,  7.04s/it]  1%|          | 96/10000 [11:03<18:19:48,  6.66s/it]  1%|          | 97/10000 [11:08<17:01:35,  6.19s/it]  1%|          | 98/10000 [11:13<16:11:27,  5.89s/it]  1%|          | 99/10000 [11:18<15:43:49,  5.72s/it]  1%|          | 100/10000 [11:25<16:13:05,  5.90s/it]                                                      {'loss': 0.0982, 'grad_norm': 0.08154296875, 'learning_rate': 0.0002, 'epoch': 0.13}
  1%|          | 100/10000 [11:25<16:13:05,  5.90s/it]  1%|          | 101/10000 [11:32<17:35:26,  6.40s/it]  1%|          | 102/10000 [11:39<17:41:56,  6.44s/it]  1%|          | 103/10000 [11:44<16:54:07,  6.15s/it]  1%|          | 104/10000 [11:49<15:58:25,  5.81s/it]  1%|          | 105/10000 [11:55<15:39:34,  5.70s/it]  1%|          | 106/10000 [12:01<16:05:52,  5.86s/it]  1%|          | 107/10000 [12:09<17:53:39,  6.51s/it]  1%|          | 108/10000 [12:16<18:03:52,  6.57s/it]  1%|          | 109/10000 [12:22<18:03:10,  6.57s/it]  1%|          | 110/10000 [12:28<17:49:34,  6.49s/it]                                                      {'loss': 0.0667, 'grad_norm': 0.05419921875, 'learning_rate': 0.0002, 'epoch': 0.15}
  1%|          | 110/10000 [12:28<17:49:34,  6.49s/it]  1%|          | 111/10000 [12:33<16:39:05,  6.06s/it]  1%|          | 112/10000 [12:39<15:56:19,  5.80s/it]  1%|          | 113/10000 [12:48<18:53:46,  6.88s/it]  1%|          | 114/10000 [12:55<18:49:32,  6.86s/it]  1%|          | 115/10000 [13:01<18:23:17,  6.70s/it]  1%|          | 116/10000 [13:07<17:41:16,  6.44s/it]  1%|          | 117/10000 [13:13<17:32:38,  6.39s/it]  1%|          | 118/10000 [13:18<16:26:16,  5.99s/it]  1%|          | 119/10000 [13:25<17:20:22,  6.32s/it]  1%|          | 120/10000 [13:31<17:04:28,  6.22s/it]                                                      {'loss': 0.0578, 'grad_norm': 0.040771484375, 'learning_rate': 0.0002, 'epoch': 0.16}
  1%|          | 120/10000 [13:31<17:04:28,  6.22s/it]  1%|          | 121/10000 [13:37<16:17:36,  5.94s/it]  1%|          | 122/10000 [13:43<16:22:39,  5.97s/it]  1%|          | 123/10000 [13:49<16:53:51,  6.16s/it]  1%|          | 124/10000 [13:56<17:13:33,  6.28s/it]  1%|▏         | 125/10000 [14:03<17:42:00,  6.45s/it]  1%|▏         | 126/10000 [14:10<18:08:59,  6.62s/it]  1%|▏         | 127/10000 [14:16<18:12:36,  6.64s/it]  1%|▏         | 128/10000 [14:23<18:11:13,  6.63s/it]  1%|▏         | 129/10000 [14:30<18:03:13,  6.58s/it]  1%|▏         | 130/10000 [14:36<18:09:41,  6.62s/it]                                                      {'loss': 0.0465, 'grad_norm': 0.057861328125, 'learning_rate': 0.0002, 'epoch': 0.17}
  1%|▏         | 130/10000 [14:36<18:09:41,  6.62s/it]  1%|▏         | 131/10000 [14:43<18:09:22,  6.62s/it]  1%|▏         | 132/10000 [14:52<19:59:53,  7.30s/it]  1%|▏         | 133/10000 [14:59<19:37:19,  7.16s/it]  1%|▏         | 134/10000 [15:05<19:12:56,  7.01s/it]  1%|▏         | 135/10000 [15:11<18:31:04,  6.76s/it]  1%|▏         | 136/10000 [15:18<18:22:43,  6.71s/it]  1%|▏         | 137/10000 [15:24<17:38:53,  6.44s/it]  1%|▏         | 138/10000 [15:32<18:50:55,  6.88s/it]  1%|▏         | 139/10000 [15:37<17:47:48,  6.50s/it]  1%|▏         | 140/10000 [15:43<16:51:48,  6.16s/it]                                                      {'loss': 0.0435, 'grad_norm': 0.048828125, 'learning_rate': 0.0002, 'epoch': 0.19}
  1%|▏         | 140/10000 [15:43<16:51:48,  6.16s/it]  1%|▏         | 141/10000 [15:49<16:44:23,  6.11s/it]  1%|▏         | 142/10000 [15:55<16:41:09,  6.09s/it]  1%|▏         | 143/10000 [16:01<16:48:30,  6.14s/it]  1%|▏         | 144/10000 [16:09<17:58:49,  6.57s/it]  1%|▏         | 145/10000 [16:15<18:05:23,  6.61s/it]  1%|▏         | 146/10000 [16:22<18:12:04,  6.65s/it]  1%|▏         | 147/10000 [16:29<18:17:17,  6.68s/it]  1%|▏         | 148/10000 [16:36<18:20:55,  6.70s/it]  1%|▏         | 149/10000 [16:42<18:23:30,  6.72s/it]  2%|▏         | 150/10000 [16:49<18:07:43,  6.63s/it]                                                      {'loss': 0.0346, 'grad_norm': 0.0673828125, 'learning_rate': 0.0002, 'epoch': 0.2}
  2%|▏         | 150/10000 [16:49<18:07:43,  6.63s/it]  2%|▏         | 151/10000 [16:57<19:26:53,  7.11s/it]  2%|▏         | 152/10000 [17:04<19:16:58,  7.05s/it]  2%|▏         | 153/10000 [17:10<18:47:35,  6.87s/it]  2%|▏         | 154/10000 [17:16<17:36:36,  6.44s/it]  2%|▏         | 155/10000 [17:21<16:49:16,  6.15s/it]  2%|▏         | 156/10000 [17:27<16:30:21,  6.04s/it]  2%|▏         | 157/10000 [17:35<18:03:08,  6.60s/it]  2%|▏         | 158/10000 [17:41<17:47:58,  6.51s/it]  2%|▏         | 159/10000 [17:48<17:57:21,  6.57s/it]  2%|▏         | 160/10000 [17:54<17:55:08,  6.56s/it]                                                      {'loss': 0.0305, 'grad_norm': 0.04833984375, 'learning_rate': 0.0002, 'epoch': 0.21}
  2%|▏         | 160/10000 [17:55<17:55:08,  6.56s/it]  2%|▏         | 161/10000 [18:02<18:20:39,  6.71s/it]  2%|▏         | 162/10000 [18:09<18:33:41,  6.79s/it]  2%|▏         | 163/10000 [18:17<20:21:25,  7.45s/it]  2%|▏         | 164/10000 [18:23<18:52:15,  6.91s/it]  2%|▏         | 165/10000 [18:30<18:48:46,  6.89s/it]  2%|▏         | 166/10000 [18:37<18:48:40,  6.89s/it]  2%|▏         | 167/10000 [18:44<18:45:04,  6.87s/it]  2%|▏         | 168/10000 [18:50<18:01:14,  6.60s/it]  2%|▏         | 169/10000 [18:57<18:28:18,  6.76s/it]  2%|▏         | 170/10000 [19:03<18:16:52,  6.70s/it]                                                      {'loss': 0.0277, 'grad_norm': 0.03662109375, 'learning_rate': 0.0002, 'epoch': 0.23}
  2%|▏         | 170/10000 [19:03<18:16:52,  6.70s/it]  2%|▏         | 171/10000 [19:09<17:34:21,  6.44s/it]  2%|▏         | 172/10000 [19:15<16:46:43,  6.15s/it]  2%|▏         | 173/10000 [19:21<17:15:21,  6.32s/it]  2%|▏         | 174/10000 [19:28<17:36:06,  6.45s/it]  2%|▏         | 175/10000 [19:35<18:04:02,  6.62s/it]  2%|▏         | 176/10000 [19:43<19:01:16,  6.97s/it]  2%|▏         | 177/10000 [19:49<18:41:34,  6.85s/it]  2%|▏         | 178/10000 [19:57<18:49:30,  6.90s/it]  2%|▏         | 179/10000 [20:03<18:34:59,  6.81s/it]  2%|▏         | 180/10000 [20:10<18:35:14,  6.81s/it]                                                      {'loss': 0.0193, 'grad_norm': 0.05078125, 'learning_rate': 0.0002, 'epoch': 0.24}
  2%|▏         | 180/10000 [20:10<18:35:14,  6.81s/it]  2%|▏         | 181/10000 [20:16<17:41:13,  6.48s/it]  2%|▏         | 182/10000 [20:23<18:44:30,  6.87s/it]  2%|▏         | 183/10000 [20:30<18:06:35,  6.64s/it]  2%|▏         | 184/10000 [20:35<17:16:19,  6.33s/it]  2%|▏         | 185/10000 [20:41<16:32:39,  6.07s/it]  2%|▏         | 186/10000 [20:46<15:55:44,  5.84s/it]  2%|▏         | 187/10000 [20:52<15:58:01,  5.86s/it]  2%|▏         | 188/10000 [21:00<17:30:46,  6.43s/it]  2%|▏         | 189/10000 [21:06<17:17:43,  6.35s/it]  2%|▏         | 190/10000 [21:11<16:22:21,  6.01s/it]                                                      {'loss': 0.0234, 'grad_norm': 0.044677734375, 'learning_rate': 0.0002, 'epoch': 0.25}
  2%|▏         | 190/10000 [21:11<16:22:21,  6.01s/it]  2%|▏         | 191/10000 [21:16<15:40:36,  5.75s/it]  2%|▏         | 192/10000 [21:22<16:08:37,  5.93s/it]  2%|▏         | 193/10000 [21:28<15:59:11,  5.87s/it]  2%|▏         | 194/10000 [21:35<17:06:03,  6.28s/it]  2%|▏         | 195/10000 [21:41<16:51:23,  6.19s/it]  2%|▏         | 196/10000 [21:47<16:18:15,  5.99s/it]  2%|▏         | 197/10000 [21:53<16:14:22,  5.96s/it]  2%|▏         | 198/10000 [21:59<16:05:09,  5.91s/it]  2%|▏         | 199/10000 [22:05<16:31:32,  6.07s/it]  2%|▏         | 200/10000 [22:12<17:24:35,  6.40s/it]                                                      {'loss': 0.0149, 'grad_norm': 0.05224609375, 'learning_rate': 0.0002, 'epoch': 0.27}
  2%|▏         | 200/10000 [22:12<17:24:35,  6.40s/it]  2%|▏         | 201/10000 [22:21<19:04:42,  7.01s/it]  2%|▏         | 202/10000 [22:27<18:26:33,  6.78s/it]  2%|▏         | 203/10000 [22:34<18:36:55,  6.84s/it]  2%|▏         | 204/10000 [22:41<18:42:59,  6.88s/it]  2%|▏         | 205/10000 [22:48<18:47:49,  6.91s/it]  2%|▏         | 206/10000 [22:54<18:18:15,  6.73s/it]  2%|▏         | 207/10000 [23:02<19:01:55,  7.00s/it]  2%|▏         | 208/10000 [23:09<18:57:44,  6.97s/it]  2%|▏         | 209/10000 [23:14<17:51:09,  6.56s/it]  2%|▏         | 210/10000 [23:21<17:38:16,  6.49s/it]                                                      {'loss': 0.0172, 'grad_norm': 0.033447265625, 'learning_rate': 0.0002, 'epoch': 0.28}
  2%|▏         | 210/10000 [23:21<17:38:16,  6.49s/it]  2%|▏         | 211/10000 [23:26<17:00:11,  6.25s/it]  2%|▏         | 212/10000 [23:32<16:15:56,  5.98s/it]  2%|▏         | 213/10000 [23:39<17:37:16,  6.48s/it]  2%|▏         | 214/10000 [23:46<17:38:01,  6.49s/it]  2%|▏         | 215/10000 [23:52<17:10:06,  6.32s/it]  2%|▏         | 216/10000 [23:58<16:53:43,  6.22s/it]  2%|▏         | 217/10000 [24:04<16:38:48,  6.13s/it]  2%|▏         | 218/10000 [24:09<16:07:07,  5.93s/it]  2%|▏         | 219/10000 [24:16<17:11:05,  6.33s/it]  2%|▏         | 220/10000 [24:23<17:20:24,  6.38s/it]                                                      {'loss': 0.0132, 'grad_norm': 0.03271484375, 'learning_rate': 0.0002, 'epoch': 0.29}
  2%|▏         | 220/10000 [24:23<17:20:24,  6.38s/it]  2%|▏         | 221/10000 [24:29<17:34:58,  6.47s/it]  2%|▏         | 222/10000 [24:36<17:48:50,  6.56s/it]  2%|▏         | 223/10000 [24:43<17:58:58,  6.62s/it]  2%|▏         | 224/10000 [24:50<18:14:50,  6.72s/it]  2%|▏         | 225/10000 [24:57<18:28:50,  6.81s/it]  2%|▏         | 226/10000 [25:05<19:26:45,  7.16s/it]  2%|▏         | 227/10000 [25:12<19:09:52,  7.06s/it]  2%|▏         | 228/10000 [25:17<17:55:53,  6.61s/it]  2%|▏         | 229/10000 [25:23<17:00:44,  6.27s/it]  2%|▏         | 230/10000 [25:28<16:18:51,  6.01s/it]                                                      {'loss': 0.0113, 'grad_norm': 0.05615234375, 'learning_rate': 0.0002, 'epoch': 0.31}
  2%|▏         | 230/10000 [25:28<16:18:51,  6.01s/it]  2%|▏         | 231/10000 [25:33<15:35:14,  5.74s/it]  2%|▏         | 232/10000 [25:42<17:46:13,  6.55s/it]  2%|▏         | 233/10000 [25:48<17:09:41,  6.33s/it]  2%|▏         | 234/10000 [25:53<16:45:24,  6.18s/it]  2%|▏         | 235/10000 [25:59<16:04:21,  5.93s/it]  2%|▏         | 236/10000 [26:04<15:42:30,  5.79s/it]  2%|▏         | 237/10000 [26:10<15:23:38,  5.68s/it]  2%|▏         | 238/10000 [26:20<18:58:26,  7.00s/it]  2%|▏         | 239/10000 [26:26<18:41:08,  6.89s/it]  2%|▏         | 240/10000 [26:32<17:46:18,  6.56s/it]                                                      {'loss': 0.0126, 'grad_norm': 0.056884765625, 'learning_rate': 0.0002, 'epoch': 0.32}
  2%|▏         | 240/10000 [26:32<17:46:18,  6.56s/it]  2%|▏         | 241/10000 [26:37<16:41:09,  6.16s/it]  2%|▏         | 242/10000 [26:43<16:13:39,  5.99s/it]  2%|▏         | 243/10000 [26:49<16:34:49,  6.12s/it]  2%|▏         | 244/10000 [26:58<18:47:13,  6.93s/it]  2%|▏         | 245/10000 [27:05<18:36:21,  6.87s/it]  2%|▏         | 246/10000 [27:11<18:08:29,  6.70s/it]  2%|▏         | 247/10000 [27:17<17:25:53,  6.43s/it]  2%|▏         | 248/10000 [27:23<17:06:23,  6.31s/it]  2%|▏         | 249/10000 [27:28<16:17:50,  6.02s/it]  2%|▎         | 250/10000 [27:34<15:58:42,  5.90s/it]                                                      {'loss': 0.0104, 'grad_norm': 0.046142578125, 'learning_rate': 0.0002, 'epoch': 0.33}
  2%|▎         | 250/10000 [27:34<15:58:42,  5.90s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...
Saving PEFT checkpoint...

Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
  3%|▎         | 251/10000 [27:49<22:59:26,  8.49s/it]  3%|▎         | 252/10000 [27:55<21:29:04,  7.93s/it]  3%|▎         | 253/10000 [28:02<20:33:43,  7.59s/it]  3%|▎         | 254/10000 [28:08<19:34:28,  7.23s/it]  3%|▎         | 255/10000 [28:15<19:11:43,  7.09s/it]  3%|▎         | 256/10000 [28:22<18:41:59,  6.91s/it]  3%|▎         | 257/10000 [28:29<19:16:18,  7.12s/it]  3%|▎         | 258/10000 [28:36<18:44:23,  6.93s/it]  3%|▎         | 259/10000 [28:42<18:28:11,  6.83s/it]  3%|▎         | 260/10000 [28:49<18:07:28,  6.70s/it]                                                      {'loss': 0.0113, 'grad_norm': 0.04541015625, 'learning_rate': 0.0002, 'epoch': 0.35}
  3%|▎         | 260/10000 [28:49<18:07:28,  6.70s/it]  3%|▎         | 261/10000 [28:55<17:59:27,  6.65s/it]  3%|▎         | 262/10000 [29:02<17:46:02,  6.57s/it]  3%|▎         | 263/10000 [29:10<18:52:57,  6.98s/it]  3%|▎         | 264/10000 [29:16<18:31:52,  6.85s/it]  3%|▎         | 265/10000 [29:24<19:32:17,  7.23s/it]  3%|▎         | 266/10000 [29:31<19:08:11,  7.08s/it]  3%|▎         | 267/10000 [29:37<18:34:50,  6.87s/it]  3%|▎         | 268/10000 [29:44<18:15:40,  6.76s/it]  3%|▎         | 269/10000 [29:50<17:44:18,  6.56s/it]  3%|▎         | 270/10000 [29:56<17:18:16,  6.40s/it]                                                      {'loss': 0.0075, 'grad_norm': 0.02294921875, 'learning_rate': 0.0002, 'epoch': 0.36}
  3%|▎         | 270/10000 [29:56<17:18:16,  6.40s/it]  3%|▎         | 271/10000 [30:03<17:24:51,  6.44s/it]  3%|▎         | 272/10000 [30:09<17:25:49,  6.45s/it]  3%|▎         | 273/10000 [30:14<16:22:17,  6.06s/it]  3%|▎         | 274/10000 [30:19<15:47:05,  5.84s/it]  3%|▎         | 275/10000 [30:25<15:56:29,  5.90s/it]  3%|▎         | 276/10000 [30:33<17:37:33,  6.53s/it]  3%|▎         | 277/10000 [30:39<16:32:50,  6.13s/it]  3%|▎         | 278/10000 [30:44<15:40:59,  5.81s/it]  3%|▎         | 279/10000 [30:49<15:04:49,  5.58s/it]  3%|▎         | 280/10000 [30:54<15:03:28,  5.58s/it]                                                      {'loss': 0.0076, 'grad_norm': 0.0238037109375, 'learning_rate': 0.0002, 'epoch': 0.37}
  3%|▎         | 280/10000 [30:54<15:03:28,  5.58s/it]  3%|▎         | 281/10000 [31:01<16:11:30,  6.00s/it]  3%|▎         | 282/10000 [31:10<18:20:48,  6.80s/it]  3%|▎         | 283/10000 [31:17<18:24:59,  6.82s/it]  3%|▎         | 284/10000 [31:23<18:03:56,  6.69s/it]  3%|▎         | 285/10000 [31:29<17:15:00,  6.39s/it]  3%|▎         | 286/10000 [31:34<16:11:44,  6.00s/it]  3%|▎         | 287/10000 [31:40<16:16:37,  6.03s/it]  3%|▎         | 288/10000 [31:49<18:28:58,  6.85s/it]  3%|▎         | 289/10000 [31:56<18:19:44,  6.79s/it]  3%|▎         | 290/10000 [32:01<17:04:58,  6.33s/it]                                                      {'loss': 0.0092, 'grad_norm': 0.049072265625, 'learning_rate': 0.0002, 'epoch': 0.39}
  3%|▎         | 290/10000 [32:01<17:04:58,  6.33s/it]  3%|▎         | 291/10000 [32:06<16:01:43,  5.94s/it]  3%|▎         | 292/10000 [32:11<15:16:10,  5.66s/it]  3%|▎         | 293/10000 [32:16<14:46:58,  5.48s/it]  3%|▎         | 294/10000 [32:25<18:00:51,  6.68s/it]  3%|▎         | 295/10000 [32:32<17:32:02,  6.50s/it]  3%|▎         | 296/10000 [32:37<16:33:55,  6.15s/it]  3%|▎         | 297/10000 [32:42<16:10:40,  6.00s/it]  3%|▎         | 298/10000 [32:48<16:05:53,  5.97s/it]  3%|▎         | 299/10000 [32:54<16:07:13,  5.98s/it]  3%|▎         | 300/10000 [33:02<17:23:21,  6.45s/it]                                                      {'loss': 0.0067, 'grad_norm': 0.04052734375, 'learning_rate': 0.0002, 'epoch': 0.4}
  3%|▎         | 300/10000 [33:02<17:23:21,  6.45s/it]  3%|▎         | 301/10000 [33:10<18:33:01,  6.89s/it]  3%|▎         | 302/10000 [33:15<17:09:54,  6.37s/it]  3%|▎         | 303/10000 [33:20<16:25:08,  6.10s/it]  3%|▎         | 304/10000 [33:27<16:58:24,  6.30s/it]  3%|▎         | 305/10000 [33:33<16:39:27,  6.19s/it]  3%|▎         | 306/10000 [33:38<15:49:42,  5.88s/it]  3%|▎         | 307/10000 [33:48<19:06:57,  7.10s/it]  3%|▎         | 308/10000 [33:54<17:47:04,  6.61s/it]  3%|▎         | 309/10000 [33:59<16:37:20,  6.17s/it]  3%|▎         | 310/10000 [34:05<16:23:27,  6.09s/it]                                                      {'loss': 0.0078, 'grad_norm': 0.0206298828125, 'learning_rate': 0.0002, 'epoch': 0.41}
  3%|▎         | 310/10000 [34:05<16:23:27,  6.09s/it]  3%|▎         | 311/10000 [34:12<16:56:53,  6.30s/it]  3%|▎         | 312/10000 [34:18<17:21:54,  6.45s/it]  3%|▎         | 313/10000 [34:27<19:29:27,  7.24s/it]  3%|▎         | 314/10000 [34:34<18:51:39,  7.01s/it]  3%|▎         | 315/10000 [34:39<17:39:20,  6.56s/it]  3%|▎         | 316/10000 [34:45<16:27:43,  6.12s/it]  3%|▎         | 317/10000 [34:50<15:52:56,  5.90s/it]  3%|▎         | 318/10000 [34:55<15:11:50,  5.65s/it]  3%|▎         | 319/10000 [35:02<16:17:17,  6.06s/it]  3%|▎         | 320/10000 [35:08<16:29:18,  6.13s/it]                                                      {'loss': 0.0072, 'grad_norm': 0.020263671875, 'learning_rate': 0.0002, 'epoch': 0.43}
  3%|▎         | 320/10000 [35:08<16:29:18,  6.13s/it]  3%|▎         | 321/10000 [35:14<16:27:16,  6.12s/it]  3%|▎         | 322/10000 [35:20<16:01:28,  5.96s/it]  3%|▎         | 323/10000 [35:25<15:30:15,  5.77s/it]  3%|▎         | 324/10000 [35:31<15:42:16,  5.84s/it]  3%|▎         | 325/10000 [35:38<16:10:25,  6.02s/it]  3%|▎         | 326/10000 [35:46<17:49:25,  6.63s/it]  3%|▎         | 327/10000 [35:52<17:36:17,  6.55s/it]  3%|▎         | 328/10000 [35:58<17:09:52,  6.39s/it]  3%|▎         | 329/10000 [36:04<16:18:11,  6.07s/it]  3%|▎         | 330/10000 [36:09<16:00:55,  5.96s/it]                                                      {'loss': 0.0082, 'grad_norm': 0.0133056640625, 'learning_rate': 0.0002, 'epoch': 0.44}
  3%|▎         | 330/10000 [36:09<16:00:55,  5.96s/it]  3%|▎         | 331/10000 [36:15<16:12:21,  6.03s/it]  3%|▎         | 332/10000 [36:24<17:53:55,  6.66s/it]  3%|▎         | 333/10000 [36:30<17:44:36,  6.61s/it]  3%|▎         | 334/10000 [36:36<17:34:19,  6.54s/it]  3%|▎         | 335/10000 [36:43<17:30:24,  6.52s/it]  3%|▎         | 336/10000 [36:49<17:06:16,  6.37s/it]  3%|▎         | 337/10000 [36:55<16:55:59,  6.31s/it]  3%|▎         | 338/10000 [37:04<18:37:48,  6.94s/it]  3%|▎         | 339/10000 [37:10<18:17:19,  6.81s/it]  3%|▎         | 340/10000 [37:16<17:55:05,  6.68s/it]                                                      {'loss': 0.0057, 'grad_norm': 0.1083984375, 'learning_rate': 0.0002, 'epoch': 0.45}
  3%|▎         | 340/10000 [37:16<17:55:05,  6.68s/it]  3%|▎         | 341/10000 [37:23<17:36:09,  6.56s/it]  3%|▎         | 342/10000 [37:29<17:16:26,  6.44s/it]  3%|▎         | 343/10000 [37:35<17:09:46,  6.40s/it]  3%|▎         | 344/10000 [37:42<17:26:24,  6.50s/it]  3%|▎         | 345/10000 [37:48<17:20:52,  6.47s/it]  3%|▎         | 346/10000 [37:55<17:46:19,  6.63s/it]  3%|▎         | 347/10000 [38:02<17:48:22,  6.64s/it]  3%|▎         | 348/10000 [38:08<17:11:28,  6.41s/it]  3%|▎         | 349/10000 [38:14<17:21:29,  6.47s/it]  4%|▎         | 350/10000 [38:22<18:04:36,  6.74s/it]                                                      {'loss': 0.0046, 'grad_norm': 0.0289306640625, 'learning_rate': 0.0002, 'epoch': 0.47}
  4%|▎         | 350/10000 [38:22<18:04:36,  6.74s/it]  4%|▎         | 351/10000 [38:29<18:48:03,  7.01s/it]  4%|▎         | 352/10000 [38:35<17:29:22,  6.53s/it]  4%|▎         | 353/10000 [38:40<16:35:09,  6.19s/it]  4%|▎         | 354/10000 [38:46<16:11:55,  6.05s/it]  4%|▎         | 355/10000 [38:53<16:42:54,  6.24s/it]  4%|▎         | 356/10000 [38:59<17:10:31,  6.41s/it]  4%|▎         | 357/10000 [39:08<19:10:09,  7.16s/it]  4%|▎         | 358/10000 [39:15<18:36:36,  6.95s/it]  4%|▎         | 359/10000 [39:21<18:05:58,  6.76s/it]  4%|▎         | 360/10000 [39:27<17:33:59,  6.56s/it]                                                      {'loss': 0.007, 'grad_norm': 0.020751953125, 'learning_rate': 0.0002, 'epoch': 0.48}
  4%|▎         | 360/10000 [39:27<17:33:59,  6.56s/it]  4%|▎         | 361/10000 [39:33<17:11:15,  6.42s/it]  4%|▎         | 362/10000 [39:39<16:46:44,  6.27s/it]  4%|▎         | 363/10000 [39:48<18:49:09,  7.03s/it]  4%|▎         | 364/10000 [39:54<18:02:00,  6.74s/it]  4%|▎         | 365/10000 [40:00<17:23:00,  6.50s/it]  4%|▎         | 366/10000 [40:05<16:09:49,  6.04s/it]  4%|▎         | 367/10000 [40:10<15:21:02,  5.74s/it]  4%|▎         | 368/10000 [40:15<14:46:34,  5.52s/it]  4%|▎         | 369/10000 [40:23<16:26:48,  6.15s/it]  4%|▎         | 370/10000 [40:29<16:23:16,  6.13s/it]                                                      {'loss': 0.009, 'grad_norm': 0.01953125, 'learning_rate': 0.0002, 'epoch': 0.49}
  4%|▎         | 370/10000 [40:29<16:23:16,  6.13s/it]  4%|▎         | 371/10000 [40:34<15:48:40,  5.91s/it]  4%|▎         | 372/10000 [40:40<15:22:00,  5.75s/it]  4%|▎         | 373/10000 [40:45<14:52:42,  5.56s/it]  4%|▎         | 374/10000 [40:50<15:03:25,  5.63s/it]  4%|▍         | 375/10000 [40:56<14:41:33,  5.50s/it]  4%|▍         | 376/10000 [41:02<15:41:49,  5.87s/it]  4%|▍         | 377/10000 [41:08<15:53:47,  5.95s/it]  4%|▍         | 378/10000 [41:15<16:31:38,  6.18s/it]  4%|▍         | 379/10000 [41:22<16:51:00,  6.30s/it]  4%|▍         | 380/10000 [41:28<17:04:50,  6.39s/it]                                                      {'loss': 0.0058, 'grad_norm': 0.025390625, 'learning_rate': 0.0002, 'epoch': 0.51}
  4%|▍         | 380/10000 [41:28<17:04:50,  6.39s/it]  4%|▍         | 381/10000 [41:35<17:19:55,  6.49s/it]  4%|▍         | 382/10000 [41:43<18:39:00,  6.98s/it]  4%|▍         | 383/10000 [41:50<18:29:46,  6.92s/it]  4%|▍         | 384/10000 [41:57<18:13:26,  6.82s/it]  4%|▍         | 385/10000 [42:03<17:36:29,  6.59s/it]  4%|▍         | 386/10000 [42:08<16:23:18,  6.14s/it]  4%|▍         | 387/10000 [42:13<15:39:07,  5.86s/it]  4%|▍         | 388/10000 [42:21<17:41:29,  6.63s/it]  4%|▍         | 389/10000 [42:28<17:20:25,  6.50s/it]  4%|▍         | 390/10000 [42:33<16:21:00,  6.12s/it]                                                      {'loss': 0.0043, 'grad_norm': 0.00836181640625, 'learning_rate': 0.0002, 'epoch': 0.52}
  4%|▍         | 390/10000 [42:33<16:21:00,  6.12s/it]  4%|▍         | 391/10000 [42:38<15:38:13,  5.86s/it]  4%|▍         | 392/10000 [42:43<15:12:34,  5.70s/it]  4%|▍         | 393/10000 [42:50<16:00:31,  6.00s/it]  4%|▍         | 394/10000 [42:57<17:03:52,  6.40s/it]  4%|▍         | 395/10000 [43:04<17:18:36,  6.49s/it]  4%|▍         | 396/10000 [43:10<17:08:04,  6.42s/it]  4%|▍         | 397/10000 [43:17<17:22:04,  6.51s/it]  4%|▍         | 398/10000 [43:24<17:21:17,  6.51s/it]  4%|▍         | 399/10000 [43:30<17:30:24,  6.56s/it]  4%|▍         | 400/10000 [43:36<17:08:22,  6.43s/it]                                                      {'loss': 0.0035, 'grad_norm': 0.01953125, 'learning_rate': 0.0002, 'epoch': 0.53}
  4%|▍         | 400/10000 [43:36<17:08:22,  6.43s/it]  4%|▍         | 401/10000 [43:44<18:05:46,  6.79s/it]  4%|▍         | 402/10000 [43:51<17:54:34,  6.72s/it]  4%|▍         | 403/10000 [43:57<17:50:02,  6.69s/it]  4%|▍         | 404/10000 [44:04<18:02:10,  6.77s/it]  4%|▍         | 405/10000 [44:11<17:55:01,  6.72s/it]  4%|▍         | 406/10000 [44:17<17:10:13,  6.44s/it]  4%|▍         | 407/10000 [44:23<17:20:40,  6.51s/it]  4%|▍         | 408/10000 [44:30<17:21:14,  6.51s/it]  4%|▍         | 409/10000 [44:36<17:18:15,  6.50s/it]  4%|▍         | 410/10000 [44:43<17:20:08,  6.51s/it]                                                      {'loss': 0.004, 'grad_norm': 0.00390625, 'learning_rate': 0.0002, 'epoch': 0.55}
  4%|▍         | 410/10000 [44:43<17:20:08,  6.51s/it]  4%|▍         | 411/10000 [44:50<17:38:01,  6.62s/it]  4%|▍         | 412/10000 [44:56<17:20:40,  6.51s/it]  4%|▍         | 413/10000 [45:04<18:40:29,  7.01s/it]  4%|▍         | 414/10000 [45:11<18:29:33,  6.94s/it]  4%|▍         | 415/10000 [45:18<18:15:11,  6.86s/it]  4%|▍         | 416/10000 [45:24<18:02:33,  6.78s/it]  4%|▍         | 417/10000 [45:31<17:57:17,  6.74s/it]  4%|▍         | 418/10000 [45:37<17:45:47,  6.67s/it]  4%|▍         | 419/10000 [45:45<18:52:41,  7.09s/it]  4%|▍         | 420/10000 [45:52<18:37:49,  7.00s/it]                                                      {'loss': 0.0045, 'grad_norm': 0.02392578125, 'learning_rate': 0.0002, 'epoch': 0.56}
  4%|▍         | 420/10000 [45:52<18:37:49,  7.00s/it]  4%|▍         | 421/10000 [45:57<17:17:00,  6.50s/it]  4%|▍         | 422/10000 [46:03<16:22:21,  6.15s/it]  4%|▍         | 423/10000 [46:08<15:31:21,  5.83s/it]  4%|▍         | 424/10000 [46:13<14:57:38,  5.62s/it]  4%|▍         | 425/10000 [46:20<16:07:45,  6.06s/it]  4%|▍         | 426/10000 [46:28<17:42:40,  6.66s/it]  4%|▍         | 427/10000 [46:33<16:38:03,  6.26s/it]  4%|▍         | 428/10000 [46:39<15:54:46,  5.98s/it]  4%|▍         | 429/10000 [46:44<15:23:31,  5.79s/it]  4%|▍         | 430/10000 [46:51<16:09:45,  6.08s/it]                                                      {'loss': 0.0023, 'grad_norm': 0.019287109375, 'learning_rate': 0.0002, 'epoch': 0.57}
  4%|▍         | 430/10000 [46:51<16:09:45,  6.08s/it]  4%|▍         | 431/10000 [46:58<16:48:21,  6.32s/it]  4%|▍         | 432/10000 [47:06<18:36:22,  7.00s/it]  4%|▍         | 433/10000 [47:13<17:58:52,  6.77s/it]  4%|▍         | 434/10000 [47:18<17:03:26,  6.42s/it]  4%|▍         | 435/10000 [47:24<16:28:46,  6.20s/it]  4%|▍         | 436/10000 [47:29<15:50:10,  5.96s/it]  4%|▍         | 437/10000 [47:35<15:14:28,  5.74s/it]  4%|▍         | 438/10000 [47:43<17:03:40,  6.42s/it]  4%|▍         | 439/10000 [47:49<17:02:02,  6.41s/it]  4%|▍         | 440/10000 [47:56<17:13:18,  6.49s/it]                                                      {'loss': 0.0023, 'grad_norm': 0.00921630859375, 'learning_rate': 0.0002, 'epoch': 0.59}
  4%|▍         | 440/10000 [47:56<17:13:18,  6.49s/it]  4%|▍         | 441/10000 [48:02<17:03:31,  6.42s/it]  4%|▍         | 442/10000 [48:07<16:00:21,  6.03s/it]  4%|▍         | 443/10000 [48:13<15:40:42,  5.91s/it]  4%|▍         | 444/10000 [48:20<17:10:51,  6.47s/it]  4%|▍         | 445/10000 [48:27<17:33:34,  6.62s/it]  4%|▍         | 446/10000 [48:33<16:55:51,  6.38s/it]  4%|▍         | 447/10000 [48:39<16:13:39,  6.12s/it]  4%|▍         | 448/10000 [48:44<15:36:15,  5.88s/it]  4%|▍         | 449/10000 [48:50<15:22:36,  5.80s/it]  4%|▍         | 450/10000 [48:59<17:57:32,  6.77s/it]                                                      {'loss': 0.0063, 'grad_norm': 0.060546875, 'learning_rate': 0.0002, 'epoch': 0.6}
  4%|▍         | 450/10000 [48:59<17:57:32,  6.77s/it]  5%|▍         | 451/10000 [49:07<18:48:45,  7.09s/it]  5%|▍         | 452/10000 [49:12<17:32:49,  6.62s/it]  5%|▍         | 453/10000 [49:18<16:40:27,  6.29s/it]  5%|▍         | 454/10000 [49:24<16:41:02,  6.29s/it]  5%|▍         | 455/10000 [49:31<17:05:32,  6.45s/it]  5%|▍         | 456/10000 [49:36<16:26:14,  6.20s/it]  5%|▍         | 457/10000 [49:45<18:26:08,  6.95s/it]  5%|▍         | 458/10000 [49:50<17:13:30,  6.50s/it]  5%|▍         | 459/10000 [49:57<17:12:18,  6.49s/it]  5%|▍         | 460/10000 [50:04<17:22:00,  6.55s/it]                                                      {'loss': 0.0068, 'grad_norm': 0.02685546875, 'learning_rate': 0.0002, 'epoch': 0.61}
  5%|▍         | 460/10000 [50:04<17:22:00,  6.55s/it]  5%|▍         | 461/10000 [50:10<17:36:49,  6.65s/it]  5%|▍         | 462/10000 [50:16<17:07:16,  6.46s/it]  5%|▍         | 463/10000 [50:24<18:19:32,  6.92s/it]  5%|▍         | 464/10000 [50:31<18:00:25,  6.80s/it]  5%|▍         | 465/10000 [50:36<16:45:11,  6.33s/it]  5%|▍         | 466/10000 [50:41<15:52:01,  5.99s/it]  5%|▍         | 467/10000 [50:47<15:12:27,  5.74s/it]  5%|▍         | 468/10000 [50:53<15:37:44,  5.90s/it]  5%|▍         | 469/10000 [51:00<16:47:24,  6.34s/it]  5%|▍         | 470/10000 [51:07<16:55:30,  6.39s/it]                                                      {'loss': 0.005, 'grad_norm': 0.0159912109375, 'learning_rate': 0.0002, 'epoch': 0.63}
  5%|▍         | 470/10000 [51:07<16:55:30,  6.39s/it]  5%|▍         | 471/10000 [51:14<17:14:13,  6.51s/it]  5%|▍         | 472/10000 [51:21<17:40:08,  6.68s/it]  5%|▍         | 473/10000 [51:27<17:26:15,  6.59s/it]  5%|▍         | 474/10000 [51:32<16:17:56,  6.16s/it]  5%|▍         | 475/10000 [51:39<16:39:08,  6.29s/it]  5%|▍         | 476/10000 [51:47<18:19:59,  6.93s/it]  5%|▍         | 477/10000 [51:53<17:08:35,  6.48s/it]  5%|▍         | 478/10000 [51:59<17:14:07,  6.52s/it]  5%|▍         | 479/10000 [52:06<17:23:30,  6.58s/it]  5%|▍         | 480/10000 [52:11<16:17:54,  6.16s/it]                                                      {'loss': 0.0029, 'grad_norm': 0.01068115234375, 'learning_rate': 0.0002, 'epoch': 0.64}
  5%|▍         | 480/10000 [52:11<16:17:54,  6.16s/it]  5%|▍         | 481/10000 [52:16<15:36:16,  5.90s/it]  5%|▍         | 482/10000 [52:24<17:06:55,  6.47s/it]  5%|▍         | 483/10000 [52:30<16:39:27,  6.30s/it]  5%|▍         | 484/10000 [52:35<15:52:29,  6.01s/it]  5%|▍         | 485/10000 [52:41<15:13:54,  5.76s/it]  5%|▍         | 486/10000 [52:47<15:38:19,  5.92s/it]  5%|▍         | 487/10000 [52:52<15:08:10,  5.73s/it]  5%|▍         | 488/10000 [53:01<17:17:40,  6.55s/it]  5%|▍         | 489/10000 [53:07<16:50:45,  6.38s/it]  5%|▍         | 490/10000 [53:12<15:56:57,  6.04s/it]                                                      {'loss': 0.0026, 'grad_norm': 0.0086669921875, 'learning_rate': 0.0002, 'epoch': 0.65}
  5%|▍         | 490/10000 [53:12<15:56:57,  6.04s/it]  5%|▍         | 491/10000 [53:17<15:17:13,  5.79s/it]  5%|▍         | 492/10000 [53:23<15:08:50,  5.74s/it]  5%|▍         | 493/10000 [53:28<14:36:29,  5.53s/it]  5%|▍         | 494/10000 [53:35<15:56:52,  6.04s/it]  5%|▍         | 495/10000 [53:42<16:30:28,  6.25s/it]  5%|▍         | 496/10000 [53:47<15:57:47,  6.05s/it]  5%|▍         | 497/10000 [53:53<15:36:48,  5.91s/it]  5%|▍         | 498/10000 [53:58<15:07:52,  5.73s/it]  5%|▍         | 499/10000 [54:04<14:48:39,  5.61s/it]  5%|▌         | 500/10000 [54:10<15:07:56,  5.73s/it]                                                      {'loss': 0.0042, 'grad_norm': 0.031494140625, 'learning_rate': 0.0002, 'epoch': 0.67}
  5%|▌         | 500/10000 [54:10<15:07:56,  5.73s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
  5%|▌         | 501/10000 [54:24<22:05:48,  8.37s/it]  5%|▌         | 502/10000 [54:31<20:47:58,  7.88s/it]  5%|▌         | 503/10000 [54:36<19:02:36,  7.22s/it]  5%|▌         | 504/10000 [54:43<18:24:09,  6.98s/it]  5%|▌         | 505/10000 [54:48<16:53:41,  6.41s/it]  5%|▌         | 506/10000 [54:54<16:23:27,  6.22s/it]  5%|▌         | 507/10000 [55:02<17:55:55,  6.80s/it]  5%|▌         | 508/10000 [55:08<17:44:22,  6.73s/it]  5%|▌         | 509/10000 [55:15<17:13:15,  6.53s/it]  5%|▌         | 510/10000 [55:20<16:05:37,  6.11s/it]                                                      {'loss': 0.0022, 'grad_norm': 0.0069580078125, 'learning_rate': 0.0002, 'epoch': 0.68}
  5%|▌         | 510/10000 [55:20<16:05:37,  6.11s/it]  5%|▌         | 511/10000 [55:25<15:17:04,  5.80s/it]  5%|▌         | 512/10000 [55:30<14:43:03,  5.58s/it]  5%|▌         | 513/10000 [55:41<18:46:11,  7.12s/it]  5%|▌         | 514/10000 [55:47<18:29:07,  7.02s/it]  5%|▌         | 515/10000 [55:54<18:13:21,  6.92s/it]  5%|▌         | 516/10000 [56:01<18:04:45,  6.86s/it]  5%|▌         | 517/10000 [56:07<17:55:03,  6.80s/it]  5%|▌         | 518/10000 [56:15<18:11:38,  6.91s/it]  5%|▌         | 519/10000 [56:23<19:32:47,  7.42s/it]  5%|▌         | 520/10000 [56:30<18:46:49,  7.13s/it]                                                      {'loss': 0.0052, 'grad_norm': 0.0196533203125, 'learning_rate': 0.0002, 'epoch': 0.69}
  5%|▌         | 520/10000 [56:30<18:46:49,  7.13s/it]  5%|▌         | 521/10000 [56:35<17:15:46,  6.56s/it]  5%|▌         | 522/10000 [56:41<16:57:34,  6.44s/it]  5%|▌         | 523/10000 [56:47<16:48:01,  6.38s/it]  5%|▌         | 524/10000 [56:53<16:29:10,  6.26s/it]  5%|▌         | 525/10000 [56:58<15:33:49,  5.91s/it]  5%|▌         | 526/10000 [57:05<16:19:13,  6.20s/it]  5%|▌         | 527/10000 [57:11<15:57:37,  6.07s/it]  5%|▌         | 528/10000 [57:17<16:02:28,  6.10s/it]  5%|▌         | 529/10000 [57:24<16:25:05,  6.24s/it]  5%|▌         | 530/10000 [57:29<15:56:21,  6.06s/it]                                                      {'loss': 0.0038, 'grad_norm': 0.0185546875, 'learning_rate': 0.0002, 'epoch': 0.71}
  5%|▌         | 530/10000 [57:29<15:56:21,  6.06s/it]  5%|▌         | 531/10000 [57:35<15:45:41,  5.99s/it]  5%|▌         | 532/10000 [57:43<17:26:34,  6.63s/it]  5%|▌         | 533/10000 [57:50<17:13:27,  6.55s/it]  5%|▌         | 534/10000 [57:55<16:33:38,  6.30s/it]  5%|▌         | 535/10000 [58:01<15:42:02,  5.97s/it]  5%|▌         | 536/10000 [58:06<15:16:10,  5.81s/it]  5%|▌         | 537/10000 [58:12<15:30:29,  5.90s/it]  5%|▌         | 538/10000 [58:21<17:35:35,  6.69s/it]  5%|▌         | 539/10000 [58:27<17:38:09,  6.71s/it]  5%|▌         | 540/10000 [58:34<17:53:37,  6.81s/it]                                                      {'loss': 0.0044, 'grad_norm': 0.00860595703125, 'learning_rate': 0.0002, 'epoch': 0.72}
  5%|▌         | 540/10000 [58:34<17:53:37,  6.81s/it]  5%|▌         | 541/10000 [58:41<17:45:02,  6.76s/it]  5%|▌         | 542/10000 [58:48<17:33:36,  6.68s/it]  5%|▌         | 543/10000 [58:53<16:23:16,  6.24s/it]  5%|▌         | 544/10000 [59:01<17:39:08,  6.72s/it]  5%|▌         | 545/10000 [59:07<17:30:34,  6.67s/it]  5%|▌         | 546/10000 [59:13<16:32:17,  6.30s/it]  5%|▌         | 547/10000 [59:18<15:40:06,  5.97s/it]  5%|▌         | 548/10000 [59:23<15:07:54,  5.76s/it]  5%|▌         | 549/10000 [59:28<14:34:09,  5.55s/it]  6%|▌         | 550/10000 [59:33<14:12:08,  5.41s/it]                                                      {'loss': 0.0017, 'grad_norm': 0.0250244140625, 'learning_rate': 0.0002, 'epoch': 0.73}
  6%|▌         | 550/10000 [59:33<14:12:08,  5.41s/it]  6%|▌         | 551/10000 [59:41<16:06:03,  6.13s/it]  6%|▌         | 552/10000 [59:47<15:46:44,  6.01s/it]  6%|▌         | 553/10000 [59:52<15:03:41,  5.74s/it]  6%|▌         | 554/10000 [59:57<14:33:30,  5.55s/it]  6%|▌         | 555/10000 [1:00:03<14:32:59,  5.55s/it]  6%|▌         | 556/10000 [1:00:09<15:16:16,  5.82s/it]  6%|▌         | 557/10000 [1:00:17<17:01:28,  6.49s/it]  6%|▌         | 558/10000 [1:00:23<16:53:30,  6.44s/it]  6%|▌         | 559/10000 [1:00:29<16:13:06,  6.18s/it]  6%|▌         | 560/10000 [1:00:35<16:14:41,  6.20s/it]                                                        {'loss': 0.0021, 'grad_norm': 0.00445556640625, 'learning_rate': 0.0002, 'epoch': 0.75}
  6%|▌         | 560/10000 [1:00:35<16:14:41,  6.20s/it]  6%|▌         | 561/10000 [1:00:42<16:21:45,  6.24s/it]  6%|▌         | 562/10000 [1:00:48<16:33:06,  6.31s/it]  6%|▌         | 563/10000 [1:00:58<19:05:40,  7.28s/it]  6%|▌         | 564/10000 [1:01:04<18:02:42,  6.88s/it]  6%|▌         | 565/10000 [1:01:09<16:57:32,  6.47s/it]  6%|▌         | 566/10000 [1:01:16<17:15:04,  6.58s/it]  6%|▌         | 567/10000 [1:01:21<16:24:37,  6.26s/it]  6%|▌         | 568/10000 [1:01:26<15:26:17,  5.89s/it]  6%|▌         | 569/10000 [1:01:33<16:17:48,  6.22s/it]  6%|▌         | 570/10000 [1:01:40<16:46:17,  6.40s/it]                                                        {'loss': 0.0026, 'grad_norm': 0.0123291015625, 'learning_rate': 0.0002, 'epoch': 0.76}
  6%|▌         | 570/10000 [1:01:40<16:46:17,  6.40s/it]  6%|▌         | 571/10000 [1:01:47<16:51:49,  6.44s/it]  6%|▌         | 572/10000 [1:01:52<16:03:27,  6.13s/it]  6%|▌         | 573/10000 [1:01:58<15:50:41,  6.05s/it]  6%|▌         | 574/10000 [1:02:04<16:08:31,  6.17s/it]  6%|▌         | 575/10000 [1:02:11<16:27:08,  6.28s/it]  6%|▌         | 576/10000 [1:02:19<17:48:48,  6.80s/it]  6%|▌         | 577/10000 [1:02:26<17:46:44,  6.79s/it]  6%|▌         | 578/10000 [1:02:33<17:56:33,  6.86s/it]  6%|▌         | 579/10000 [1:02:40<17:57:12,  6.86s/it]  6%|▌         | 580/10000 [1:02:47<18:28:29,  7.06s/it]                                                        {'loss': 0.0032, 'grad_norm': 0.021240234375, 'learning_rate': 0.0002, 'epoch': 0.77}
  6%|▌         | 580/10000 [1:02:47<18:28:29,  7.06s/it]  6%|▌         | 581/10000 [1:02:55<19:16:18,  7.37s/it]  6%|▌         | 582/10000 [1:03:04<20:34:40,  7.87s/it]  6%|▌         | 583/10000 [1:03:11<19:45:43,  7.55s/it]  6%|▌         | 584/10000 [1:03:19<19:45:27,  7.55s/it]  6%|▌         | 585/10000 [1:03:26<19:55:41,  7.62s/it]  6%|▌         | 586/10000 [1:03:33<19:18:21,  7.38s/it]  6%|▌         | 587/10000 [1:03:40<18:48:40,  7.19s/it]  6%|▌         | 588/10000 [1:03:49<19:57:04,  7.63s/it]  6%|▌         | 589/10000 [1:03:56<19:56:49,  7.63s/it]  6%|▌         | 590/10000 [1:04:03<19:18:09,  7.38s/it]                                                        {'loss': 0.0044, 'grad_norm': 0.0031585693359375, 'learning_rate': 0.0002, 'epoch': 0.79}
  6%|▌         | 590/10000 [1:04:03<19:18:09,  7.38s/it]  6%|▌         | 591/10000 [1:04:10<18:43:42,  7.17s/it]  6%|▌         | 592/10000 [1:04:16<18:20:57,  7.02s/it]  6%|▌         | 593/10000 [1:04:23<18:10:56,  6.96s/it]  6%|▌         | 594/10000 [1:04:32<19:12:25,  7.35s/it]  6%|▌         | 595/10000 [1:04:38<18:47:01,  7.19s/it]  6%|▌         | 596/10000 [1:04:45<18:23:15,  7.04s/it]  6%|▌         | 597/10000 [1:04:52<18:13:29,  6.98s/it]  6%|▌         | 598/10000 [1:04:58<17:55:02,  6.86s/it]  6%|▌         | 599/10000 [1:05:05<17:29:11,  6.70s/it]  6%|▌         | 600/10000 [1:05:10<16:31:43,  6.33s/it]                                                        {'loss': 0.0025, 'grad_norm': 0.0302734375, 'learning_rate': 0.0002, 'epoch': 0.8}
  6%|▌         | 600/10000 [1:05:10<16:31:43,  6.33s/it]  6%|▌         | 601/10000 [1:05:18<17:33:26,  6.72s/it]  6%|▌         | 602/10000 [1:05:24<16:46:06,  6.42s/it]  6%|▌         | 603/10000 [1:05:29<16:08:30,  6.18s/it]  6%|▌         | 604/10000 [1:05:35<15:38:47,  5.99s/it]  6%|▌         | 605/10000 [1:05:41<15:54:05,  6.09s/it]  6%|▌         | 606/10000 [1:05:48<16:29:24,  6.32s/it]  6%|▌         | 607/10000 [1:05:55<16:50:47,  6.46s/it]  6%|▌         | 608/10000 [1:06:01<16:55:15,  6.49s/it]  6%|▌         | 609/10000 [1:06:08<16:58:14,  6.51s/it]  6%|▌         | 610/10000 [1:06:15<17:11:56,  6.59s/it]                                                        {'loss': 0.0019, 'grad_norm': 0.014892578125, 'learning_rate': 0.0002, 'epoch': 0.81}
  6%|▌         | 610/10000 [1:06:15<17:11:56,  6.59s/it]  6%|▌         | 611/10000 [1:06:21<17:15:08,  6.62s/it]  6%|▌         | 612/10000 [1:06:28<16:57:23,  6.50s/it]  6%|▌         | 613/10000 [1:06:35<17:59:35,  6.90s/it]  6%|▌         | 614/10000 [1:06:42<17:43:20,  6.80s/it]  6%|▌         | 615/10000 [1:06:49<17:35:19,  6.75s/it]  6%|▌         | 616/10000 [1:06:55<17:30:51,  6.72s/it]  6%|▌         | 617/10000 [1:07:02<17:18:59,  6.64s/it]  6%|▌         | 618/10000 [1:07:10<18:57:10,  7.27s/it]  6%|▌         | 619/10000 [1:07:19<19:50:09,  7.61s/it]  6%|▌         | 620/10000 [1:07:26<19:17:39,  7.41s/it]                                                        {'loss': 0.0041, 'grad_norm': 0.010986328125, 'learning_rate': 0.0002, 'epoch': 0.83}
  6%|▌         | 620/10000 [1:07:26<19:17:39,  7.41s/it]  6%|▌         | 621/10000 [1:07:32<18:25:18,  7.07s/it]  6%|▌         | 622/10000 [1:07:39<18:07:38,  6.96s/it]  6%|▌         | 623/10000 [1:07:45<17:17:07,  6.64s/it]  6%|▌         | 624/10000 [1:07:50<16:34:17,  6.36s/it]  6%|▋         | 625/10000 [1:07:57<16:35:42,  6.37s/it]  6%|▋         | 626/10000 [1:08:05<17:43:57,  6.81s/it]  6%|▋         | 627/10000 [1:08:10<16:58:08,  6.52s/it]  6%|▋         | 628/10000 [1:08:16<16:34:19,  6.37s/it]  6%|▋         | 629/10000 [1:08:22<15:36:57,  6.00s/it]  6%|▋         | 630/10000 [1:08:28<15:33:29,  5.98s/it]                                                        {'loss': 0.0026, 'grad_norm': 0.00885009765625, 'learning_rate': 0.0002, 'epoch': 0.84}
  6%|▋         | 630/10000 [1:08:28<15:33:29,  5.98s/it]  6%|▋         | 631/10000 [1:08:33<15:04:33,  5.79s/it]  6%|▋         | 632/10000 [1:08:41<16:32:57,  6.36s/it]  6%|▋         | 633/10000 [1:08:47<16:50:56,  6.48s/it]  6%|▋         | 634/10000 [1:08:54<16:56:51,  6.51s/it]  6%|▋         | 635/10000 [1:09:01<17:09:08,  6.59s/it]  6%|▋         | 636/10000 [1:09:07<17:11:40,  6.61s/it]  6%|▋         | 637/10000 [1:09:14<17:07:37,  6.59s/it]  6%|▋         | 638/10000 [1:09:21<17:28:52,  6.72s/it]  6%|▋         | 639/10000 [1:09:27<16:47:09,  6.46s/it]  6%|▋         | 640/10000 [1:09:32<15:55:11,  6.12s/it]                                                        {'loss': 0.002, 'grad_norm': 0.00677490234375, 'learning_rate': 0.0002, 'epoch': 0.85}
  6%|▋         | 640/10000 [1:09:32<15:55:11,  6.12s/it]  6%|▋         | 641/10000 [1:09:39<16:35:46,  6.38s/it]  6%|▋         | 642/10000 [1:09:46<17:02:10,  6.55s/it]  6%|▋         | 643/10000 [1:09:52<16:40:23,  6.41s/it]  6%|▋         | 644/10000 [1:10:03<19:50:14,  7.63s/it]  6%|▋         | 645/10000 [1:10:09<18:56:47,  7.29s/it]  6%|▋         | 646/10000 [1:10:15<17:53:49,  6.89s/it]  6%|▋         | 647/10000 [1:10:22<17:40:42,  6.80s/it]  6%|▋         | 648/10000 [1:10:28<17:08:31,  6.60s/it]  6%|▋         | 649/10000 [1:10:34<16:44:16,  6.44s/it]  6%|▋         | 650/10000 [1:10:40<16:48:38,  6.47s/it]                                                        {'loss': 0.0067, 'grad_norm': 0.038330078125, 'learning_rate': 0.0002, 'epoch': 0.87}
  6%|▋         | 650/10000 [1:10:40<16:48:38,  6.47s/it]  7%|▋         | 651/10000 [1:10:47<17:15:53,  6.65s/it]  7%|▋         | 652/10000 [1:10:54<17:00:54,  6.55s/it]  7%|▋         | 653/10000 [1:11:00<16:47:42,  6.47s/it]  7%|▋         | 654/10000 [1:11:06<16:40:57,  6.43s/it]  7%|▋         | 655/10000 [1:11:14<17:19:45,  6.68s/it]  7%|▋         | 656/10000 [1:11:20<16:44:30,  6.45s/it]  7%|▋         | 657/10000 [1:11:28<18:13:59,  7.03s/it]  7%|▋         | 658/10000 [1:11:34<17:28:28,  6.73s/it]  7%|▋         | 659/10000 [1:11:40<16:39:25,  6.42s/it]  7%|▋         | 660/10000 [1:11:46<16:54:10,  6.52s/it]                                                        {'loss': 0.0046, 'grad_norm': 0.01953125, 'learning_rate': 0.0002, 'epoch': 0.88}
  7%|▋         | 660/10000 [1:11:46<16:54:10,  6.52s/it]  7%|▋         | 661/10000 [1:11:53<16:43:04,  6.44s/it]  7%|▋         | 662/10000 [1:11:58<15:56:06,  6.14s/it]  7%|▋         | 663/10000 [1:12:05<16:52:07,  6.50s/it]  7%|▋         | 664/10000 [1:12:12<16:39:21,  6.42s/it]  7%|▋         | 665/10000 [1:12:17<15:50:42,  6.11s/it]  7%|▋         | 666/10000 [1:12:22<15:15:17,  5.88s/it]  7%|▋         | 667/10000 [1:12:28<14:57:27,  5.77s/it]  7%|▋         | 668/10000 [1:12:33<14:43:36,  5.68s/it]  7%|▋         | 669/10000 [1:12:40<15:40:30,  6.05s/it]  7%|▋         | 670/10000 [1:12:47<15:56:37,  6.15s/it]                                                        {'loss': 0.0044, 'grad_norm': 0.01214599609375, 'learning_rate': 0.0002, 'epoch': 0.89}
  7%|▋         | 670/10000 [1:12:47<15:56:37,  6.15s/it]  7%|▋         | 671/10000 [1:12:53<16:02:18,  6.19s/it]  7%|▋         | 672/10000 [1:12:58<15:15:21,  5.89s/it]  7%|▋         | 673/10000 [1:13:03<14:48:38,  5.72s/it]  7%|▋         | 674/10000 [1:13:09<14:55:47,  5.76s/it]  7%|▋         | 675/10000 [1:13:16<15:38:07,  6.04s/it]  7%|▋         | 676/10000 [1:13:24<17:02:35,  6.58s/it]  7%|▋         | 677/10000 [1:13:29<16:06:37,  6.22s/it]  7%|▋         | 678/10000 [1:13:34<15:17:24,  5.90s/it]  7%|▋         | 679/10000 [1:13:40<15:00:00,  5.79s/it]  7%|▋         | 680/10000 [1:13:46<14:55:30,  5.77s/it]                                                        {'loss': 0.0047, 'grad_norm': 0.029296875, 'learning_rate': 0.0002, 'epoch': 0.91}
  7%|▋         | 680/10000 [1:13:46<14:55:30,  5.77s/it]  7%|▋         | 681/10000 [1:13:51<14:58:14,  5.78s/it]  7%|▋         | 682/10000 [1:13:58<15:43:47,  6.08s/it]  7%|▋         | 683/10000 [1:14:04<15:22:20,  5.94s/it]  7%|▋         | 684/10000 [1:14:09<14:45:51,  5.71s/it]  7%|▋         | 685/10000 [1:14:14<14:20:16,  5.54s/it]  7%|▋         | 686/10000 [1:14:19<14:07:15,  5.46s/it]  7%|▋         | 687/10000 [1:14:25<14:12:05,  5.49s/it]  7%|▋         | 688/10000 [1:14:33<16:27:55,  6.37s/it]  7%|▋         | 689/10000 [1:14:40<16:27:51,  6.37s/it]  7%|▋         | 690/10000 [1:14:46<16:16:35,  6.29s/it]                                                        {'loss': 0.0017, 'grad_norm': 0.01019287109375, 'learning_rate': 0.0002, 'epoch': 0.92}
  7%|▋         | 690/10000 [1:14:46<16:16:35,  6.29s/it]  7%|▋         | 691/10000 [1:14:51<15:23:01,  5.95s/it]  7%|▋         | 692/10000 [1:14:57<15:12:42,  5.88s/it]  7%|▋         | 693/10000 [1:15:04<16:28:11,  6.37s/it]  7%|▋         | 694/10000 [1:15:16<20:54:04,  8.09s/it]  7%|▋         | 695/10000 [1:15:24<20:20:16,  7.87s/it]  7%|▋         | 696/10000 [1:15:31<20:08:53,  7.80s/it]  7%|▋         | 697/10000 [1:15:38<18:57:39,  7.34s/it]  7%|▋         | 698/10000 [1:15:43<17:22:41,  6.73s/it]  7%|▋         | 699/10000 [1:15:49<16:32:38,  6.40s/it]  7%|▋         | 700/10000 [1:15:54<16:07:24,  6.24s/it]                                                        {'loss': 0.0042, 'grad_norm': 0.0269775390625, 'learning_rate': 0.0002, 'epoch': 0.93}
  7%|▋         | 700/10000 [1:15:54<16:07:24,  6.24s/it]  7%|▋         | 701/10000 [1:16:03<17:39:25,  6.84s/it]  7%|▋         | 702/10000 [1:16:09<17:09:22,  6.64s/it]  7%|▋         | 703/10000 [1:16:15<16:32:46,  6.41s/it]  7%|▋         | 704/10000 [1:16:21<16:22:24,  6.34s/it]  7%|▋         | 705/10000 [1:16:27<16:29:56,  6.39s/it]  7%|▋         | 706/10000 [1:16:34<16:38:03,  6.44s/it]  7%|▋         | 707/10000 [1:16:42<17:39:48,  6.84s/it]  7%|▋         | 708/10000 [1:16:48<17:14:04,  6.68s/it]  7%|▋         | 709/10000 [1:16:53<16:13:39,  6.29s/it]  7%|▋         | 710/10000 [1:16:59<15:20:02,  5.94s/it]                                                        {'loss': 0.0038, 'grad_norm': 0.007293701171875, 'learning_rate': 0.0002, 'epoch': 0.95}
  7%|▋         | 710/10000 [1:16:59<15:20:02,  5.94s/it]  7%|▋         | 711/10000 [1:17:05<15:48:39,  6.13s/it]  7%|▋         | 712/10000 [1:17:12<16:22:43,  6.35s/it]  7%|▋         | 713/10000 [1:17:19<16:51:46,  6.54s/it]  7%|▋         | 714/10000 [1:17:26<16:52:26,  6.54s/it]  7%|▋         | 715/10000 [1:17:32<17:03:38,  6.61s/it]  7%|▋         | 716/10000 [1:17:39<17:13:46,  6.68s/it]  7%|▋         | 717/10000 [1:17:46<17:21:05,  6.73s/it]  7%|▋         | 718/10000 [1:17:53<17:21:57,  6.74s/it]  7%|▋         | 719/10000 [1:18:01<18:27:52,  7.16s/it]  7%|▋         | 720/10000 [1:18:07<17:41:29,  6.86s/it]                                                        {'loss': 0.0018, 'grad_norm': 0.01068115234375, 'learning_rate': 0.0002, 'epoch': 0.96}
  7%|▋         | 720/10000 [1:18:07<17:41:29,  6.86s/it]  7%|▋         | 721/10000 [1:18:12<16:22:03,  6.35s/it]  7%|▋         | 722/10000 [1:18:17<15:21:03,  5.96s/it]  7%|▋         | 723/10000 [1:18:22<14:38:13,  5.68s/it]  7%|▋         | 724/10000 [1:18:29<15:10:44,  5.89s/it]  7%|▋         | 725/10000 [1:18:35<15:53:03,  6.17s/it]  7%|▋         | 726/10000 [1:18:43<16:41:44,  6.48s/it]  7%|▋         | 727/10000 [1:18:49<16:44:52,  6.50s/it]  7%|▋         | 728/10000 [1:18:56<17:13:33,  6.69s/it]  7%|▋         | 729/10000 [1:19:02<16:45:49,  6.51s/it]  7%|▋         | 730/10000 [1:19:08<15:54:43,  6.18s/it]                                                        {'loss': 0.0023, 'grad_norm': 0.01287841796875, 'learning_rate': 0.0002, 'epoch': 0.97}
  7%|▋         | 730/10000 [1:19:08<15:54:43,  6.18s/it]  7%|▋         | 731/10000 [1:19:14<15:39:10,  6.08s/it]  7%|▋         | 732/10000 [1:19:22<17:33:50,  6.82s/it]  7%|▋         | 733/10000 [1:19:28<16:53:21,  6.56s/it]  7%|▋         | 734/10000 [1:19:33<15:48:29,  6.14s/it]  7%|▋         | 735/10000 [1:19:39<15:23:27,  5.98s/it]  7%|▋         | 736/10000 [1:19:45<15:25:41,  6.00s/it]  7%|▋         | 737/10000 [1:19:51<15:36:50,  6.07s/it]  7%|▋         | 738/10000 [1:20:04<20:48:38,  8.09s/it]  7%|▋         | 739/10000 [1:20:11<19:37:09,  7.63s/it]  7%|▋         | 740/10000 [1:20:17<18:31:15,  7.20s/it]                                                        {'loss': 0.0029, 'grad_norm': 0.0869140625, 'learning_rate': 0.0002, 'epoch': 0.99}
  7%|▋         | 740/10000 [1:20:17<18:31:15,  7.20s/it]  7%|▋         | 741/10000 [1:20:23<17:51:49,  6.95s/it]  7%|▋         | 742/10000 [1:20:29<17:12:41,  6.69s/it]  7%|▋         | 743/10000 [1:20:36<16:52:45,  6.56s/it]  7%|▋         | 744/10000 [1:20:43<17:21:21,  6.75s/it]  7%|▋         | 745/10000 [1:20:49<17:04:50,  6.64s/it]  7%|▋         | 746/10000 [1:20:54<15:52:28,  6.18s/it]  7%|▋         | 747/10000 [1:21:01<16:00:18,  6.23s/it]  7%|▋         | 748/10000 [1:21:07<16:10:49,  6.30s/it]  7%|▋         | 749/10000 [1:21:13<16:01:36,  6.24s/it]  8%|▊         | 750/10000 [1:21:20<16:15:29,  6.33s/it]                                                        {'loss': 0.0039, 'grad_norm': 0.0703125, 'learning_rate': 0.0002, 'epoch': 1.0}
  8%|▊         | 750/10000 [1:21:20<16:15:29,  6.33s/it]Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...
Saving PEFT checkpoint...Saving PEFT checkpoint...

Saving PEFT checkpoint...
Saving PEFT checkpoint...
  8%|▊         | 751/10000 [1:21:37<24:24:34,  9.50s/it]  8%|▊         | 752/10000 [1:22:09<42:17:42, 16.46s/it]  8%|▊         | 753/10000 [1:22:15<33:54:18, 13.20s/it]  8%|▊         | 754/10000 [1:22:20<27:37:46, 10.76s/it]  8%|▊         | 755/10000 [1:22:25<23:14:28,  9.05s/it]  8%|▊         | 756/10000 [1:22:31<20:33:00,  8.00s/it]  8%|▊         | 757/10000 [1:22:36<18:42:21,  7.29s/it]  8%|▊         | 758/10000 [1:22:43<18:33:24,  7.23s/it]  8%|▊         | 759/10000 [1:22:49<17:43:22,  6.90s/it]  8%|▊         | 760/10000 [1:22:55<16:41:35,  6.50s/it]                                                        {'loss': 0.0042, 'grad_norm': 0.0123291015625, 'learning_rate': 0.0002, 'epoch': 1.01}
  8%|▊         | 760/10000 [1:22:55<16:41:35,  6.50s/it]  8%|▊         | 761/10000 [1:23:01<16:14:32,  6.33s/it]  8%|▊         | 762/10000 [1:23:07<16:06:39,  6.28s/it]  8%|▊         | 763/10000 [1:23:14<16:25:55,  6.40s/it]  8%|▊         | 764/10000 [1:23:22<18:15:49,  7.12s/it]  8%|▊         | 765/10000 [1:23:30<18:10:25,  7.08s/it]  8%|▊         | 766/10000 [1:23:35<17:04:38,  6.66s/it]  8%|▊         | 767/10000 [1:23:40<15:52:40,  6.19s/it]  8%|▊         | 768/10000 [1:23:45<15:01:11,  5.86s/it]  8%|▊         | 769/10000 [1:23:51<14:32:06,  5.67s/it]  8%|▊         | 770/10000 [1:23:58<15:51:26,  6.18s/it]                                                        {'loss': 0.0041, 'grad_norm': 0.01611328125, 'learning_rate': 0.0002, 'epoch': 1.03}
  8%|▊         | 770/10000 [1:23:58<15:51:26,  6.18s/it]  8%|▊         | 771/10000 [1:24:04<15:59:03,  6.24s/it]  8%|▊         | 772/10000 [1:24:11<16:14:49,  6.34s/it]  8%|▊         | 773/10000 [1:24:16<15:35:56,  6.09s/it]  8%|▊         | 774/10000 [1:24:21<14:50:07,  5.79s/it]  8%|▊         | 775/10000 [1:24:27<14:26:07,  5.63s/it]  8%|▊         | 776/10000 [1:24:32<14:19:41,  5.59s/it]  8%|▊         | 777/10000 [1:24:40<16:18:38,  6.37s/it]  8%|▊         | 778/10000 [1:24:46<15:48:49,  6.17s/it]  8%|▊         | 779/10000 [1:24:52<15:36:14,  6.09s/it]  8%|▊         | 780/10000 [1:24:58<15:23:32,  6.01s/it]                                                        {'loss': 0.0025, 'grad_norm': 0.010986328125, 'learning_rate': 0.0002, 'epoch': 1.04}
  8%|▊         | 780/10000 [1:24:58<15:23:32,  6.01s/it]  8%|▊         | 781/10000 [1:25:04<15:29:14,  6.05s/it]  8%|▊         | 782/10000 [1:25:11<15:50:00,  6.18s/it]  8%|▊         | 783/10000 [1:25:20<18:23:11,  7.18s/it]  8%|▊         | 784/10000 [1:25:27<17:55:23,  7.00s/it]  8%|▊         | 785/10000 [1:25:33<17:21:04,  6.78s/it]  8%|▊         | 786/10000 [1:25:39<17:09:00,  6.70s/it]  8%|▊         | 787/10000 [1:25:45<16:08:07,  6.30s/it]  8%|▊         | 788/10000 [1:25:51<15:48:41,  6.18s/it]  8%|▊         | 789/10000 [1:26:02<19:33:18,  7.64s/it]  8%|▊         | 790/10000 [1:26:08<18:48:15,  7.35s/it]                                                        {'loss': 0.0024, 'grad_norm': 0.01470947265625, 'learning_rate': 0.0002, 'epoch': 1.05}
  8%|▊         | 790/10000 [1:26:08<18:48:15,  7.35s/it]  8%|▊         | 791/10000 [1:26:14<17:44:04,  6.93s/it]  8%|▊         | 792/10000 [1:26:20<16:24:04,  6.41s/it]  8%|▊         | 793/10000 [1:26:25<16:01:36,  6.27s/it]  8%|▊         | 794/10000 [1:26:32<15:51:26,  6.20s/it]  8%|▊         | 795/10000 [1:26:39<16:52:00,  6.60s/it]  8%|▊         | 796/10000 [1:26:45<16:38:01,  6.51s/it]  8%|▊         | 797/10000 [1:26:53<17:19:06,  6.77s/it]  8%|▊         | 798/10000 [1:27:01<18:11:56,  7.12s/it]  8%|▊         | 799/10000 [1:27:08<18:12:45,  7.13s/it]  8%|▊         | 800/10000 [1:27:14<17:15:22,  6.75s/it]                                                        {'loss': 0.0017, 'grad_norm': 0.0289306640625, 'learning_rate': 0.0002, 'epoch': 1.07}
  8%|▊         | 800/10000 [1:27:14<17:15:22,  6.75s/it]  8%|▊         | 801/10000 [1:27:20<16:37:39,  6.51s/it]  8%|▊         | 802/10000 [1:27:28<17:56:06,  7.02s/it]  8%|▊         | 803/10000 [1:27:33<16:43:53,  6.55s/it]  8%|▊         | 804/10000 [1:27:38<15:42:01,  6.15s/it]  8%|▊         | 805/10000 [1:27:44<15:01:03,  5.88s/it]  8%|▊         | 806/10000 [1:27:49<14:34:44,  5.71s/it]  8%|▊         | 807/10000 [1:27:54<14:15:50,  5.59s/it]  8%|▊         | 808/10000 [1:28:02<16:10:26,  6.33s/it]  8%|▊         | 809/10000 [1:28:09<16:13:59,  6.36s/it]  8%|▊         | 810/10000 [1:28:15<16:04:19,  6.30s/it]                                                        {'loss': 0.0017, 'grad_norm': 0.01708984375, 'learning_rate': 0.0002, 'epoch': 1.08}
  8%|▊         | 810/10000 [1:28:15<16:04:19,  6.30s/it]  8%|▊         | 811/10000 [1:28:22<16:18:37,  6.39s/it]  8%|▊         | 812/10000 [1:28:28<16:33:40,  6.49s/it]  8%|▊         | 813/10000 [1:28:35<16:45:46,  6.57s/it]  8%|▊         | 814/10000 [1:28:43<17:43:37,  6.95s/it]  8%|▊         | 815/10000 [1:28:49<16:53:15,  6.62s/it]  8%|▊         | 816/10000 [1:28:55<16:14:15,  6.36s/it]  8%|▊         | 817/10000 [1:29:00<15:29:01,  6.07s/it]  8%|▊         | 818/10000 [1:29:05<14:45:56,  5.79s/it]  8%|▊         | 819/10000 [1:29:12<15:16:59,  5.99s/it]  8%|▊         | 820/10000 [1:29:19<16:44:47,  6.57s/it]                                                        {'loss': 0.0012, 'grad_norm': 0.00933837890625, 'learning_rate': 0.0002, 'epoch': 1.09}
  8%|▊         | 820/10000 [1:29:19<16:44:47,  6.57s/it]  8%|▊         | 821/10000 [1:29:26<16:25:16,  6.44s/it]  8%|▊         | 822/10000 [1:29:32<16:17:46,  6.39s/it]  8%|▊         | 823/10000 [1:29:40<17:30:02,  6.87s/it]  8%|▊         | 824/10000 [1:29:47<17:39:31,  6.93s/it]  8%|▊         | 825/10000 [1:29:54<17:39:30,  6.93s/it]  8%|▊         | 826/10000 [1:30:00<17:22:09,  6.82s/it]  8%|▊         | 827/10000 [1:30:08<18:10:49,  7.14s/it]  8%|▊         | 828/10000 [1:30:13<16:44:21,  6.57s/it]  8%|▊         | 829/10000 [1:30:19<15:41:00,  6.16s/it]  8%|▊         | 830/10000 [1:30:24<15:16:08,  5.99s/it]                                                        {'loss': 0.0027, 'grad_norm': 0.02099609375, 'learning_rate': 0.0002, 'epoch': 1.11}
  8%|▊         | 830/10000 [1:30:24<15:16:08,  5.99s/it]  8%|▊         | 831/10000 [1:30:31<15:45:01,  6.18s/it]  8%|▊         | 832/10000 [1:30:37<15:43:18,  6.17s/it]  8%|▊         | 833/10000 [1:30:46<17:31:01,  6.88s/it]  8%|▊         | 834/10000 [1:30:53<17:41:33,  6.95s/it]  8%|▊         | 835/10000 [1:30:59<17:22:41,  6.83s/it]  8%|▊         | 836/10000 [1:31:06<17:10:37,  6.75s/it]  8%|▊         | 837/10000 [1:31:13<17:19:50,  6.81s/it]  8%|▊         | 838/10000 [1:31:20<17:16:35,  6.79s/it]  8%|▊         | 839/10000 [1:31:28<18:27:04,  7.25s/it]  8%|▊         | 840/10000 [1:31:35<18:06:41,  7.12s/it]                                                        {'loss': 0.0041, 'grad_norm': 0.01177978515625, 'learning_rate': 0.0002, 'epoch': 1.12}
  8%|▊         | 840/10000 [1:31:35<18:06:41,  7.12s/it]  8%|▊         | 841/10000 [1:31:42<17:55:40,  7.05s/it]  8%|▊         | 842/10000 [1:31:48<17:37:15,  6.93s/it]  8%|▊         | 843/10000 [1:31:55<17:36:12,  6.92s/it]  8%|▊         | 844/10000 [1:32:02<17:34:15,  6.91s/it]  8%|▊         | 845/10000 [1:32:11<18:56:03,  7.45s/it]  8%|▊         | 846/10000 [1:32:17<18:21:17,  7.22s/it]  8%|▊         | 847/10000 [1:32:24<18:04:30,  7.11s/it]  8%|▊         | 848/10000 [1:32:31<17:53:19,  7.04s/it]  8%|▊         | 849/10000 [1:32:38<17:43:12,  6.97s/it]  8%|▊         | 850/10000 [1:32:44<17:09:04,  6.75s/it]                                                        {'loss': 0.0029, 'grad_norm': 0.00823974609375, 'learning_rate': 0.0002, 'epoch': 1.13}
  8%|▊         | 850/10000 [1:32:44<17:09:04,  6.75s/it]  9%|▊         | 851/10000 [1:32:51<17:14:47,  6.79s/it]  9%|▊         | 852/10000 [1:32:59<17:58:06,  7.07s/it]  9%|▊         | 853/10000 [1:33:05<17:10:03,  6.76s/it]  9%|▊         | 854/10000 [1:33:11<16:36:34,  6.54s/it]  9%|▊         | 855/10000 [1:33:17<16:24:15,  6.46s/it]  9%|▊         | 856/10000 [1:33:23<15:47:31,  6.22s/it]  9%|▊         | 857/10000 [1:33:28<15:24:40,  6.07s/it]  9%|▊         | 858/10000 [1:33:37<17:06:39,  6.74s/it]  9%|▊         | 859/10000 [1:33:43<16:32:50,  6.52s/it]  9%|▊         | 860/10000 [1:33:49<16:39:12,  6.56s/it]                                                        {'loss': 0.0047, 'grad_norm': 0.005584716796875, 'learning_rate': 0.0002, 'epoch': 1.15}
  9%|▊         | 860/10000 [1:33:49<16:39:12,  6.56s/it]  9%|▊         | 861/10000 [1:33:56<16:45:33,  6.60s/it]  9%|▊         | 862/10000 [1:34:02<16:03:47,  6.33s/it]  9%|▊         | 863/10000 [1:34:08<15:39:44,  6.17s/it]  9%|▊         | 864/10000 [1:34:18<18:34:00,  7.32s/it]  9%|▊         | 865/10000 [1:34:24<18:00:14,  7.10s/it]  9%|▊         | 866/10000 [1:34:30<16:46:51,  6.61s/it]  9%|▊         | 867/10000 [1:34:35<15:58:29,  6.30s/it]  9%|▊         | 868/10000 [1:34:41<15:32:18,  6.13s/it]  9%|▊         | 869/10000 [1:34:46<14:49:57,  5.85s/it]  9%|▊         | 870/10000 [1:34:54<16:15:10,  6.41s/it]                                                        {'loss': 0.0029, 'grad_norm': 0.01708984375, 'learning_rate': 0.0002, 'epoch': 1.16}
  9%|▊         | 870/10000 [1:34:54<16:15:10,  6.41s/it]  9%|▊         | 871/10000 [1:35:00<15:54:29,  6.27s/it]  9%|▊         | 872/10000 [1:35:05<15:02:29,  5.93s/it]  9%|▊         | 873/10000 [1:35:10<14:24:47,  5.69s/it]  9%|▊         | 874/10000 [1:35:16<14:25:47,  5.69s/it]  9%|▉         | 875/10000 [1:35:23<15:22:41,  6.07s/it]  9%|▉         | 876/10000 [1:35:31<17:25:38,  6.88s/it]  9%|▉         | 877/10000 [1:35:39<18:15:58,  7.21s/it]  9%|▉         | 878/10000 [1:35:45<17:12:15,  6.79s/it]  9%|▉         | 879/10000 [1:35:50<15:55:31,  6.29s/it]  9%|▉         | 880/10000 [1:35:57<15:56:05,  6.29s/it]                                                        {'loss': 0.0012, 'grad_norm': 0.002288818359375, 'learning_rate': 0.0002, 'epoch': 1.17}
  9%|▉         | 880/10000 [1:35:57<15:56:05,  6.29s/it]  9%|▉         | 881/10000 [1:36:03<16:20:31,  6.45s/it]  9%|▉         | 882/10000 [1:36:09<15:52:32,  6.27s/it]  9%|▉         | 883/10000 [1:36:18<17:32:52,  6.93s/it]  9%|▉         | 884/10000 [1:36:24<16:41:30,  6.59s/it]  9%|▉         | 885/10000 [1:36:29<15:47:47,  6.24s/it]  9%|▉         | 886/10000 [1:36:34<14:56:09,  5.90s/it]  9%|▉         | 887/10000 [1:36:39<14:18:47,  5.65s/it]  9%|▉         | 888/10000 [1:36:44<14:01:00,  5.54s/it]  9%|▉         | 889/10000 [1:36:52<15:29:55,  6.12s/it]  9%|▉         | 890/10000 [1:36:57<15:01:35,  5.94s/it]                                                        {'loss': 0.001, 'grad_norm': 0.01361083984375, 'learning_rate': 0.0002, 'epoch': 1.19}
  9%|▉         | 890/10000 [1:36:57<15:01:35,  5.94s/it]  9%|▉         | 891/10000 [1:37:03<14:22:40,  5.68s/it]  9%|▉         | 892/10000 [1:37:08<14:05:32,  5.57s/it]  9%|▉         | 893/10000 [1:37:14<14:24:43,  5.70s/it]  9%|▉         | 894/10000 [1:37:19<14:10:15,  5.60s/it]  9%|▉         | 895/10000 [1:37:27<15:26:46,  6.11s/it]  9%|▉         | 896/10000 [1:37:33<15:28:29,  6.12s/it]  9%|▉         | 897/10000 [1:37:38<14:48:56,  5.86s/it]  9%|▉         | 898/10000 [1:37:43<14:18:08,  5.66s/it]  9%|▉         | 899/10000 [1:37:48<13:53:17,  5.49s/it]  9%|▉         | 900/10000 [1:37:53<13:39:23,  5.40s/it]                                                        {'loss': 0.003, 'grad_norm': 0.04150390625, 'learning_rate': 0.0002, 'epoch': 1.2}
  9%|▉         | 900/10000 [1:37:53<13:39:23,  5.40s/it]  9%|▉         | 901/10000 [1:37:59<13:33:52,  5.37s/it]  9%|▉         | 902/10000 [1:38:06<15:18:54,  6.06s/it]  9%|▉         | 903/10000 [1:38:12<14:56:09,  5.91s/it]  9%|▉         | 904/10000 [1:38:18<15:15:28,  6.04s/it]  9%|▉         | 905/10000 [1:38:25<15:45:53,  6.24s/it]  9%|▉         | 906/10000 [1:38:30<14:56:21,  5.91s/it]  9%|▉         | 907/10000 [1:38:36<14:48:07,  5.86s/it]  9%|▉         | 908/10000 [1:38:44<16:48:53,  6.66s/it]  9%|▉         | 909/10000 [1:38:51<16:35:26,  6.57s/it]  9%|▉         | 910/10000 [1:38:57<16:08:51,  6.40s/it]                                                        {'loss': 0.0014, 'grad_norm': 0.01202392578125, 'learning_rate': 0.0002, 'epoch': 1.21}
  9%|▉         | 910/10000 [1:38:57<16:08:51,  6.40s/it]  9%|▉         | 911/10000 [1:39:03<16:21:16,  6.48s/it]  9%|▉         | 912/10000 [1:39:10<16:28:27,  6.53s/it]  9%|▉         | 913/10000 [1:39:16<16:24:27,  6.50s/it]  9%|▉         | 914/10000 [1:39:24<17:09:11,  6.80s/it]  9%|▉         | 915/10000 [1:39:30<16:50:42,  6.67s/it]  9%|▉         | 916/10000 [1:39:37<16:33:44,  6.56s/it]  9%|▉         | 917/10000 [1:39:43<16:33:35,  6.56s/it]  9%|▉         | 918/10000 [1:39:49<16:19:21,  6.47s/it]  9%|▉         | 919/10000 [1:39:55<15:27:40,  6.13s/it]  9%|▉         | 920/10000 [1:40:02<16:21:19,  6.48s/it]                                                        {'loss': 0.0011, 'grad_norm': 0.0223388671875, 'learning_rate': 0.0002, 'epoch': 1.23}
  9%|▉         | 920/10000 [1:40:02<16:21:19,  6.48s/it]  9%|▉         | 921/10000 [1:40:09<16:35:11,  6.58s/it]  9%|▉         | 922/10000 [1:40:15<16:21:13,  6.49s/it]  9%|▉         | 923/10000 [1:40:22<16:49:27,  6.67s/it]  9%|▉         | 924/10000 [1:40:29<16:37:08,  6.59s/it]  9%|▉         | 925/10000 [1:40:35<16:29:07,  6.54s/it]  9%|▉         | 926/10000 [1:40:43<17:34:40,  6.97s/it]  9%|▉         | 927/10000 [1:40:51<18:18:41,  7.27s/it]  9%|▉         | 928/10000 [1:40:56<16:48:59,  6.67s/it]  9%|▉         | 929/10000 [1:41:02<15:55:21,  6.32s/it]  9%|▉         | 930/10000 [1:41:08<15:26:20,  6.13s/it]                                                        {'loss': 0.0031, 'grad_norm': 0.03515625, 'learning_rate': 0.0002, 'epoch': 1.24}
  9%|▉         | 930/10000 [1:41:08<15:26:20,  6.13s/it]  9%|▉         | 931/10000 [1:41:13<14:40:37,  5.83s/it]  9%|▉         | 932/10000 [1:41:18<14:24:54,  5.72s/it]  9%|▉         | 933/10000 [1:41:25<15:15:12,  6.06s/it]  9%|▉         | 934/10000 [1:41:31<14:54:22,  5.92s/it]  9%|▉         | 935/10000 [1:41:36<14:44:17,  5.85s/it]  9%|▉         | 936/10000 [1:41:42<14:46:23,  5.87s/it]  9%|▉         | 937/10000 [1:41:48<14:48:28,  5.88s/it]  9%|▉         | 938/10000 [1:41:54<14:49:06,  5.89s/it]  9%|▉         | 939/10000 [1:42:01<15:55:39,  6.33s/it]  9%|▉         | 940/10000 [1:42:08<16:13:30,  6.45s/it]                                                        {'loss': 0.0022, 'grad_norm': 0.0274658203125, 'learning_rate': 0.0002, 'epoch': 1.25}
  9%|▉         | 940/10000 [1:42:08<16:13:30,  6.45s/it]  9%|▉         | 941/10000 [1:42:15<16:22:23,  6.51s/it]  9%|▉         | 942/10000 [1:42:22<16:48:33,  6.68s/it]  9%|▉         | 943/10000 [1:42:29<16:56:26,  6.73s/it]  9%|▉         | 944/10000 [1:42:35<16:51:21,  6.70s/it]  9%|▉         | 945/10000 [1:42:45<19:01:13,  7.56s/it]  9%|▉         | 946/10000 [1:42:52<18:54:14,  7.52s/it]  9%|▉         | 947/10000 [1:42:59<18:16:22,  7.27s/it]  9%|▉         | 948/10000 [1:43:06<17:47:24,  7.08s/it]  9%|▉         | 949/10000 [1:43:12<17:26:11,  6.94s/it] 10%|▉         | 950/10000 [1:43:19<17:12:02,  6.84s/it]                                                        {'loss': 0.0022, 'grad_norm': 0.025634765625, 'learning_rate': 0.0002, 'epoch': 1.26}
 10%|▉         | 950/10000 [1:43:19<17:12:02,  6.84s/it] 10%|▉         | 951/10000 [1:43:26<17:30:25,  6.96s/it] 10%|▉         | 952/10000 [1:43:34<18:07:04,  7.21s/it] 10%|▉         | 953/10000 [1:43:39<16:36:57,  6.61s/it] 10%|▉         | 954/10000 [1:43:44<15:31:32,  6.18s/it] 10%|▉         | 955/10000 [1:43:50<14:58:09,  5.96s/it] 10%|▉         | 956/10000 [1:43:55<14:24:22,  5.73s/it] 10%|▉         | 957/10000 [1:44:00<13:59:09,  5.57s/it] 10%|▉         | 958/10000 [1:44:09<16:32:05,  6.58s/it] 10%|▉         | 959/10000 [1:44:16<16:47:22,  6.69s/it] 10%|▉         | 960/10000 [1:44:21<15:46:33,  6.28s/it]                                                        {'loss': 0.0019, 'grad_norm': 0.0283203125, 'learning_rate': 0.0002, 'epoch': 1.28}
 10%|▉         | 960/10000 [1:44:21<15:46:33,  6.28s/it] 10%|▉         | 961/10000 [1:44:27<15:17:05,  6.09s/it] 10%|▉         | 962/10000 [1:44:33<15:10:57,  6.05s/it] 10%|▉         | 963/10000 [1:44:40<15:40:06,  6.24s/it] 10%|▉         | 964/10000 [1:44:49<18:15:46,  7.28s/it] 10%|▉         | 965/10000 [1:44:56<18:12:33,  7.26s/it] 10%|▉         | 966/10000 [1:45:03<17:31:37,  6.98s/it] 10%|▉         | 967/10000 [1:45:09<17:07:28,  6.82s/it] 10%|▉         | 968/10000 [1:45:16<17:03:42,  6.80s/it] 10%|▉         | 969/10000 [1:45:23<17:02:49,  6.80s/it] 10%|▉         | 970/10000 [1:45:31<18:14:21,  7.27s/it]                                                        {'loss': 0.0038, 'grad_norm': 0.0145263671875, 'learning_rate': 0.0002, 'epoch': 1.29}
 10%|▉         | 970/10000 [1:45:31<18:14:21,  7.27s/it] 10%|▉         | 971/10000 [1:45:37<17:25:37,  6.95s/it] 10%|▉         | 972/10000 [1:45:44<16:55:20,  6.75s/it] 10%|▉         | 973/10000 [1:45:49<15:42:26,  6.26s/it] 10%|▉         | 974/10000 [1:45:54<15:16:41,  6.09s/it] 10%|▉         | 975/10000 [1:46:01<15:54:18,  6.34s/it] 10%|▉         | 976/10000 [1:46:07<15:30:37,  6.19s/it] 10%|▉         | 977/10000 [1:46:17<18:09:24,  7.24s/it] 10%|▉         | 978/10000 [1:46:23<17:15:07,  6.88s/it] 10%|▉         | 979/10000 [1:46:30<17:11:38,  6.86s/it] 10%|▉         | 980/10000 [1:46:35<16:11:40,  6.46s/it]                                                        {'loss': 0.0016, 'grad_norm': 0.003387451171875, 'learning_rate': 0.0002, 'epoch': 1.3}
 10%|▉         | 980/10000 [1:46:35<16:11:40,  6.46s/it] 10%|▉         | 981/10000 [1:46:41<15:18:56,  6.11s/it] 10%|▉         | 982/10000 [1:46:46<14:34:17,  5.82s/it] 10%|▉         | 983/10000 [1:46:53<15:50:48,  6.33s/it]